{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# necessary libraries, functions, and constants\n",
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "classes = ['ocean', 'ship', 'shore', 'sky']\n",
    "\n",
    "# options\n",
    "oversample = False\n",
    "\n",
    "\n",
    "def prepare_data(input_file, description, oversample=False):\n",
    "    \"\"\"\n",
    "    prepares the data, also prints some information about it\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    y_numeric = []\n",
    "    \n",
    "    print(\"\\n##############\\n%s Data summary:\" % (description))\n",
    "    with open(input_file, 'r') as csvfile:\n",
    "        data_reader = csv.reader(csvfile, dialect='excel')\n",
    "        for row in data_reader:\n",
    "            if len(row) > 0:\n",
    "                y.append(row[0])\n",
    "                X_float = [ float(x) for x in row[1:] ]\n",
    "                X.append(X_float)\n",
    "\n",
    "    # create a y_numeric for use with tensorflow\n",
    "    for obs in y:\n",
    "        y_numeric.append(classes.index(obs))\n",
    "\n",
    "    assert len(X) == len(y) == len(y_numeric)\n",
    "\n",
    "    # convert to np.array objects\n",
    "    y = np.array(y)\n",
    "    y_numeric = np.array(y_numeric)\n",
    "    X = np.array(X)\n",
    "\n",
    "    # how many features?\n",
    "    num_features = len(X[0])\n",
    "    print(\"%s features\" % (num_features))\n",
    "\n",
    "    # count the classes\n",
    "    largest_class = \"none\"\n",
    "    largest_class_count = -1\n",
    "\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "\n",
    "    for key in class_counts.keys():\n",
    "        count = class_counts[key]\n",
    "        if count > largest_class_count:\n",
    "            largest_class_count = count\n",
    "            largest_class = key\n",
    "\n",
    "    # raw data stats            \n",
    "    print(\"Raw observations:\")\n",
    "    print(\"%s observations\" % (len(y)))\n",
    "    print(\"Class counts:\")\n",
    "    for cl in class_counts.keys():\n",
    "        print(\"%s - %s\" % (cl, class_counts[cl]))\n",
    "\n",
    "    # if desired, use oversampling for any class that has less than 75% of the observations\n",
    "    # of the largest class\n",
    "    if oversample == True:\n",
    "        print(\"\\nOversampling enabled\")\n",
    "        print(\"Largest class is \" + largest_class + \" with %s observations\" % (largest_class_count))\n",
    "\n",
    "        for cl in class_counts.keys():\n",
    "            if class_counts[cl] < 0.8 * largest_class_count:\n",
    "                # oversample\n",
    "                X, y = oversample(X, y, cl, largest_class_count)\n",
    "\n",
    "        class_counts = {}\n",
    "        for cl in classes:\n",
    "            class_counts[cl] = y.count(cl)\n",
    "\n",
    "        print(\"\\nObservations after oversampling:\")\n",
    "        for cl in class_counts.keys():\n",
    "            print(\"%s - %s\" % (cl, class_counts[cl]))\n",
    "    \n",
    "    return X, y, y_numeric\n",
    "\n",
    "\n",
    "# this function taken from:\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() * 0.75\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{0:.4f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "\n",
    "    \n",
    "def oversample(X, y, cl, largest_class):\n",
    "    \"\"\"\n",
    "    returns new X and y lists with oversampling\n",
    "    \"\"\"\n",
    "    X_new = list(X)\n",
    "    y_new = list(y)\n",
    "    \n",
    "    # first count each class in y\n",
    "    class_count = y.count(cl)\n",
    "    obs_add = largest_class - class_count\n",
    "    print(\"oversample - %s samples with %s class in y, adding %s observations\" % (class_count, cl, obs_add))\n",
    "    \n",
    "    # keep adding samples for the class\n",
    "    obs_added = 0\n",
    "    class_index = 0\n",
    "    all_in_class = [i for i, x in enumerate(y) if x == cl ] # => [1, 4, 6], all indexes for the class\n",
    "    assert len(all_in_class) > 0\n",
    "    \n",
    "    # take observations from the class sequentially, looping around when off the end\n",
    "    while obs_added < obs_add:\n",
    "        index = all_in_class[class_index]\n",
    "        \n",
    "        y_new.append(y[index])\n",
    "        X_new.append(X[index])\n",
    "        \n",
    "        obs_added += 1\n",
    "        \n",
    "        class_index += 1\n",
    "        if class_index >= len(all_in_class):\n",
    "            class_index = 0\n",
    "    \n",
    "    return X_new, y_new\n",
    "\n",
    "\n",
    "def next_batch(X, y, offset, step):\n",
    "    \"\"\"\n",
    "    returns a batch of observations and new offset, given offset and step\n",
    "    if the batch will run off the end, loops back around\n",
    "    \"\"\"\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "    if offset + step >= len(X):\n",
    "        new_offset = offset + step - len(X)\n",
    "        X_batch = list(X[offset:])\n",
    "        X_batch.extend(list(X[:new_offset]))\n",
    "        y_batch = list(y[offset:])\n",
    "        y_batch.extend(list(y[:new_offset]))\n",
    "    else:\n",
    "        new_offset = offset + step\n",
    "        X_batch = X[offset:offset + step]\n",
    "        y_batch = y[offset:offset + step]\n",
    "    \n",
    "    return X_batch, y_batch, new_offset\n",
    "\n",
    "\n",
    "def one_hot(y, classes):\n",
    "    \"\"\"\n",
    "    takes as input a list of response values as strings, returns\n",
    "    a one-hot matrix given the class ordering provided, and the map\n",
    "    to return the index to classes\n",
    "    \"\"\"\n",
    "    class_map = {}\n",
    "    for i, cl in enumerate(classes):\n",
    "        class_map[i] = cl\n",
    "        \n",
    "    one_hot_matrix = []\n",
    "    for response in y:\n",
    "        row = [0] * len(classes)\n",
    "        row[classes.index(response)] = 1\n",
    "        one_hot_matrix.append(row)\n",
    "    \n",
    "    one_hot_matrix = np.array(one_hot_matrix)\n",
    "    return one_hot_matrix, class_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############\n",
      "RGB 32x32 Data summary:\n",
      "3072 features\n",
      "Raw observations:\n",
      "3347 observations\n",
      "Class counts:\n",
      "ship - 2347\n",
      "ocean - 504\n",
      "sky - 496\n",
      "\n",
      "##############\n",
      "Grayscale 28x28 Data summary:\n",
      "784 features\n",
      "Raw observations:\n",
      "3347 observations\n",
      "Class counts:\n",
      "ship - 2347\n",
      "ocean - 504\n",
      "sky - 496\n"
     ]
    }
   ],
   "source": [
    "# import data from data.csv\n",
    "y_rgb = []\n",
    "X_rgb = []\n",
    "y_rgb_numeric = []\n",
    "\n",
    "y_gray = []\n",
    "X_gray = []\n",
    "y_gray_numeric = []\n",
    "\n",
    "X_rgb, y_rgb, y_rgb_numeric = prepare_data(\"data_rgb.csv\", \"RGB 32x32\")\n",
    "X_gray, y_gray, y_gray_numeric = prepare_data(\"data_gray.csv\", \"Grayscale 28x28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test sets\n",
    "# first method - split data set into two random sets\n",
    "# randomly select from the original data into training and test sets\n",
    "\n",
    "# validation sets\n",
    "# TODO need to cross-validate the size of the train/test sets\n",
    "test_set_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "def generate_validation_sets(X, y, test_size):\n",
    "    \"\"\"\n",
    "    generates training and test sets using validation set split, given the test_size variable,\n",
    "    which is a float between 0 and 1\n",
    "    \"\"\"\n",
    "    print(\"generating train/test split with test size = %s\" % (test_size))\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, \n",
    "                                                                        y, \n",
    "                                                                        test_size=test_size, \n",
    "                                                                        random_state=0)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "Inputs: number of neighbors, training and test sets\n",
    "\n",
    "Outputs: accuracy score, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "test set size = 0.1\n",
      "generating train/test split with test size = 0.1\n",
      "generating train/test split with test size = 0.1\n",
      "Color:\n",
      "max k = 27\n",
      "k = 1, metric = (1, 0.79104477611940294, array([[ 28,  11,   9],\n",
      "       [ 18, 206,  12],\n",
      "       [ 14,   6,  31]])) (9.84349285254575s)\n",
      "k = 5, metric = (5, 0.76716417910447765, array([[ 29,  11,   8],\n",
      "       [ 23, 204,   9],\n",
      "       [ 15,  12,  24]])) (10.159136654683607s)\n",
      "k = 10, metric = (10, 0.77313432835820894, array([[ 25,  15,   8],\n",
      "       [ 18, 209,   9],\n",
      "       [ 12,  14,  25]])) (10.378653267278423s)\n",
      "k = 15, metric = (15, 0.77014925373134324, array([[ 23,  19,   6],\n",
      "       [ 18, 211,   7],\n",
      "       [  9,  18,  24]])) (10.392737476709954s)\n",
      "k = 20, metric = (20, 0.76119402985074625, array([[ 20,  22,   6],\n",
      "       [ 16, 214,   6],\n",
      "       [  8,  22,  21]])) (10.379731513154184s)\n",
      "k = 25, metric = (25, 0.74925373134328355, array([[ 18,  23,   7],\n",
      "       [ 16, 213,   7],\n",
      "       [ 11,  20,  20]])) (10.497039049887462s)\n",
      "Grayscale:\n",
      "max k = 27\n",
      "k = 1, metric = (1, 0.77910447761194035, array([[ 26,  11,  11],\n",
      "       [ 17, 203,  16],\n",
      "       [ 14,   5,  32]])) (2.4423823679626366s)\n",
      "k = 5, metric = (5, 0.75223880597014925, array([[ 24,  16,   8],\n",
      "       [ 15, 205,  16],\n",
      "       [ 14,  14,  23]])) (2.610842287524065s)\n",
      "k = 10, metric = (10, 0.75820895522388054, array([[ 23,  19,   6],\n",
      "       [ 15, 210,  11],\n",
      "       [  9,  21,  21]])) (2.6371992412441614s)\n",
      "k = 15, metric = (15, 0.74029850746268655, array([[ 17,  24,   7],\n",
      "       [ 12, 212,  12],\n",
      "       [  8,  24,  19]])) (2.6602121838168387s)\n",
      "k = 20, metric = (20, 0.72835820895522385, array([[ 16,  26,   6],\n",
      "       [ 14, 210,  12],\n",
      "       [  7,  26,  18]])) (2.703924307865236s)\n",
      "k = 25, metric = (25, 0.73134328358208955, array([[ 14,  28,   6],\n",
      "       [ 12, 214,  10],\n",
      "       [  5,  29,  17]])) (2.6903058692569175s)\n",
      "\n",
      "####################\n",
      "test set size = 0.2\n",
      "generating train/test split with test size = 0.2\n",
      "generating train/test split with test size = 0.2\n",
      "Color:\n",
      "max k = 25\n",
      "k = 1, metric = (1, 0.77761194029850744, array([[ 52,  23,  21],\n",
      "       [ 36, 405,  29],\n",
      "       [ 26,  14,  64]])) (17.526142632730625s)\n",
      "k = 5, metric = (5, 0.76716417910447765, array([[ 54,  27,  15],\n",
      "       [ 47, 403,  20],\n",
      "       [ 30,  17,  57]])) (18.614864579321875s)\n",
      "k = 10, metric = (10, 0.76865671641791045, array([[ 49,  33,  14],\n",
      "       [ 43, 410,  17],\n",
      "       [ 25,  23,  56]])) (18.708062212250297s)\n",
      "k = 15, metric = (15, 0.78656716417910444, array([[ 47,  33,  16],\n",
      "       [ 33, 422,  15],\n",
      "       [ 19,  27,  58]])) (19.187815821598633s)\n",
      "k = 20, metric = (20, 0.76268656716417915, array([[ 44,  39,  13],\n",
      "       [ 37, 418,  15],\n",
      "       [ 24,  31,  49]])) (19.02800782905433s)\n",
      "k = 25, metric = (25, 0.76716417910447765, array([[ 44,  38,  14],\n",
      "       [ 38, 420,  12],\n",
      "       [ 21,  33,  50]])) (19.13646300336768s)\n",
      "Grayscale:\n",
      "max k = 25\n",
      "k = 1, metric = (1, 0.75820895522388054, array([[ 52,  24,  20],\n",
      "       [ 36, 397,  37],\n",
      "       [ 32,  13,  59]])) (4.630045749983765s)\n",
      "k = 5, metric = (5, 0.75522388059701495, array([[ 46,  29,  21],\n",
      "       [ 30, 407,  33],\n",
      "       [ 28,  23,  53]])) (4.810274608578766s)\n",
      "k = 10, metric = (10, 0.77014925373134324, array([[ 40,  39,  17],\n",
      "       [ 24, 422,  24],\n",
      "       [ 17,  33,  54]])) (4.866996922230101s)\n",
      "k = 15, metric = (15, 0.75970149253731345, array([[ 33,  47,  16],\n",
      "       [ 21, 425,  24],\n",
      "       [ 18,  35,  51]])) (4.90000808986224s)\n",
      "k = 20, metric = (20, 0.75820895522388054, array([[ 34,  52,  10],\n",
      "       [ 22, 424,  24],\n",
      "       [ 16,  38,  50]])) (4.937682399237019s)\n",
      "k = 25, metric = (25, 0.76119402985074625, array([[ 32,  54,  10],\n",
      "       [ 19, 431,  20],\n",
      "       [ 12,  45,  47]])) (5.090177216290613s)\n",
      "\n",
      "####################\n",
      "test set size = 0.3\n",
      "generating train/test split with test size = 0.3\n",
      "generating train/test split with test size = 0.3\n",
      "Color:\n",
      "max k = 24\n",
      "k = 1, metric = (1, 0.76517412935323381, array([[ 83,  34,  33],\n",
      "       [ 63, 592,  40],\n",
      "       [ 47,  19,  94]])) (24.23268739201012s)\n",
      "k = 5, metric = (5, 0.76815920398009951, array([[ 88,  40,  22],\n",
      "       [ 73, 596,  26],\n",
      "       [ 47,  25,  88]])) (25.782564112161708s)\n",
      "k = 10, metric = (10, 0.75721393034825868, array([[ 81,  49,  20],\n",
      "       [ 78, 598,  19],\n",
      "       [ 44,  34,  82]])) (25.821167548299854s)\n",
      "k = 15, metric = (15, 0.76218905472636811, array([[ 73,  54,  23],\n",
      "       [ 69, 610,  16],\n",
      "       [ 38,  39,  83]])) (25.9017830298817s)\n",
      "k = 20, metric = (20, 0.75721393034825868, array([[ 70,  59,  21],\n",
      "       [ 67, 615,  13],\n",
      "       [ 41,  43,  76]])) (26.083964624358487s)\n",
      "Grayscale:\n",
      "max k = 24\n",
      "k = 1, metric = (1, 0.74029850746268655, array([[ 76,  39,  35],\n",
      "       [ 60, 588,  47],\n",
      "       [ 49,  31,  80]])) (6.179653814249832s)\n",
      "k = 5, metric = (5, 0.75422885572139309, array([[ 62,  58,  30],\n",
      "       [ 48, 614,  33],\n",
      "       [ 33,  45,  82]])) (6.602700202969572s)\n",
      "k = 10, metric = (10, 0.75223880597014925, array([[ 56,  73,  21],\n",
      "       [ 45, 626,  24],\n",
      "       [ 29,  57,  74]])) (6.62545867710287s)\n",
      "k = 15, metric = (15, 0.75522388059701495, array([[ 54,  76,  20],\n",
      "       [ 40, 633,  22],\n",
      "       [ 22,  66,  72]])) (6.704400945669477s)\n",
      "k = 20, metric = (20, 0.75422885572139309, array([[ 55,  76,  19],\n",
      "       [ 41, 634,  20],\n",
      "       [ 23,  68,  69]])) (6.709007941348318s)\n",
      "\n",
      "####################\n",
      "test set size = 0.4\n",
      "generating train/test split with test size = 0.4\n",
      "generating train/test split with test size = 0.4\n",
      "Color:\n",
      "max k = 22\n",
      "k = 1, metric = (1, 0.74906646751306949, array([[108,  46,  43],\n",
      "       [ 91, 785,  58],\n",
      "       [ 69,  29, 110]])) (29.48996162748699s)\n",
      "k = 5, metric = (5, 0.76325616131441376, array([[108,  61,  28],\n",
      "       [ 93, 809,  32],\n",
      "       [ 64,  39, 105]])) (31.202794141974664s)\n",
      "k = 10, metric = (10, 0.75728155339805825, array([[103,  66,  28],\n",
      "       [ 95, 809,  30],\n",
      "       [ 61,  45, 102]])) (31.709995327199067s)\n",
      "k = 15, metric = (15, 0.76400298730395821, array([[103,  67,  27],\n",
      "       [ 84, 825,  25],\n",
      "       [ 59,  54,  95]])) (31.928538439524345s)\n",
      "k = 20, metric = (20, 0.7610156833457804, array([[ 96,  73,  28],\n",
      "       [ 84, 831,  19],\n",
      "       [ 56,  60,  92]])) (31.972413266408694s)\n",
      "Grayscale:\n",
      "max k = 22\n",
      "k = 1, metric = (1, 0.73114264376400295, array([[ 96,  60,  41],\n",
      "       [ 83, 785,  66],\n",
      "       [ 64,  46,  98]])) (7.48112435809162s)\n",
      "k = 5, metric = (5, 0.74831964152352504, array([[ 79,  82,  36],\n",
      "       [ 63, 826,  45],\n",
      "       [ 46,  65,  97]])) (8.071447373029514s)\n",
      "k = 10, metric = (10, 0.75056011949215828, array([[ 74,  93,  30],\n",
      "       [ 63, 843,  28],\n",
      "       [ 47,  73,  88]])) (8.203756473398244s)\n",
      "k = 15, metric = (15, 0.75504107542942489, array([[ 69, 103,  25],\n",
      "       [ 51, 854,  29],\n",
      "       [ 39,  81,  88]])) (8.248015870620293s)\n",
      "k = 20, metric = (20, 0.75504107542942489, array([[ 70, 108,  19],\n",
      "       [ 51, 858,  25],\n",
      "       [ 38,  87,  83]])) (8.288391831894842s)\n",
      "\n",
      "####################\n",
      "test set size = 0.5\n",
      "generating train/test split with test size = 0.5\n",
      "generating train/test split with test size = 0.5\n",
      "Color:\n",
      "max k = 20\n",
      "k = 1, metric = (1, 0.75507765830346474, array([[143,  64,  44],\n",
      "       [113, 993,  57],\n",
      "       [ 90,  42, 128]])) (24.628265678333264s)\n",
      "k = 5, metric = (5, 0.77060931899641572, array([[ 138,   81,   32],\n",
      "       [ 107, 1029,   27],\n",
      "       [  83,   54,  123]])) (25.718627029556956s)\n",
      "k = 10, metric = (10, 0.77598566308243733, array([[ 134,   88,   29],\n",
      "       [ 100, 1042,   21],\n",
      "       [  73,   64,  123]])) (26.047406293433596s)\n",
      "k = 15, metric = (15, 0.77419354838709675, array([[ 132,   86,   33],\n",
      "       [ 105, 1044,   14],\n",
      "       [  66,   74,  120]])) (26.099620319801033s)\n",
      "k = 20, metric = (20, 0.77897252090800473, array([[ 127,   94,   30],\n",
      "       [  92, 1059,   12],\n",
      "       [  61,   81,  118]])) (26.091468273854844s)\n",
      "Grayscale:\n",
      "max k = 20\n",
      "k = 1, metric = (1, 0.74492234169653526, array([[131,  76,  44],\n",
      "       [ 94, 995,  74],\n",
      "       [ 75,  64, 121]])) (6.285881257717847s)\n",
      "k = 5, metric = (5, 0.75328554360812428, array([[ 112,  105,   34],\n",
      "       [  80, 1041,   42],\n",
      "       [  64,   88,  108]])) (6.586249104984745s)\n",
      "k = 10, metric = (10, 0.7514934289127837, array([[ 101,  121,   29],\n",
      "       [  74, 1057,   32],\n",
      "       [  58,  102,  100]])) (6.6255727803873015s)\n",
      "k = 15, metric = (15, 0.74671445639187572, array([[  96,  130,   25],\n",
      "       [  71, 1066,   26],\n",
      "       [  51,  121,   88]])) (6.657474308649398s)\n",
      "k = 20, metric = (20, 0.74970131421744324, array([[  93,  137,   21],\n",
      "       [  61, 1077,   25],\n",
      "       [  44,  131,   85]])) (6.684318767154764s)\n",
      "\n",
      "####################\n",
      "test set size = 0.6\n",
      "generating train/test split with test size = 0.6\n",
      "generating train/test split with test size = 0.6\n",
      "Color:\n",
      "max k = 18\n",
      "k = 1, metric = (1, 0.7685415629666501, array([[ 167,   86,   56],\n",
      "       [ 125, 1201,   72],\n",
      "       [  75,   51,  176]])) (25.821847338254884s)\n",
      "k = 5, metric = (5, 0.77551020408163263, array([[ 164,   98,   47],\n",
      "       [ 120, 1237,   41],\n",
      "       [  81,   64,  157]])) (26.8897792433454s)\n",
      "k = 10, metric = (10, 0.77799900447984072, array([[ 155,  116,   38],\n",
      "       [ 108, 1261,   29],\n",
      "       [  77,   78,  147]])) (27.168426413041743s)\n",
      "k = 15, metric = (15, 0.77302140368342454, array([[ 149,  120,   40],\n",
      "       [ 115, 1262,   21],\n",
      "       [  71,   89,  142]])) (27.36163376587865s)\n",
      "Grayscale:\n",
      "max k = 18\n",
      "k = 1, metric = (1, 0.74166251866600297, array([[ 152,   96,   61],\n",
      "       [ 114, 1175,  109],\n",
      "       [  72,   67,  163]])) (6.4376271587534575s)\n",
      "k = 5, metric = (5, 0.74664011946241915, array([[ 125,  143,   41],\n",
      "       [  93, 1242,   63],\n",
      "       [  67,  102,  133]])) (7.0545169204924605s)\n",
      "k = 10, metric = (10, 0.75311100049776003, array([[ 114,  161,   34],\n",
      "       [  80, 1273,   45],\n",
      "       [  57,  119,  126]])) (6.8974365398571535s)\n",
      "k = 15, metric = (15, 0.75460428073668495, array([[ 111,  173,   25],\n",
      "       [  67, 1295,   36],\n",
      "       [  47,  145,  110]])) (6.931559459888376s)\n",
      "\n",
      "####################\n",
      "test set size = 0.7\n",
      "generating train/test split with test size = 0.7\n",
      "generating train/test split with test size = 0.7\n",
      "Color:\n",
      "max k = 15\n",
      "k = 1, metric = (1, 0.74989329918907388, array([[ 180,  117,   70],\n",
      "       [ 155, 1380,   89],\n",
      "       [  79,   76,  197]])) (26.106704383135366s)\n",
      "k = 5, metric = (5, 0.77294067434912506, array([[ 185,  129,   53],\n",
      "       [ 122, 1444,   58],\n",
      "       [  83,   87,  182]])) (27.196567060735106s)\n",
      "k = 10, metric = (10, 0.78147673922321814, array([[ 172,  150,   45],\n",
      "       [ 110, 1488,   26],\n",
      "       [  69,  112,  171]])) (28.127173848733946s)\n",
      "k = 15, metric = (15, 0.77763551002987619, array([[ 153,  170,   44],\n",
      "       [  82, 1514,   28],\n",
      "       [  66,  131,  155]])) (27.390672146844736s)\n",
      "Grayscale:\n",
      "max k = 15\n",
      "k = 1, metric = (1, 0.72343149807938545, array([[ 159,  136,   72],\n",
      "       [ 136, 1357,  131],\n",
      "       [  66,  107,  179]])) (6.2928904596374196s)\n",
      "k = 5, metric = (5, 0.74647887323943662, array([[ 136,  187,   44],\n",
      "       [  91, 1464,   69],\n",
      "       [  63,  140,  149]])) (6.6479792851205275s)\n",
      "k = 10, metric = (10, 0.75245411865130174, array([[ 121,  209,   37],\n",
      "       [  77, 1502,   45],\n",
      "       [  56,  156,  140]])) (6.718130468958407s)\n",
      "k = 15, metric = (15, 0.75160051216389245, array([[ 105,  232,   30],\n",
      "       [  54, 1534,   36],\n",
      "       [  43,  187,  122]])) (6.762982116575586s)\n",
      "\n",
      "####################\n",
      "test set size = 0.8\n",
      "generating train/test split with test size = 0.8\n",
      "generating train/test split with test size = 0.8\n",
      "Color:\n",
      "max k = 12\n",
      "k = 1, metric = (1, 0.75168035847647496, array([[ 194,  146,   75],\n",
      "       [ 166, 1607,   88],\n",
      "       [  86,  104,  212]])) (17.00803009428637s)\n",
      "k = 5, metric = (5, 0.76848394324122482, array([[ 177,  184,   54],\n",
      "       [ 111, 1695,   55],\n",
      "       [  89,  127,  186]])) (17.561015072500595s)\n",
      "k = 10, metric = (10, 0.76997759522031362, array([[ 143,  227,   45],\n",
      "       [  75, 1746,   40],\n",
      "       [  69,  160,  173]])) (17.665810791393596s)\n",
      "Grayscale:\n",
      "max k = 12\n",
      "k = 1, metric = (1, 0.72068707991038083, array([[ 176,  170,   69],\n",
      "       [ 150, 1577,  134],\n",
      "       [  82,  143,  177]])) (4.02285567322906s)\n",
      "k = 5, metric = (5, 0.73711725168035847, array([[ 149,  219,   47],\n",
      "       [ 110, 1678,   73],\n",
      "       [  67,  188,  147]])) (4.163977290427283s)\n",
      "k = 10, metric = (10, 0.73711725168035847, array([[ 114,  272,   29],\n",
      "       [  70, 1743,   48],\n",
      "       [  50,  235,  117]])) (4.203997056254593s)\n",
      "\n",
      "####################\n",
      "test set size = 0.9\n",
      "generating train/test split with test size = 0.9\n",
      "generating train/test split with test size = 0.9\n",
      "Color:\n",
      "max k = 9\n",
      "k = 1, metric = (1, 0.7314968469963492, array([[ 230,  152,   81],\n",
      "       [ 189, 1758,  157],\n",
      "       [  98,  132,  216]])) (9.178154171047936s)\n",
      "k = 5, metric = (5, 0.75705277132426152, array([[ 204,  209,   50],\n",
      "       [ 143, 1889,   72],\n",
      "       [  93,  165,  188]])) (9.397804206004366s)\n",
      "Grayscale:\n",
      "max k = 9\n",
      "k = 1, metric = (1, 0.67208762031198144, array([[ 176,  215,   72],\n",
      "       [ 170, 1667,  267],\n",
      "       [  94,  170,  182]])) (2.224377171132801s)\n",
      "k = 5, metric = (5, 0.71589777630268836, array([[ 133,  290,   40],\n",
      "       [ 101, 1899,  104],\n",
      "       [  76,  245,  125]])) (2.2744588545938313s)\n"
     ]
    }
   ],
   "source": [
    "def knn(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Performs K-nearest Neighbors with provided parameters, returns a tuple containing\n",
    "    k, accuracy, precision, confusion matrix\n",
    "    \"\"\"\n",
    "    # record metrics for the cross-validated value - n-neighbors\n",
    "    # format of knn_metrics is [index, neighbors, score, confusion matrix]\n",
    "    # score is the mean accuracy and is provided by the score function of the classifier\n",
    "    knn_metrics = pd.DataFrame(columns=('neighbors', 'mean_accuracy', 'confusion matrix'))\n",
    "    \n",
    "    max_k = int(math.sqrt(len(X_train))/2)\n",
    "    print(\"max k = \" + str(max_k))\n",
    "\n",
    "    # TODO - look into parallelizing this\n",
    "    i = 0\n",
    "    for k in range(1, max_k + 1):\n",
    "        start = timer()\n",
    "        \n",
    "        classifier = neighbors.KNeighborsClassifier(k, 'distance')\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        y_prediction = classifier.predict(X_test)\n",
    "        cm = metrics.confusion_matrix(y_test, y_prediction, labels=classes)\n",
    "        mean_accuracy = classifier.score(X_test, y_test)\n",
    "        \n",
    "        end = timer()\n",
    "        \n",
    "        metric = (k, mean_accuracy, cm)\n",
    "        if k == 1 or k%5 == 0:\n",
    "            print(\"k = %s, metric = %s (%ss)\" % (k, metric, end - start))\n",
    "        knn_metrics.loc[i] = metric\n",
    "        i += 1\n",
    "    \n",
    "    return knn_metrics\n",
    "\n",
    "knn_metrics_rgb = {}\n",
    "knn_metrics_gray = {}\n",
    "\n",
    "for t in test_set_sizes:\n",
    "    print(\"\\n####################\\ntest set size = %s\" % (t))\n",
    "    # generate validation sets\n",
    "    X_train_rgb, y_train_rgb, X_test_rgb, y_test_rgb = generate_validation_sets(X_rgb, y_rgb, t)\n",
    "    X_train_gray, y_train_gray, X_test_gray, y_test_gray = generate_validation_sets(X_gray, y_gray, t)\n",
    "    \n",
    "    # generate metrics for knn for rgb and gray-scale\n",
    "    print(\"Color:\")\n",
    "    knn_metrics_rgb[t] = knn(X_train_rgb, y_train_rgb, X_test_rgb, y_test_rgb)\n",
    "    print(\"Grayscale:\")\n",
    "    knn_metrics_gray[t] = knn(X_train_gray, y_train_gray, X_test_gray, y_test_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.1:     neighbors  mean_accuracy                             confusion matrix\n",
      "0         1.0       0.791045    [[28, 11, 9], [18, 206, 12], [14, 6, 31]]\n",
      "1         2.0       0.791045    [[28, 11, 9], [18, 206, 12], [14, 6, 31]]\n",
      "2         3.0       0.779104     [[28, 11, 9], [20, 207, 9], [16, 9, 26]]\n",
      "3         4.0       0.773134  [[28, 10, 10], [21, 205, 10], [15, 10, 26]]\n",
      "4         5.0       0.767164    [[29, 11, 8], [23, 204, 9], [15, 12, 24]]\n",
      "5         6.0       0.764179    [[26, 13, 9], [24, 204, 8], [13, 12, 26]]\n",
      "6         7.0       0.761194  [[25, 12, 11], [21, 205, 10], [13, 13, 25]]\n",
      "7         8.0       0.758209   [[25, 17, 6], [21, 205, 10], [13, 14, 24]]\n",
      "8         9.0       0.773134   [[26, 14, 8], [16, 210, 10], [14, 14, 23]]\n",
      "9        10.0       0.773134    [[25, 15, 8], [18, 209, 9], [12, 14, 25]]\n",
      "10       11.0       0.779104    [[25, 16, 7], [18, 209, 9], [10, 14, 27]]\n",
      "11       12.0       0.758209   [[24, 16, 8], [19, 207, 10], [12, 16, 23]]\n",
      "12       13.0       0.767164    [[24, 19, 5], [17, 211, 8], [12, 17, 22]]\n",
      "13       14.0       0.776119    [[25, 18, 5], [17, 211, 8], [10, 17, 24]]\n",
      "14       15.0       0.770149     [[23, 19, 6], [18, 211, 7], [9, 18, 24]]\n",
      "15       16.0       0.761194     [[21, 21, 6], [15, 212, 9], [8, 21, 22]]\n",
      "16       17.0       0.767164     [[20, 22, 6], [15, 213, 8], [9, 18, 24]]\n",
      "17       18.0       0.755224     [[18, 24, 6], [17, 212, 7], [8, 20, 23]]\n",
      "18       19.0       0.761194     [[19, 23, 6], [16, 213, 7], [8, 20, 23]]\n",
      "19       20.0       0.761194     [[20, 22, 6], [16, 214, 6], [8, 22, 21]]\n",
      "20       21.0       0.758209    [[20, 22, 6], [15, 214, 7], [10, 21, 20]]\n",
      "21       22.0       0.758209     [[19, 23, 6], [15, 214, 7], [8, 22, 21]]\n",
      "22       23.0       0.743284    [[18, 24, 6], [15, 213, 8], [10, 23, 18]]\n",
      "23       24.0       0.746269    [[17, 24, 7], [16, 213, 7], [10, 21, 20]]\n",
      "24       25.0       0.749254    [[18, 23, 7], [16, 213, 7], [11, 20, 20]]\n",
      "25       26.0       0.752239    [[19, 23, 6], [15, 213, 8], [10, 21, 20]]\n",
      "26       27.0       0.749254    [[19, 22, 7], [16, 213, 7], [10, 22, 19]], 0.2:     neighbors  mean_accuracy                             confusion matrix\n",
      "0         1.0       0.777612  [[52, 23, 21], [36, 405, 29], [26, 14, 64]]\n",
      "1         2.0       0.777612  [[52, 23, 21], [36, 405, 29], [26, 14, 64]]\n",
      "2         3.0       0.773134  [[53, 25, 18], [38, 407, 25], [30, 16, 58]]\n",
      "3         4.0       0.767164  [[53, 26, 17], [42, 405, 23], [32, 16, 56]]\n",
      "4         5.0       0.767164  [[54, 27, 15], [47, 403, 20], [30, 17, 57]]\n",
      "5         6.0       0.770149  [[53, 29, 14], [44, 407, 19], [29, 19, 56]]\n",
      "6         7.0       0.771642  [[53, 28, 15], [46, 405, 19], [25, 20, 59]]\n",
      "7         8.0       0.767164  [[50, 31, 15], [44, 408, 18], [26, 22, 56]]\n",
      "8         9.0       0.767164  [[49, 31, 16], [42, 410, 18], [27, 22, 55]]\n",
      "9        10.0       0.768657  [[49, 33, 14], [43, 410, 17], [25, 23, 56]]\n",
      "10       11.0       0.765672  [[48, 33, 15], [44, 408, 18], [23, 24, 57]]\n",
      "11       12.0       0.767164  [[48, 34, 14], [41, 409, 20], [21, 26, 57]]\n",
      "12       13.0       0.774627  [[46, 36, 14], [35, 418, 17], [22, 27, 55]]\n",
      "13       14.0       0.786567  [[50, 32, 14], [35, 419, 16], [20, 26, 58]]\n",
      "14       15.0       0.786567  [[47, 33, 16], [33, 422, 15], [19, 27, 58]]\n",
      "15       16.0       0.776119  [[45, 36, 15], [35, 420, 15], [19, 30, 55]]\n",
      "16       17.0       0.780597  [[46, 36, 14], [32, 423, 15], [22, 28, 54]]\n",
      "17       18.0       0.774627  [[47, 35, 14], [38, 421, 11], [20, 33, 51]]\n",
      "18       19.0       0.765672  [[46, 37, 13], [38, 419, 13], [23, 33, 48]]\n",
      "19       20.0       0.762687  [[44, 39, 13], [37, 418, 15], [24, 31, 49]]\n",
      "20       21.0       0.762687  [[44, 38, 14], [37, 419, 14], [23, 33, 48]]\n",
      "21       22.0       0.764179  [[43, 39, 14], [36, 420, 14], [22, 33, 49]]\n",
      "22       23.0       0.761194  [[43, 38, 15], [39, 417, 14], [23, 31, 50]]\n",
      "23       24.0       0.761194  [[42, 40, 14], [38, 418, 14], [22, 32, 50]]\n",
      "24       25.0       0.767164  [[44, 38, 14], [38, 420, 12], [21, 33, 50]], 0.3:     neighbors  mean_accuracy                             confusion matrix\n",
      "0         1.0       0.765174  [[83, 34, 33], [63, 592, 40], [47, 19, 94]]\n",
      "1         2.0       0.765174  [[83, 34, 33], [63, 592, 40], [47, 19, 94]]\n",
      "2         3.0       0.768159  [[87, 39, 24], [65, 595, 35], [48, 22, 90]]\n",
      "3         4.0       0.760199  [[84, 39, 27], [73, 592, 30], [51, 21, 88]]\n",
      "4         5.0       0.768159  [[88, 40, 22], [73, 596, 26], [47, 25, 88]]\n",
      "5         6.0       0.759204  [[87, 39, 24], [74, 596, 25], [50, 30, 80]]\n",
      "6         7.0       0.761194  [[82, 47, 21], [77, 597, 21], [45, 29, 86]]\n",
      "7         8.0       0.767164  [[85, 44, 21], [76, 600, 19], [47, 27, 86]]\n",
      "8         9.0       0.762189  [[78, 49, 23], [75, 601, 19], [44, 29, 87]]\n",
      "9        10.0       0.757214  [[81, 49, 20], [78, 598, 19], [44, 34, 82]]\n",
      "10       11.0       0.757214  [[75, 51, 24], [66, 605, 24], [43, 36, 81]]\n",
      "11       12.0       0.763184  [[76, 50, 24], [70, 605, 20], [40, 34, 86]]\n",
      "12       13.0       0.761194  [[71, 55, 24], [69, 610, 16], [41, 35, 84]]\n",
      "13       14.0       0.763184  [[75, 51, 24], [70, 608, 17], [38, 38, 84]]\n",
      "14       15.0       0.762189  [[73, 54, 23], [69, 610, 16], [38, 39, 83]]\n",
      "15       16.0       0.764179  [[75, 54, 21], [66, 613, 16], [37, 43, 80]]\n",
      "16       17.0       0.758209  [[72, 56, 22], [65, 611, 19], [39, 42, 79]]\n",
      "17       18.0       0.765174  [[71, 57, 22], [64, 616, 15], [37, 41, 82]]\n",
      "18       19.0       0.758209  [[69, 59, 22], [65, 616, 14], [38, 45, 77]]\n",
      "19       20.0       0.757214  [[70, 59, 21], [67, 615, 13], [41, 43, 76]]\n",
      "20       21.0       0.757214  [[69, 58, 23], [64, 617, 14], [40, 45, 75]]\n",
      "21       22.0       0.760199  [[70, 57, 23], [62, 619, 14], [42, 43, 75]]\n",
      "22       23.0       0.761194  [[71, 58, 21], [61, 619, 15], [41, 44, 75]]\n",
      "23       24.0       0.759204  [[70, 60, 20], [63, 618, 14], [42, 43, 75]], 0.6:     neighbors  mean_accuracy                                  confusion matrix\n",
      "0         1.0       0.768542   [[167, 86, 56], [125, 1201, 72], [75, 51, 176]]\n",
      "1         2.0       0.768542   [[167, 86, 56], [125, 1201, 72], [75, 51, 176]]\n",
      "2         3.0       0.773519   [[160, 99, 50], [119, 1225, 54], [80, 53, 169]]\n",
      "3         4.0       0.776506   [[170, 98, 41], [121, 1227, 50], [87, 52, 163]]\n",
      "4         5.0       0.775510   [[164, 98, 47], [120, 1237, 41], [81, 64, 157]]\n",
      "5         6.0       0.782479  [[160, 101, 48], [113, 1247, 38], [76, 61, 165]]\n",
      "6         7.0       0.780488  [[159, 107, 43], [112, 1249, 37], [75, 67, 160]]\n",
      "7         8.0       0.776506  [[155, 110, 44], [116, 1250, 32], [77, 70, 155]]\n",
      "8         9.0       0.780488  [[163, 105, 41], [114, 1254, 30], [76, 75, 151]]\n",
      "9        10.0       0.777999  [[155, 116, 38], [108, 1261, 29], [77, 78, 147]]\n",
      "10       11.0       0.777999  [[156, 116, 37], [110, 1259, 29], [75, 79, 148]]\n",
      "11       12.0       0.780488  [[157, 113, 39], [107, 1261, 30], [71, 81, 150]]\n",
      "12       13.0       0.781483  [[155, 114, 40], [105, 1267, 26], [70, 84, 148]]\n",
      "13       14.0       0.778497  [[154, 112, 43], [106, 1265, 27], [73, 84, 145]]\n",
      "14       15.0       0.773021  [[149, 120, 40], [115, 1262, 21], [71, 89, 142]]\n",
      "15       16.0       0.777003  [[148, 122, 39], [108, 1272, 18], [70, 91, 141]]\n",
      "16       17.0       0.777003  [[149, 121, 39], [102, 1275, 21], [68, 97, 137]]\n",
      "17       18.0       0.778497   [[144, 126, 39], [96, 1283, 19], [67, 98, 137]], 0.8:     neighbors  mean_accuracy  \\\n",
      "0         1.0       0.751680   \n",
      "1         2.0       0.751680   \n",
      "2         3.0       0.765870   \n",
      "3         4.0       0.767364   \n",
      "4         5.0       0.768484   \n",
      "5         6.0       0.772591   \n",
      "6         7.0       0.773338   \n",
      "7         8.0       0.776699   \n",
      "8         9.0       0.777446   \n",
      "9        10.0       0.769978   \n",
      "10       11.0       0.772218   \n",
      "11       12.0       0.764003   \n",
      "\n",
      "                                     confusion matrix  \n",
      "0   [[194, 146, 75], [166, 1607, 88], [86, 104, 212]]  \n",
      "1   [[194, 146, 75], [166, 1607, 88], [86, 104, 212]]  \n",
      "2   [[181, 166, 68], [123, 1666, 72], [86, 112, 204]]  \n",
      "3   [[189, 168, 58], [135, 1668, 58], [92, 112, 198]]  \n",
      "4   [[177, 184, 54], [111, 1695, 55], [89, 127, 186]]  \n",
      "5   [[172, 190, 53], [112, 1704, 45], [83, 126, 193]]  \n",
      "6    [[162, 202, 51], [97, 1722, 42], [77, 138, 187]]  \n",
      "7    [[158, 211, 46], [92, 1730, 39], [69, 141, 192]]  \n",
      "8    [[155, 213, 47], [80, 1744, 37], [67, 152, 183]]  \n",
      "9    [[143, 227, 45], [75, 1746, 40], [69, 160, 173]]  \n",
      "10   [[142, 233, 40], [61, 1760, 40], [64, 172, 166]]  \n",
      "11   [[130, 242, 43], [67, 1757, 37], [68, 175, 159]]  , 0.5:     neighbors  mean_accuracy                                 confusion matrix\n",
      "0         1.0       0.755078   [[143, 64, 44], [113, 993, 57], [90, 42, 128]]\n",
      "1         2.0       0.755078   [[143, 64, 44], [113, 993, 57], [90, 42, 128]]\n",
      "2         3.0       0.768817  [[146, 71, 34], [110, 1010, 43], [81, 48, 131]]\n",
      "3         4.0       0.760454  [[140, 75, 36], [117, 1007, 39], [86, 48, 126]]\n",
      "4         5.0       0.770609  [[138, 81, 32], [107, 1029, 27], [83, 54, 123]]\n",
      "5         6.0       0.769415  [[141, 75, 35], [115, 1020, 28], [78, 55, 127]]\n",
      "6         7.0       0.770609  [[135, 79, 37], [110, 1030, 23], [80, 55, 125]]\n",
      "7         8.0       0.772401  [[132, 82, 37], [107, 1035, 21], [77, 57, 126]]\n",
      "8         9.0       0.771804  [[136, 81, 34], [111, 1030, 22], [71, 63, 126]]\n",
      "9        10.0       0.775986  [[134, 88, 29], [100, 1042, 21], [73, 64, 123]]\n",
      "10       11.0       0.775388  [[132, 88, 31], [100, 1045, 18], [70, 69, 121]]\n",
      "11       12.0       0.775986  [[135, 87, 29], [104, 1041, 18], [69, 68, 123]]\n",
      "12       13.0       0.774194  [[129, 89, 33], [102, 1047, 14], [66, 74, 120]]\n",
      "13       14.0       0.775388  [[131, 88, 32], [104, 1045, 14], [66, 72, 122]]\n",
      "14       15.0       0.774194  [[132, 86, 33], [105, 1044, 14], [66, 74, 120]]\n",
      "15       16.0       0.777180  [[133, 87, 31], [101, 1050, 12], [64, 78, 118]]\n",
      "16       17.0       0.773596  [[128, 92, 31], [103, 1048, 12], [65, 76, 119]]\n",
      "17       18.0       0.776583   [[128, 95, 28], [95, 1055, 13], [65, 78, 117]]\n",
      "18       19.0       0.772999  [[129, 93, 29], [100, 1051, 12], [63, 83, 114]]\n",
      "19       20.0       0.778973   [[127, 94, 30], [92, 1059, 12], [61, 81, 118]], 0.7:     neighbors  mean_accuracy  \\\n",
      "0         1.0       0.749893   \n",
      "1         2.0       0.749893   \n",
      "2         3.0       0.770807   \n",
      "3         4.0       0.771233   \n",
      "4         5.0       0.772941   \n",
      "5         6.0       0.772941   \n",
      "6         7.0       0.778489   \n",
      "7         8.0       0.781050   \n",
      "8         9.0       0.781477   \n",
      "9        10.0       0.781477   \n",
      "10       11.0       0.779770   \n",
      "11       12.0       0.776782   \n",
      "12       13.0       0.780623   \n",
      "13       14.0       0.775501   \n",
      "14       15.0       0.777636   \n",
      "\n",
      "                                     confusion matrix  \n",
      "0    [[180, 117, 70], [155, 1380, 89], [79, 76, 197]]  \n",
      "1    [[180, 117, 70], [155, 1380, 89], [79, 76, 197]]  \n",
      "2    [[184, 123, 60], [132, 1428, 64], [79, 79, 194]]  \n",
      "3    [[190, 122, 55], [136, 1427, 61], [85, 77, 190]]  \n",
      "4    [[185, 129, 53], [122, 1444, 58], [83, 87, 182]]  \n",
      "5    [[180, 138, 49], [125, 1454, 45], [83, 92, 177]]  \n",
      "6    [[178, 137, 52], [119, 1466, 39], [74, 98, 180]]  \n",
      "7    [[174, 145, 48], [116, 1473, 35], [77, 92, 183]]  \n",
      "8   [[169, 147, 51], [109, 1486, 29], [72, 104, 176]]  \n",
      "9   [[172, 150, 45], [110, 1488, 26], [69, 112, 171]]  \n",
      "10  [[169, 152, 46], [102, 1490, 32], [70, 114, 168]]  \n",
      "11  [[162, 157, 48], [102, 1494, 28], [71, 117, 164]]  \n",
      "12   [[160, 161, 46], [90, 1506, 28], [68, 121, 163]]  \n",
      "13   [[152, 170, 45], [90, 1506, 28], [69, 124, 159]]  \n",
      "14   [[153, 170, 44], [82, 1514, 28], [66, 131, 155]]  , 0.9:    neighbors  mean_accuracy                                   confusion matrix\n",
      "0        1.0       0.731497  [[230, 152, 81], [189, 1758, 157], [98, 132, 2...\n",
      "1        2.0       0.731497  [[230, 152, 81], [189, 1758, 157], [98, 132, 2...\n",
      "2        3.0       0.745768  [[200, 198, 65], [161, 1843, 100], [95, 147, 2...\n",
      "3        4.0       0.753734  [[215, 187, 61], [162, 1861, 81], [102, 149, 1...\n",
      "4        5.0       0.757053  [[204, 209, 50], [143, 1889, 72], [93, 165, 188]]\n",
      "5        6.0       0.759044  [[199, 213, 51], [139, 1901, 64], [88, 171, 187]]\n",
      "6        7.0       0.755393  [[186, 231, 46], [122, 1919, 63], [87, 188, 171]]\n",
      "7        8.0       0.762363  [[182, 241, 40], [108, 1937, 59], [87, 181, 178]]\n",
      "8        9.0       0.757717   [[162, 258, 43], [86, 1961, 57], [79, 207, 160]], 0.4:     neighbors  mean_accuracy                                confusion matrix\n",
      "0         1.0       0.749066   [[108, 46, 43], [91, 785, 58], [69, 29, 110]]\n",
      "1         2.0       0.749066   [[108, 46, 43], [91, 785, 58], [69, 29, 110]]\n",
      "2         3.0       0.766243   [[112, 52, 33], [91, 801, 42], [63, 32, 113]]\n",
      "3         4.0       0.755041  [[109, 53, 35], [103, 793, 38], [66, 33, 109]]\n",
      "4         5.0       0.763256   [[108, 61, 28], [93, 809, 32], [64, 39, 105]]\n",
      "5         6.0       0.756535   [[107, 57, 33], [98, 805, 31], [64, 43, 101]]\n",
      "6         7.0       0.759522   [[103, 63, 31], [99, 810, 25], [60, 44, 104]]\n",
      "7         8.0       0.760269   [[104, 62, 31], [99, 810, 25], [63, 41, 104]]\n",
      "8         9.0       0.759522   [[102, 67, 28], [90, 814, 30], [64, 43, 101]]\n",
      "9        10.0       0.757282   [[103, 66, 28], [95, 809, 30], [61, 45, 102]]\n",
      "10       11.0       0.753547    [[100, 67, 30], [96, 812, 26], [64, 47, 97]]\n",
      "11       12.0       0.759522    [[102, 66, 29], [91, 817, 26], [61, 49, 98]]\n",
      "12       13.0       0.761763    [[101, 66, 30], [87, 824, 23], [60, 53, 95]]\n",
      "13       14.0       0.760269    [[101, 67, 29], [86, 822, 26], [61, 52, 95]]\n",
      "14       15.0       0.764003    [[103, 67, 27], [84, 825, 25], [59, 54, 95]]\n",
      "15       16.0       0.768484    [[103, 67, 27], [85, 828, 21], [54, 56, 98]]\n",
      "16       17.0       0.761763     [[98, 73, 26], [86, 827, 21], [53, 60, 95]]\n",
      "17       18.0       0.762509     [[99, 71, 27], [84, 830, 20], [54, 62, 92]]\n",
      "18       19.0       0.757282     [[96, 74, 27], [86, 828, 20], [57, 61, 90]]\n",
      "19       20.0       0.761016     [[96, 73, 28], [84, 831, 19], [56, 60, 92]]\n",
      "20       21.0       0.768484     [[97, 74, 26], [80, 836, 18], [52, 60, 96]]\n",
      "21       22.0       0.762509     [[97, 74, 26], [81, 834, 19], [55, 63, 90]]}\n",
      "{0.1:     neighbors  mean_accuracy                            confusion matrix\n",
      "0         1.0       0.779104  [[26, 11, 11], [17, 203, 16], [14, 5, 32]]\n",
      "1         2.0       0.779104  [[26, 11, 11], [17, 203, 16], [14, 5, 32]]\n",
      "2         3.0       0.770149  [[24, 12, 12], [16, 204, 16], [13, 8, 30]]\n",
      "3         4.0       0.770149  [[22, 15, 11], [15, 206, 15], [12, 9, 30]]\n",
      "4         5.0       0.752239  [[24, 16, 8], [15, 205, 16], [14, 14, 23]]\n",
      "5         6.0       0.749254  [[23, 19, 6], [18, 204, 14], [12, 15, 24]]\n",
      "6         7.0       0.752239  [[21, 19, 8], [18, 206, 12], [11, 15, 25]]\n",
      "7         8.0       0.755224  [[21, 18, 9], [15, 208, 13], [11, 16, 24]]\n",
      "8         9.0       0.740299  [[21, 18, 9], [16, 206, 14], [10, 20, 21]]\n",
      "9        10.0       0.758209   [[23, 19, 6], [15, 210, 11], [9, 21, 21]]\n",
      "10       11.0       0.749254   [[21, 22, 5], [14, 209, 13], [8, 22, 21]]\n",
      "11       12.0       0.737313   [[20, 22, 6], [16, 208, 12], [9, 23, 19]]\n",
      "12       13.0       0.734328   [[18, 23, 7], [15, 209, 12], [8, 24, 19]]\n",
      "13       14.0       0.740299   [[19, 22, 7], [14, 209, 13], [8, 23, 20]]\n",
      "14       15.0       0.740299   [[17, 24, 7], [12, 212, 12], [8, 24, 19]]\n",
      "15       16.0       0.740299   [[19, 22, 7], [14, 209, 13], [7, 24, 20]]\n",
      "16       17.0       0.737313   [[17, 24, 7], [12, 211, 13], [6, 26, 19]]\n",
      "17       18.0       0.731343   [[17, 25, 6], [14, 210, 12], [8, 25, 18]]\n",
      "18       19.0       0.728358   [[16, 26, 6], [14, 210, 12], [7, 26, 18]]\n",
      "19       20.0       0.728358   [[16, 26, 6], [14, 210, 12], [7, 26, 18]]\n",
      "20       21.0       0.731343   [[16, 25, 7], [12, 211, 13], [5, 28, 18]]\n",
      "21       22.0       0.728358   [[16, 26, 6], [13, 211, 12], [6, 28, 17]]\n",
      "22       23.0       0.728358   [[14, 28, 6], [12, 213, 11], [6, 28, 17]]\n",
      "23       24.0       0.734328   [[16, 26, 6], [14, 212, 10], [5, 28, 18]]\n",
      "24       25.0       0.731343   [[14, 28, 6], [12, 214, 10], [5, 29, 17]]\n",
      "25       26.0       0.719403   [[13, 29, 6], [13, 213, 10], [6, 30, 15]]\n",
      "26       27.0       0.722388   [[13, 29, 6], [12, 214, 10], [6, 30, 15]], 0.2:     neighbors  mean_accuracy                             confusion matrix\n",
      "0         1.0       0.758209  [[52, 24, 20], [36, 397, 37], [32, 13, 59]]\n",
      "1         2.0       0.758209  [[52, 24, 20], [36, 397, 37], [32, 13, 59]]\n",
      "2         3.0       0.762687  [[46, 29, 21], [27, 403, 40], [24, 18, 62]]\n",
      "3         4.0       0.774627  [[47, 31, 18], [29, 410, 31], [23, 19, 62]]\n",
      "4         5.0       0.755224  [[46, 29, 21], [30, 407, 33], [28, 23, 53]]\n",
      "5         6.0       0.756716  [[44, 35, 17], [36, 408, 26], [22, 27, 55]]\n",
      "6         7.0       0.773134  [[40, 36, 20], [28, 421, 21], [20, 27, 57]]\n",
      "7         8.0       0.770149  [[42, 35, 19], [26, 419, 25], [20, 29, 55]]\n",
      "8         9.0       0.765672  [[39, 36, 21], [23, 422, 25], [21, 31, 52]]\n",
      "9        10.0       0.770149  [[40, 39, 17], [24, 422, 24], [17, 33, 54]]\n",
      "10       11.0       0.768657  [[41, 40, 15], [26, 423, 21], [18, 35, 51]]\n",
      "11       12.0       0.759701  [[38, 41, 17], [30, 420, 20], [18, 35, 51]]\n",
      "12       13.0       0.759701  [[38, 43, 15], [27, 421, 22], [19, 35, 50]]\n",
      "13       14.0       0.759701  [[36, 46, 14], [28, 421, 21], [17, 35, 52]]\n",
      "14       15.0       0.759701  [[33, 47, 16], [21, 425, 24], [18, 35, 51]]\n",
      "15       16.0       0.759701  [[35, 47, 14], [24, 422, 24], [16, 36, 52]]\n",
      "16       17.0       0.765672  [[36, 47, 13], [21, 427, 22], [20, 34, 50]]\n",
      "17       18.0       0.764179  [[35, 49, 12], [22, 427, 21], [19, 35, 50]]\n",
      "18       19.0       0.759701  [[34, 50, 12], [20, 425, 25], [17, 37, 50]]\n",
      "19       20.0       0.758209  [[34, 52, 10], [22, 424, 24], [16, 38, 50]]\n",
      "20       21.0       0.758209  [[34, 50, 12], [24, 425, 21], [16, 39, 49]]\n",
      "21       22.0       0.759701  [[34, 51, 11], [20, 427, 23], [15, 41, 48]]\n",
      "22       23.0       0.764179  [[33, 53, 10], [19, 430, 21], [14, 41, 49]]\n",
      "23       24.0       0.764179  [[33, 51, 12], [19, 430, 21], [13, 42, 49]]\n",
      "24       25.0       0.761194  [[32, 54, 10], [19, 431, 20], [12, 45, 47]], 0.3:     neighbors  mean_accuracy                             confusion matrix\n",
      "0         1.0       0.740299  [[76, 39, 35], [60, 588, 47], [49, 31, 80]]\n",
      "1         2.0       0.740299  [[76, 39, 35], [60, 588, 47], [49, 31, 80]]\n",
      "2         3.0       0.750249  [[70, 52, 28], [52, 601, 42], [42, 35, 83]]\n",
      "3         4.0       0.760199  [[71, 54, 25], [56, 605, 34], [38, 34, 88]]\n",
      "4         5.0       0.754229  [[62, 58, 30], [48, 614, 33], [33, 45, 82]]\n",
      "5         6.0       0.758209  [[63, 59, 28], [54, 616, 25], [32, 45, 83]]\n",
      "6         7.0       0.757214  [[61, 63, 26], [48, 622, 25], [30, 52, 78]]\n",
      "7         8.0       0.760199  [[61, 63, 26], [44, 626, 25], [31, 52, 77]]\n",
      "8         9.0       0.757214  [[58, 67, 25], [42, 627, 26], [29, 55, 76]]\n",
      "9        10.0       0.752239  [[56, 73, 21], [45, 626, 24], [29, 57, 74]]\n",
      "10       11.0       0.744279  [[51, 76, 23], [45, 626, 24], [30, 59, 71]]\n",
      "11       12.0       0.743284  [[53, 74, 23], [52, 623, 20], [27, 62, 71]]\n",
      "12       13.0       0.752239  [[57, 72, 21], [48, 628, 19], [24, 65, 71]]\n",
      "13       14.0       0.758209  [[58, 73, 19], [40, 632, 23], [26, 62, 72]]\n",
      "14       15.0       0.755224  [[54, 76, 20], [40, 633, 22], [22, 66, 72]]\n",
      "15       16.0       0.754229  [[55, 76, 19], [41, 632, 22], [22, 67, 71]]\n",
      "16       17.0       0.753234  [[56, 75, 19], [41, 633, 21], [27, 65, 68]]\n",
      "17       18.0       0.756219  [[57, 73, 20], [42, 632, 21], [24, 65, 71]]\n",
      "18       19.0       0.752239  [[53, 78, 19], [36, 636, 23], [27, 66, 67]]\n",
      "19       20.0       0.754229  [[55, 76, 19], [41, 634, 20], [23, 68, 69]]\n",
      "20       21.0       0.753234  [[52, 79, 19], [39, 638, 18], [25, 68, 67]]\n",
      "21       22.0       0.754229  [[52, 79, 19], [37, 640, 18], [24, 70, 66]]\n",
      "22       23.0       0.754229  [[51, 83, 16], [33, 643, 19], [24, 72, 64]]\n",
      "23       24.0       0.755224  [[50, 83, 17], [36, 641, 18], [21, 71, 68]], 0.6:     neighbors  mean_accuracy                                  confusion matrix\n",
      "0         1.0       0.741663  [[152, 96, 61], [114, 1175, 109], [72, 67, 163]]\n",
      "1         2.0       0.741663  [[152, 96, 61], [114, 1175, 109], [72, 67, 163]]\n",
      "2         3.0       0.744649  [[131, 127, 51], [101, 1219, 78], [70, 86, 146]]\n",
      "3         4.0       0.751120  [[137, 126, 46], [107, 1227, 64], [72, 85, 145]]\n",
      "4         5.0       0.746640  [[125, 143, 41], [93, 1242, 63], [67, 102, 133]]\n",
      "5         6.0       0.747636  [[127, 143, 39], [93, 1247, 58], [68, 106, 128]]\n",
      "6         7.0       0.747636  [[116, 151, 42], [84, 1259, 55], [61, 114, 127]]\n",
      "7         8.0       0.751618  [[119, 152, 38], [80, 1267, 51], [64, 114, 124]]\n",
      "8         9.0       0.754107  [[116, 158, 35], [79, 1275, 44], [59, 119, 124]]\n",
      "9        10.0       0.753111  [[114, 161, 34], [80, 1273, 45], [57, 119, 126]]\n",
      "10       11.0       0.755600  [[115, 165, 29], [79, 1279, 40], [53, 125, 124]]\n",
      "11       12.0       0.756098  [[117, 164, 28], [72, 1283, 43], [51, 132, 119]]\n",
      "12       13.0       0.757093  [[113, 167, 29], [68, 1291, 39], [47, 138, 117]]\n",
      "13       14.0       0.749627  [[109, 172, 28], [66, 1291, 41], [52, 144, 106]]\n",
      "14       15.0       0.754604  [[111, 173, 25], [67, 1295, 36], [47, 145, 110]]\n",
      "15       16.0       0.751120  [[104, 180, 25], [67, 1294, 37], [46, 145, 111]]\n",
      "16       17.0       0.750622  [[104, 179, 26], [66, 1297, 35], [45, 150, 107]]\n",
      "17       18.0       0.753111  [[100, 186, 23], [57, 1304, 37], [41, 152, 109]], 0.8:     neighbors  mean_accuracy  \\\n",
      "0         1.0       0.720687   \n",
      "1         2.0       0.720687   \n",
      "2         3.0       0.730022   \n",
      "3         4.0       0.738984   \n",
      "4         5.0       0.737117   \n",
      "5         6.0       0.739358   \n",
      "6         7.0       0.739358   \n",
      "7         8.0       0.734877   \n",
      "8         9.0       0.736744   \n",
      "9        10.0       0.737117   \n",
      "10       11.0       0.737864   \n",
      "11       12.0       0.735997   \n",
      "\n",
      "                                     confusion matrix  \n",
      "0   [[176, 170, 69], [150, 1577, 134], [82, 143, 1...  \n",
      "1   [[176, 170, 69], [150, 1577, 134], [82, 143, 1...  \n",
      "2   [[157, 210, 48], [127, 1641, 93], [78, 167, 157]]  \n",
      "3   [[157, 210, 48], [121, 1667, 73], [78, 169, 155]]  \n",
      "4   [[149, 219, 47], [110, 1678, 73], [67, 188, 147]]  \n",
      "5   [[145, 231, 39], [105, 1696, 60], [73, 190, 139]]  \n",
      "6    [[133, 249, 33], [85, 1721, 55], [66, 210, 126]]  \n",
      "7    [[124, 259, 32], [89, 1722, 50], [65, 215, 122]]  \n",
      "8    [[117, 265, 33], [75, 1738, 48], [56, 228, 118]]  \n",
      "9    [[114, 272, 29], [70, 1743, 48], [50, 235, 117]]  \n",
      "10   [[110, 280, 25], [59, 1757, 45], [48, 245, 109]]  \n",
      "11   [[103, 287, 25], [65, 1762, 34], [45, 251, 106]]  , 0.5:     neighbors  mean_accuracy                                  confusion matrix\n",
      "0         1.0       0.744922     [[131, 76, 44], [94, 995, 74], [75, 64, 121]]\n",
      "1         2.0       0.744922     [[131, 76, 44], [94, 995, 74], [75, 64, 121]]\n",
      "2         3.0       0.752688    [[117, 94, 40], [79, 1031, 53], [67, 81, 112]]\n",
      "3         4.0       0.761051    [[119, 94, 38], [86, 1040, 37], [70, 75, 115]]\n",
      "4         5.0       0.753286   [[112, 105, 34], [80, 1041, 42], [64, 88, 108]]\n",
      "5         6.0       0.753286   [[110, 108, 33], [86, 1043, 34], [63, 89, 108]]\n",
      "6         7.0       0.749701  [[108, 113, 30], [81, 1043, 39], [55, 101, 104]]\n",
      "7         8.0       0.755675  [[111, 112, 28], [78, 1050, 35], [56, 100, 104]]\n",
      "8         9.0       0.749104   [[104, 118, 29], [75, 1054, 34], [58, 106, 96]]\n",
      "9        10.0       0.751493  [[101, 121, 29], [74, 1057, 32], [58, 102, 100]]\n",
      "10       11.0       0.749104    [[98, 125, 28], [70, 1062, 31], [62, 104, 94]]\n",
      "11       12.0       0.750299    [[99, 125, 27], [70, 1062, 31], [58, 107, 95]]\n",
      "12       13.0       0.753883   [[100, 124, 27], [72, 1066, 25], [54, 110, 96]]\n",
      "13       14.0       0.749104    [[95, 128, 28], [71, 1065, 27], [48, 118, 94]]\n",
      "14       15.0       0.746714    [[96, 130, 25], [71, 1066, 26], [51, 121, 88]]\n",
      "15       16.0       0.745520    [[95, 132, 24], [71, 1068, 24], [46, 129, 85]]\n",
      "16       17.0       0.749701    [[95, 134, 22], [67, 1072, 24], [45, 127, 88]]\n",
      "17       18.0       0.748507    [[96, 135, 20], [64, 1074, 25], [45, 132, 83]]\n",
      "18       19.0       0.749104    [[91, 138, 22], [62, 1077, 24], [44, 130, 86]]\n",
      "19       20.0       0.749701    [[93, 137, 21], [61, 1077, 25], [44, 131, 85]], 0.7:     neighbors  mean_accuracy  \\\n",
      "0         1.0       0.723431   \n",
      "1         2.0       0.723431   \n",
      "2         3.0       0.732394   \n",
      "3         4.0       0.738370   \n",
      "4         5.0       0.746479   \n",
      "5         6.0       0.753308   \n",
      "6         7.0       0.759283   \n",
      "7         8.0       0.757576   \n",
      "8         9.0       0.756722   \n",
      "9        10.0       0.752454   \n",
      "10       11.0       0.754161   \n",
      "11       12.0       0.754161   \n",
      "12       13.0       0.755015   \n",
      "13       14.0       0.750747   \n",
      "14       15.0       0.751601   \n",
      "\n",
      "                                     confusion matrix  \n",
      "0   [[159, 136, 72], [136, 1357, 131], [66, 107, 1...  \n",
      "1   [[159, 136, 72], [136, 1357, 131], [66, 107, 1...  \n",
      "2   [[136, 171, 60], [107, 1427, 90], [66, 133, 153]]  \n",
      "3   [[134, 177, 56], [106, 1435, 83], [69, 122, 161]]  \n",
      "4    [[136, 187, 44], [91, 1464, 69], [63, 140, 149]]  \n",
      "5    [[140, 186, 41], [89, 1474, 61], [60, 141, 151]]  \n",
      "6    [[139, 185, 43], [82, 1485, 57], [54, 143, 155]]  \n",
      "7    [[140, 187, 40], [82, 1483, 59], [55, 145, 152]]  \n",
      "8    [[131, 202, 34], [80, 1496, 48], [53, 153, 146]]  \n",
      "9    [[121, 209, 37], [77, 1502, 45], [56, 156, 140]]  \n",
      "10   [[119, 214, 34], [65, 1513, 46], [49, 168, 135]]  \n",
      "11   [[116, 217, 34], [66, 1518, 40], [46, 173, 133]]  \n",
      "12   [[111, 227, 29], [58, 1526, 40], [44, 176, 132]]  \n",
      "13   [[109, 225, 33], [57, 1526, 41], [48, 180, 124]]  \n",
      "14   [[105, 232, 30], [54, 1534, 36], [43, 187, 122]]  , 0.9:    neighbors  mean_accuracy                                   confusion matrix\n",
      "0        1.0       0.672088  [[176, 215, 72], [170, 1667, 267], [94, 170, 1...\n",
      "1        2.0       0.672088  [[176, 215, 72], [170, 1667, 267], [94, 170, 1...\n",
      "2        3.0       0.710256  [[155, 253, 55], [127, 1833, 144], [90, 204, 1...\n",
      "3        4.0       0.709924  [[153, 253, 57], [127, 1838, 139], [86, 212, 1...\n",
      "4        5.0       0.715898  [[133, 290, 40], [101, 1899, 104], [76, 245, 1...\n",
      "5        6.0       0.719549   [[118, 305, 40], [86, 1929, 89], [79, 246, 121]]\n",
      "6        7.0       0.719549    [[99, 329, 35], [65, 1960, 79], [68, 269, 109]]\n",
      "7        8.0       0.721540    [[96, 331, 36], [64, 1975, 65], [64, 279, 103]]\n",
      "8        9.0       0.722204     [[87, 343, 33], [46, 1996, 62], [54, 299, 93]], 0.4:     neighbors  mean_accuracy                              confusion matrix\n",
      "0         1.0       0.731143   [[96, 60, 41], [83, 785, 66], [64, 46, 98]]\n",
      "1         2.0       0.731143   [[96, 60, 41], [83, 785, 66], [64, 46, 98]]\n",
      "2         3.0       0.743092   [[91, 69, 37], [72, 806, 56], [53, 57, 98]]\n",
      "3         4.0       0.755788  [[89, 76, 32], [70, 821, 43], [55, 51, 102]]\n",
      "4         5.0       0.748320   [[79, 82, 36], [63, 826, 45], [46, 65, 97]]\n",
      "5         6.0       0.752054   [[83, 78, 36], [66, 833, 35], [52, 65, 91]]\n",
      "6         7.0       0.749066   [[77, 86, 34], [64, 834, 36], [46, 70, 92]]\n",
      "7         8.0       0.753547   [[78, 87, 32], [59, 837, 38], [40, 74, 94]]\n",
      "8         9.0       0.752801   [[76, 91, 30], [58, 844, 32], [49, 71, 88]]\n",
      "9        10.0       0.750560   [[74, 93, 30], [63, 843, 28], [47, 73, 88]]\n",
      "10       11.0       0.751307   [[72, 96, 29], [56, 847, 31], [43, 78, 87]]\n",
      "11       12.0       0.750560   [[73, 97, 27], [56, 847, 31], [44, 79, 85]]\n",
      "12       13.0       0.752801  [[69, 102, 26], [52, 852, 30], [42, 79, 87]]\n",
      "13       14.0       0.753547  [[71, 101, 25], [52, 853, 29], [41, 82, 85]]\n",
      "14       15.0       0.755041  [[69, 103, 25], [51, 854, 29], [39, 81, 88]]\n",
      "15       16.0       0.754294  [[69, 105, 23], [52, 856, 26], [37, 86, 85]]\n",
      "16       17.0       0.757282  [[72, 102, 23], [51, 857, 26], [38, 85, 85]]\n",
      "17       18.0       0.758028  [[70, 106, 21], [50, 860, 24], [39, 84, 85]]\n",
      "18       19.0       0.755788  [[70, 109, 18], [49, 858, 27], [39, 85, 84]]\n",
      "19       20.0       0.755041  [[70, 108, 19], [51, 858, 25], [38, 87, 83]]\n",
      "20       21.0       0.755788  [[71, 109, 17], [50, 859, 25], [36, 90, 82]]\n",
      "21       22.0       0.756535  [[68, 111, 18], [46, 863, 25], [35, 91, 82]]}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-7da73446801a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmax_accuracy_rgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_metrics_rgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mknn_metrics_rgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"best mean accuracy for RGB = %s @ k = %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_accuracy_rgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mean_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_accuracy_rgb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'neighbors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "# determine mean mean accuracy for each test set size\n",
    "print(knn_metrics_rgb)\n",
    "print(knn_metrics_gray)\n",
    "\n",
    "mean_mean_accuracies = {}\n",
    "highest_mean_accuracy = -1\n",
    "\n",
    "for key in knn_metrics_rgb.keys():\n",
    "    means = knn_metrics_rgb[key].mean()\n",
    "    mean_mean_accuracies[key] = means['mean_accuracy']\n",
    "    \n",
    "    if means['mean_accuracy'] > highest_mean_accuracy:\n",
    "        highest_mean_accuracy = means['mean_accuracy']\n",
    "    \n",
    "\n",
    "max_accuracy_rgb = knn_metrics_rgb.loc[knn_metrics_rgb['mean_accuracy'].idxmax()]\n",
    "print(\"best mean accuracy for RGB = %s @ k = %s\" % (max_accuracy_rgb['mean_accuracy'], max_accuracy_rgb['neighbors']))\n",
    "\n",
    "max_accuracy_gray = knn_metrics_gray.loc[knn_metrics_gray['mean_accuracy'].idxmax()]\n",
    "print(\"best mean accuracy for grayscale = %s @ k = %s\" % (max_accuracy_gray['score'], max_accuracy_gray['neighbors']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.50199203  0.40239044  0.09561753]\n",
      " [ 0.06448839  0.92605331  0.0094583 ]\n",
      " [ 0.23846154  0.34230769  0.41923077]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.47410359  0.37450199  0.15139442]\n",
      " [ 0.07394669  0.89423904  0.03181427]\n",
      " [ 0.26923077  0.28846154  0.44230769]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(max_accuracy_rgb['confusion matrix'], \n",
    "                      classes, \n",
    "                      title='Normalized confusion matrix RGB images, k=%s' % (int(max_accuracy_rgb['neighbors'])))\n",
    "plt.show()\n",
    "\n",
    "plot_confusion_matrix(max_accuracy_gray['confusion matrix'], \n",
    "                      classes, \n",
    "                      title='Normalized confusion matrix grayscale images, k=%s' % (int(max_accuracy_gray['neighbors'])))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_plot, = plt.plot(knn_metrics_rgb['neighbors'], knn_metrics_rgb['mean_accuracy'], label='color accuracy', color='blue')\n",
    "gray_plot, = plt.plot(knn_metrics_gray['neighbors'], knn_metrics_gray['mean_accuracy'], label='grayscale accuracy', color='red')\n",
    "plt.title(\"Mean accuracy\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"mean accuracy\")\n",
    "\n",
    "# show a legend\n",
    "plt.legend(handles=[rgb_plot, gray_plot], loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and QDA\n",
    "Variables: N/A\n",
    "\n",
    "Output: accuracy score, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jchadwick\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Results:\n",
      "accuracy = 0.553166069295\n",
      "confusion matrix = \n",
      "[[104  90  57]\n",
      " [256 735 172]\n",
      " [ 80  93  87]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.41434263  0.35856574  0.22709163]\n",
      " [ 0.22012038  0.63198624  0.14789338]\n",
      " [ 0.30769231  0.35769231  0.33461538]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEmCAYAAADbUaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/8aAfCu+XSdSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23a46e0ed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# response has to be numeric for LDA and QDA\n",
    "y_train_numeric = []\n",
    "y_test_numeric = []\n",
    "for cl in y_train:\n",
    "    y_train_numeric.append(classes.index(cl))\n",
    "\n",
    "for cl in y_test:\n",
    "    y_test_numeric.append(classes.index(cl))\n",
    "\n",
    "# LDA\n",
    "    \n",
    "#print(y_train_numeric[:5])\n",
    "#print(X_train[:5])\n",
    "classifier = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "classifier.fit(X_train, y_train_numeric)\n",
    "\n",
    "y_prediction = []\n",
    "y_prediction_num = classifier.predict(X_test)\n",
    "for pred in y_prediction_num:\n",
    "    y_prediction.append(classes[pred])\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_prediction, labels=classes)\n",
    "accuracy = metrics.accuracy_score(y_test, y_prediction, normalize=True)\n",
    "\n",
    "print(\"LDA Results:\")\n",
    "print(\"accuracy = \" + str(accuracy))\n",
    "print(\"confusion matrix = \")\n",
    "print(cm)\n",
    "\n",
    "plot_confusion_matrix(cm, classes, title=\"LDA Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jchadwick\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Results:\n",
      "accuracy = 0.589008363202\n",
      "confusion matrix = \n",
      "[[ 55 166  30]\n",
      " [160 891 112]\n",
      " [ 49 171  40]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.21912351  0.66135458  0.11952191]\n",
      " [ 0.13757524  0.7661221   0.09630267]\n",
      " [ 0.18846154  0.65769231  0.15384615]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEmCAYAAADbUaM7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVwEeRJ1zrgI8iDrnXAX8P93LiIfLCU04AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23a3e6a0208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# QDA\n",
    "classifier = discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "classifier.fit(X_train, y_train_numeric)\n",
    "\n",
    "y_prediction = []\n",
    "y_prediction_num = classifier.predict(X_test)\n",
    "for pred in y_prediction_num:\n",
    "    y_prediction.append(classes[pred])\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_prediction, labels=classes)\n",
    "accuracy = metrics.accuracy_score(y_test, y_prediction, normalize=True)\n",
    "\n",
    "print(\"QDA Results:\")\n",
    "print(\"accuracy = \" + str(accuracy))\n",
    "print(\"confusion matrix = \")\n",
    "print(cm)\n",
    "\n",
    "plot_confusion_matrix(cm, classes, title=\"QDA Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax Regression (precursor to CNN)\n",
    "\n",
    "This section is largely adapted from:\n",
    "\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# setup inputs\n",
    "# x has 3072 features since it consists of 32x32 pixels\n",
    "# y_ is a one-hot multi-dimensional vector the size of the number\n",
    "# of classes\n",
    "num_features = len(X_train_rgb[0])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, num_features])\n",
    "y_ = tf.placeholder(tf.int32, [None, len(classes)])\n",
    "\n",
    "# define weights (W) and biases (b)\n",
    "W = tf.Variable(tf.zeros([num_features, len(classes)]))\n",
    "b = tf.Variable(tf.zeros([len(classes)]))\n",
    "\n",
    "# initialize variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement the regression model\n",
    "model = tf.matmul(x, W) + b\n",
    "\n",
    "# specify the loss function\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=model))\n",
    "\n",
    "# use steepest gradient descent to train the model\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now repeatedly run train_step to perform gradient descent\n",
    "offset = 0\n",
    "batch_size = 100\n",
    "for _ in range(1000):\n",
    "    X_batch, y_batch, offset = next_batch(X_train_rgb, y_train_rgb, offset, batch_size)\n",
    "    # generate one-hot encoding for the response\n",
    "    y_one_hot, class_map = one_hot(y_batch, classes)\n",
    "    sess.run(train_step, feed_dict={x: X_batch, y_: y_one_hot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.700119\n",
      "Normalized confusion matrix\n",
      "[[  4.78087649e-02   9.48207171e-01   3.98406375e-03]\n",
      " [  4.29922614e-03   9.94840929e-01   8.59845228e-04]\n",
      " [  7.69230769e-02   9.11538462e-01   1.15384615e-02]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_true = tf.argmax(y_, 1)\n",
    "y_pred = tf.argmax(model, 1)\n",
    "\n",
    "predictions = y_pred.eval(feed_dict={ x: X_test_rgb })\n",
    "\n",
    "# define metric\n",
    "correct_prediction = tf.equal(y_pred, y_true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "y_one_hot, _ = one_hot(y_test_rgb, classes)\n",
    "#print(accuracy.eval(feed_dict={x: X_test, y_: y_one_hot}))\n",
    "accuracy_metric = sess.run(accuracy, feed_dict={x: X_test_rgb, y_: y_one_hot})\n",
    "print(\"accuracy = %s\" % (accuracy_metric))\n",
    "\n",
    "sess.close()\n",
    "\n",
    "# calculate confusion matrix\n",
    "# return the predictions to class names\n",
    "class_predictions = []\n",
    "for pred in predictions:\n",
    "    class_predictions.append(class_map[pred])\n",
    "cm = metrics.confusion_matrix(y_test_rgb, np.array(class_predictions))\n",
    "plot_confusion_matrix(cm, classes, title=\"Softmax Regression Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "This code adapted from https://www.tensorflow.org/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Setup\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "num_features = len(X_train_gray[0])\n",
    "num_classes = len(classes)\n",
    "\n",
    "# placeholders\n",
    "x = tf.placeholder(tf.float32, [None, num_features])\n",
    "y_ = tf.placeholder(tf.int32, [None, num_classes])\n",
    "\n",
    "# initialize weights with small amount of noise\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# give the neurons a slightly positive bias to avoid dead neurons\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# conv2d uses stride of one and are zero-padded - output\n",
    "# is the same size as the input\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# pooling is max pooling over 2x2 blocks\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first convolutional layer\n",
    "# convolution , followed by max pooling\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# reshape x to a 4d tensor\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# reshape x_image with weight tensor, add the bias, apply ReLU function\n",
    "# finally max pool\n",
    "# max_pool_2x2 reduces image to 14x14\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# second convolutional layer\n",
    "# 64 features for each 5x5 patch\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "# max_pool_2x2 reduces image size to 7x7\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# densely connected layer\n",
    "# fully-connected layer with 1024 neurons\n",
    "# reshape the tnsor from the pooling layer into a batch of vectors\n",
    "# multiply by weight matrix, add a bias, and apply ReLU\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout - reduces overfitting\n",
    "# turned on during training, turned off during testing, controlled by the keep_prob placeholder\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# readout layer\n",
    "W_fc2 = weight_variable([1024, num_classes])\n",
    "b_fc2 = bias_variable([num_classes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.34\n",
      "step 100, training accuracy 0.82\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.96\n",
      "step 400, training accuracy 0.88\n",
      "step 500, training accuracy 0.84\n",
      "step 600, training accuracy 0.82\n",
      "step 700, training accuracy 0.92\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.94\n",
      "step 1100, training accuracy 0.94\n",
      "step 1200, training accuracy 0.94\n",
      "step 1300, training accuracy 0.94\n",
      "step 1400, training accuracy 0.9\n",
      "step 1500, training accuracy 0.98\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.98\n",
      "step 1800, training accuracy 1\n",
      "step 1900, training accuracy 0.98\n",
      "step 2000, training accuracy 1\n",
      "step 2100, training accuracy 1\n",
      "step 2200, training accuracy 1\n",
      "step 2300, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2500, training accuracy 1\n",
      "step 2600, training accuracy 1\n",
      "step 2700, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 2900, training accuracy 1\n",
      "test accuracy 0.848268\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate the model\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size = 50\n",
    "iterations = 3000\n",
    "offset = 0\n",
    "\n",
    "for i in range(iterations):\n",
    "    X_batch, y_batch, offset = next_batch(X_train_gray, y_train_gray, offset, batch_size)\n",
    "    y_batch_one_hot, _ = one_hot(y_batch, classes)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: X_batch, y_: y_batch_one_hot, keep_prob: 1.0 })\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={ x: X_batch, y_: y_batch_one_hot, keep_prob: 0.5})\n",
    "\n",
    "y_test_one_hot, _ = one_hot(y_test_gray, classes)\n",
    "print(\"test accuracy %g\" % (accuracy.eval(feed_dict={ x: X_test_gray, y_: y_test_one_hot, keep_prob: 1.0})))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
