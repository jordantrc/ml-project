{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# necessary libraries, functions, and constants\n",
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "classes = ['ocean', 'ship', 'sky']\n",
    "\n",
    "# options\n",
    "oversample = False\n",
    "\n",
    "# this function taken from:\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() * 0.75\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{0:.4f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "\n",
    "    \n",
    "def oversample(X, y, cl, largest_class):\n",
    "    \"\"\"\n",
    "    returns new X and y lists with oversampling\n",
    "    \"\"\"\n",
    "    X_new = list(X)\n",
    "    y_new = list(y)\n",
    "    \n",
    "    # first count each class in y\n",
    "    class_count = y.count(cl)\n",
    "    obs_add = largest_class - class_count\n",
    "    print(\"oversample - %s samples with %s class in y, adding %s observations\" % (class_count, cl, obs_add))\n",
    "    \n",
    "    # keep adding samples for the class\n",
    "    obs_added = 0\n",
    "    class_index = 0\n",
    "    all_in_class = [i for i, x in enumerate(y) if x == cl ] # => [1, 4, 6], all indexes for the class\n",
    "    assert len(all_in_class) > 0\n",
    "    \n",
    "    # take observations from the class sequentially, looping around when off the end\n",
    "    while obs_added < obs_add:\n",
    "        index = all_in_class[class_index]\n",
    "        \n",
    "        y_new.append(y[index])\n",
    "        X_new.append(X[index])\n",
    "        \n",
    "        obs_added += 1\n",
    "        \n",
    "        class_index += 1\n",
    "        if class_index >= len(all_in_class):\n",
    "            class_index = 0\n",
    "    \n",
    "    return X_new, y_new\n",
    "\n",
    "\n",
    "def next_batch(X, y, offset, step):\n",
    "    \"\"\"\n",
    "    returns a batch of observations and new offset, given offset and step\n",
    "    if the batch will run off the end, loops back around\n",
    "    \"\"\"\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "    if offset + step >= len(X):\n",
    "        new_offset = offset + step - len(X)\n",
    "        X_batch = list(X[offset:])\n",
    "        X_batch.extend(list(X[:new_offset]))\n",
    "        y_batch = list(y[offset:])\n",
    "        y_batch.extend(list(y[:new_offset]))\n",
    "    else:\n",
    "        new_offset = offset + step\n",
    "        X_batch = X[offset:offset + step]\n",
    "        y_batch = y[offset:offset + step]\n",
    "    \n",
    "    return X_batch, y_batch, new_offset\n",
    "\n",
    "\n",
    "def one_hot(y, classes):\n",
    "    \"\"\"\n",
    "    takes as input a list of response values as strings, returns\n",
    "    a one-hot matrix given the class ordering provided\n",
    "    \"\"\"\n",
    "    one_hot_matrix = []\n",
    "    for response in y:\n",
    "        row = [0] * len(classes)\n",
    "        row[classes.index(response)] = 1\n",
    "        one_hot_matrix.append(row)\n",
    "    \n",
    "    one_hot_matrix = np.array(one_hot_matrix)\n",
    "    return one_hot_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072 features\n",
      "Raw observations:\n",
      "3347 observations\n",
      "Class counts:\n",
      "ship - 2347\n",
      "sky - 496\n",
      "ocean - 504\n"
     ]
    }
   ],
   "source": [
    "# import data from data.csv\n",
    "y = []\n",
    "X = []\n",
    "y_numeric = []\n",
    "\n",
    "with open('data.csv', 'r') as csvfile:\n",
    "    data_reader = csv.reader(csvfile, dialect='excel')\n",
    "    for row in data_reader:\n",
    "        if len(row) > 0:\n",
    "            y.append(row[0])\n",
    "            X_float = [ float(x) for x in row[1:] ]\n",
    "            X.append(X_float)\n",
    "\n",
    "# create a y_numeric for use with tensorflow\n",
    "for obs in y:\n",
    "    y_numeric.append(classes.index(obs))\n",
    "\n",
    "assert len(X) == len(y) == len(y_numeric)\n",
    "    \n",
    "# convert to np.array objects\n",
    "y = np.array(y)\n",
    "y_numeric = np.array(y_numeric)\n",
    "X = np.array(X)\n",
    "\n",
    "# how many features?\n",
    "num_features = len(X[0])\n",
    "print(\"%s features\" % (num_features))\n",
    "\n",
    "# count the classes\n",
    "largest_class = \"none\"\n",
    "largest_class_count = -1\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "for key in class_counts.keys():\n",
    "    count = class_counts[key]\n",
    "    if count > largest_class_count:\n",
    "        largest_class_count = count\n",
    "        largest_class = key\n",
    "\n",
    "#for cl in classes:\n",
    "#    c = y.count(cl)\n",
    "#    \n",
    "#    if c > largest_class_count:\n",
    "#        largest_class_count = c\n",
    "#        largest_class = cl\n",
    "#    \n",
    "#    class_counts[cl] = c\n",
    "\n",
    "# raw data stats            \n",
    "print(\"Raw observations:\")\n",
    "print(\"%s observations\" % (len(y)))\n",
    "print(\"Class counts:\")\n",
    "for cl in class_counts.keys():\n",
    "    print(\"%s - %s\" % (cl, class_counts[cl]))\n",
    "    \n",
    "# if desired, use oversampling for any class that has less than 75% of the observations\n",
    "# of the largest class\n",
    "if oversample == True:\n",
    "    print(\"\\nOversampling enabled\")\n",
    "    print(\"Largest class is \" + largest_class + \" with %s observations\" % (largest_class_count))\n",
    "    \n",
    "    for cl in class_counts.keys():\n",
    "        if class_counts[cl] < 0.8 * largest_class_count:\n",
    "            # oversample\n",
    "            X, y = oversample(X, y, cl, largest_class_count)\n",
    "    \n",
    "    class_counts = {}\n",
    "    for cl in classes:\n",
    "        class_counts[cl] = y.count(cl)\n",
    "    \n",
    "    print(\"\\nObservations after oversampling:\")\n",
    "    for cl in class_counts.keys():\n",
    "        print(\"%s - %s\" % (cl, class_counts[cl]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "Inputs: number of neighbors, training and test sets\n",
    "\n",
    "Outputs: accuracy score, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max k = 40\n",
      "k = 1, metric = (1, 0.75507765830346474, array([[143,  64,  44],\n",
      "       [113, 993,  57],\n",
      "       [ 90,  42, 128]])) (24.61286294202949s)\n",
      "k = 11, metric = (11, 0.7753882915173238, array([[ 132,   88,   31],\n",
      "       [ 100, 1045,   18],\n",
      "       [  70,   69,  121]])) (25.548377982533143s)\n",
      "k = 21, metric = (21, 0.77479091995221028, array([[ 126,   97,   28],\n",
      "       [  97, 1055,   11],\n",
      "       [  63,   81,  116]])) (26.613854553928597s)\n",
      "k = 31, metric = (31, 0.77897252090800473, array([[ 116,  108,   27],\n",
      "       [  69, 1087,    7],\n",
      "       [  61,   98,  101]])) (25.97552152446822s)\n"
     ]
    }
   ],
   "source": [
    "# record metrics for the cross-validated value - n-neighbors\n",
    "# format of knn_metrics is [index, neighbors, score, confusion matrix]\n",
    "# score is the mean accuracy and is provided by the score function of the classifier\n",
    "knn_metrics = pd.DataFrame(columns=('neighbors', 'score', 'confusion matrix'))\n",
    "\n",
    "# create training and test sets\n",
    "# first method - evenly split training and test set into two random sets\n",
    "# randomly select from the original data into even-sized training and test sets\n",
    "# using training/test set cross-validation, with equally sized training and test sets\n",
    "\n",
    "# validation sets\n",
    "# TODO need to cross-validate the size of the train/test sets\n",
    "test_size = 0.5\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "\n",
    "def knn(k, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Performs K-nearest Neighbors with provided parameters, returns a tuple containing\n",
    "    k, accuracy, precision, confusion matrix\n",
    "    \"\"\"\n",
    "    classifier = neighbors.KNeighborsClassifier(k, 'distance')\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    y_prediction = classifier.predict(X_test)\n",
    "    cm = metrics.confusion_matrix(y_test, y_prediction, labels=classes)\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    \n",
    "    metric = (k, score, cm)\n",
    "    return metric\n",
    "\n",
    "max_k = int(math.sqrt(len(X_train)))\n",
    "print(\"max k = \" + str(max_k))\n",
    "\n",
    "# TODO - look into parallelizing this\n",
    "i = 0\n",
    "for k in range(1, max_k + 1, 10):\n",
    "    start = timer()\n",
    "    metric = knn(k, X_train, y_train, X_test, y_test)\n",
    "    end = timer()\n",
    "    print(\"k = %s, metric = %s (%ss)\" % (k, metric, end - start))\n",
    "    knn_metrics.loc[i] = metric\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   neighbors     score                                 confusion matrix\n",
      "0        1.0  0.755078   [[143, 64, 44], [113, 993, 57], [90, 42, 128]]\n",
      "1       11.0  0.775388  [[132, 88, 31], [100, 1045, 18], [70, 69, 121]]\n",
      "2       21.0  0.774791   [[126, 97, 28], [97, 1055, 11], [63, 81, 116]]\n",
      "3       31.0  0.778973   [[116, 108, 27], [69, 1087, 7], [61, 98, 101]]\n",
      "0    0.755078\n",
      "1    0.775388\n",
      "2    0.774791\n",
      "3    0.778973\n",
      "Name: score, dtype: float64\n",
      "best mean accuracy = 0.778972520908 @ k = 31.0\n"
     ]
    }
   ],
   "source": [
    "print(knn_metrics)\n",
    "print(knn_metrics['score'])\n",
    "\n",
    "max_accuracy = knn_metrics.loc[knn_metrics['score'].idxmax()]\n",
    "print(\"best mean accuracy = %s @ k = %s\" % (max_accuracy['score'], max_accuracy['neighbors']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[ 0.46215139  0.43027888  0.10756972]\n",
      " [ 0.05932932  0.93465176  0.00601892]\n",
      " [ 0.23461538  0.37692308  0.38846154]]\n"
     ]
    }
   ],
   "source": [
    "plot_confusion_matrix(max_accuracy['confusion matrix'], \n",
    "                      classes, \n",
    "                      title='Normalized confusion matrix for k=%s' % (int(max_accuracy['neighbors'])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(knn_metrics) + 1), knn_metrics['score'], label='accuracy')\n",
    "plt.title(\"Accuracy versus k\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"mean accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and QDA\n",
    "Variables: N/A\n",
    "\n",
    "Output: accuracy score, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Results:\n",
      "accuracy = 0.55017921147\n",
      "confusion matrix = \n",
      "[[104  89  58]\n",
      " [258 730 175]\n",
      " [ 79  94  87]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.41434263  0.35458167  0.2310757 ]\n",
      " [ 0.22184007  0.62768702  0.15047291]\n",
      " [ 0.30384615  0.36153846  0.33461538]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "# response has to be numeric for LDA and QDA\n",
    "y_train_numeric = []\n",
    "y_test_numeric = []\n",
    "for cl in y_train:\n",
    "    y_train_numeric.append(classes.index(cl))\n",
    "\n",
    "for cl in y_test:\n",
    "    y_test_numeric.append(classes.index(cl))\n",
    "\n",
    "# LDA\n",
    "    \n",
    "#print(y_train_numeric[:5])\n",
    "#print(X_train[:5])\n",
    "classifier = discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "classifier.fit(X_train, y_train_numeric)\n",
    "\n",
    "y_prediction = []\n",
    "y_prediction_num = classifier.predict(X_test)\n",
    "for pred in y_prediction_num:\n",
    "    y_prediction.append(classes[pred])\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_prediction, labels=classes)\n",
    "accuracy = metrics.accuracy_score(y_test, y_prediction, normalize=True)\n",
    "\n",
    "print(\"LDA Results:\")\n",
    "print(\"accuracy = \" + str(accuracy))\n",
    "print(\"confusion matrix = \")\n",
    "print(cm)\n",
    "\n",
    "plot_confusion_matrix(cm, classes, title=\"LDA Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Results:\n",
      "accuracy = 0.58064516129\n",
      "confusion matrix = \n",
      "[[ 53 164  34]\n",
      " [170 883 110]\n",
      " [ 51 173  36]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.21115538  0.65338645  0.13545817]\n",
      " [ 0.14617369  0.75924334  0.09458298]\n",
      " [ 0.19615385  0.66538462  0.13846154]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    }
   ],
   "source": [
    "# QDA\n",
    "classifier = discriminant_analysis.QuadraticDiscriminantAnalysis()\n",
    "classifier.fit(X_train, y_train_numeric)\n",
    "\n",
    "y_prediction = []\n",
    "y_prediction_num = classifier.predict(X_test)\n",
    "for pred in y_prediction_num:\n",
    "    y_prediction.append(classes[pred])\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_prediction, labels=classes)\n",
    "accuracy = metrics.accuracy_score(y_test, y_prediction, normalize=True)\n",
    "\n",
    "print(\"QDA Results:\")\n",
    "print(\"accuracy = \" + str(accuracy))\n",
    "print(\"confusion matrix = \")\n",
    "print(cm)\n",
    "\n",
    "plot_confusion_matrix(cm, classes, title=\"QDA Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax Regression (precursor to CNN)\n",
    "\n",
    "This section is largely adapted from:\n",
    "\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# setup inputs\n",
    "# x has 3072 features since it consists of 32x32 pixels\n",
    "# y_ is a one-hot multi-dimensional vector the size of the number\n",
    "# of classes\n",
    "x = tf.placeholder(tf.float32, [None, num_features])\n",
    "y_ = tf.placeholder(tf.int32, [None, len(classes)])\n",
    "\n",
    "# define weights (W) and biases (b)\n",
    "W = tf.Variable(tf.zeros([num_features, len(classes)]))\n",
    "b = tf.Variable(tf.zeros([len(classes)]))\n",
    "\n",
    "# initialize variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement the regression model\n",
    "model = tf.matmul(x, W) + b\n",
    "\n",
    "# specify the loss function\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=model))\n",
    "\n",
    "# use steepest gradient descent to train the model\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# define metric\n",
    "correct_prediction = tf.equal(y_pred, y_true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now repeatedly run train_step to perform gradient descent\n",
    "offset = 0\n",
    "batch_size = 100\n",
    "for _ in range(1000):\n",
    "    X_batch, y_batch, offset = next_batch(X_train, y_train, offset, batch_size)\n",
    "    # generate one-hot encoding for the response\n",
    "    y_one_hot = one_hot(y_batch, classes)\n",
    "    sess.run(train_step, feed_dict={x: X_batch, y_: y_one_hot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706093\n",
      "Tensor(\"confusion_matrix_5/SparseTensorDenseAdd:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_true = tf.argmax(y_, 1)\n",
    "y_pred = tf.argmax(model, 1)\n",
    "\n",
    "y_one_hot = one_hot(y_test, classes)\n",
    "print(accuracy.eval(feed_dict={x: X_test, y_: y_one_hot}))\n",
    "print(tf.confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
