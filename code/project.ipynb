{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# necessary libraries, functions, and constants\n",
    "import csv\n",
    "import itertools\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from scipy.stats import norm\n",
    "from sklearn import discriminant_analysis\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import neighbors\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "classes = ['ocean', 'ship', 'shore', 'sky']\n",
    "binary_classes = ['non-obstacle', 'obstacle']\n",
    "\n",
    "# options\n",
    "oversample = False\n",
    "\n",
    "\n",
    "def prepare_data(input_file, description, oversample=False):\n",
    "    \"\"\"\n",
    "    prepares the data, also prints some information about it\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    X = []\n",
    "    y_numeric = []\n",
    "    y_binary = []\n",
    "    y_binary_numeric = []\n",
    "    \n",
    "    print(\"\\n##############\\n%s Data summary:\" % (description))\n",
    "    with open(input_file, 'r') as csvfile:\n",
    "        data_reader = csv.reader(csvfile, dialect='excel')\n",
    "        for row in data_reader:\n",
    "            if len(row) > 0:\n",
    "                y.append(row[0])\n",
    "                X_float = [ float(x) for x in row[1:] ]\n",
    "                X.append(X_float)\n",
    "                if row[0] in ('ship', 'shore'):\n",
    "                    y_binary.append('obstacle')\n",
    "                else:\n",
    "                    y_binary.append('non-obstacle')\n",
    "\n",
    "    # create a y_numeric for use with tensorflow\n",
    "    for obs in y:\n",
    "        y_numeric.append(classes.index(obs))\n",
    "        \n",
    "    \n",
    "    # create a y_binary_numeric for binary classification in TF\n",
    "    binary_classes = ['non-obstacle', 'obstacle']\n",
    "    for obs in y_binary:\n",
    "        y_binary_numeric.append(binary_classes.index(obs))\n",
    "\n",
    "    assert len(X) == len(y) == len(y_numeric) == len(y_binary) == len(y_binary_numeric)\n",
    "\n",
    "    # convert to np.array objects\n",
    "    y = np.array(y)\n",
    "    y_numeric = np.array(y_numeric)\n",
    "    X = np.array(X)\n",
    "    y_binary = np.array(y_binary)\n",
    "    y_binary_numeric = np.array(y_binary_numeric)\n",
    "\n",
    "    # how many features?\n",
    "    num_features = len(X[0])\n",
    "    print(\"%s features\" % (num_features))\n",
    "\n",
    "    # count the classes\n",
    "    largest_class = \"none\"\n",
    "    largest_class_count = -1\n",
    "\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    class_counts = dict(zip(unique, counts))\n",
    "\n",
    "    for key in class_counts.keys():\n",
    "        count = class_counts[key]\n",
    "        if count > largest_class_count:\n",
    "            largest_class_count = count\n",
    "            largest_class = key\n",
    "\n",
    "    # raw data stats            \n",
    "    print(\"Raw observations:\")\n",
    "    print(\"%s observations\" % (len(y)))\n",
    "    print(\"Class counts:\")\n",
    "    for cl in class_counts.keys():\n",
    "        print(\"%s - %s\" % (cl, class_counts[cl]))\n",
    "\n",
    "    # if desired, use oversampling for any class that has less than 75% of the observations\n",
    "    # of the largest class\n",
    "    if oversample == True:\n",
    "        print(\"\\nOversampling enabled\")\n",
    "        print(\"Largest class is \" + largest_class + \" with %s observations\" % (largest_class_count))\n",
    "\n",
    "        for cl in class_counts.keys():\n",
    "            if class_counts[cl] < 0.8 * largest_class_count:\n",
    "                # oversample\n",
    "                X, y = oversample(X, y, cl, largest_class_count)\n",
    "\n",
    "        class_counts = {}\n",
    "        for cl in classes:\n",
    "            class_counts[cl] = y.count(cl)\n",
    "\n",
    "        print(\"\\nObservations after oversampling:\")\n",
    "        for cl in class_counts.keys():\n",
    "            print(\"%s - %s\" % (cl, class_counts[cl]))\n",
    "    \n",
    "    return X, y, y_numeric, y_binary, y_binary_numeric\n",
    "\n",
    "\n",
    "# this function taken from:\n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=True,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() * 0.75\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{0:.4f}\".format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "\n",
    "    \n",
    "def oversample(X, y, cl, largest_class):\n",
    "    \"\"\"\n",
    "    returns new X and y lists with oversampling\n",
    "    \"\"\"\n",
    "    X_new = list(X)\n",
    "    y_new = list(y)\n",
    "    \n",
    "    # first count each class in y\n",
    "    class_count = y.count(cl)\n",
    "    obs_add = largest_class - class_count\n",
    "    print(\"oversample - %s samples with %s class in y, adding %s observations\" % (class_count, cl, obs_add))\n",
    "    \n",
    "    # keep adding samples for the class\n",
    "    obs_added = 0\n",
    "    class_index = 0\n",
    "    all_in_class = [i for i, x in enumerate(y) if x == cl ] # => [1, 4, 6], all indexes for the class\n",
    "    assert len(all_in_class) > 0\n",
    "    \n",
    "    # take observations from the class sequentially, looping around when off the end\n",
    "    while obs_added < obs_add:\n",
    "        index = all_in_class[class_index]\n",
    "        \n",
    "        y_new.append(y[index])\n",
    "        X_new.append(X[index])\n",
    "        \n",
    "        obs_added += 1\n",
    "        \n",
    "        class_index += 1\n",
    "        if class_index >= len(all_in_class):\n",
    "            class_index = 0\n",
    "    \n",
    "    return X_new, y_new\n",
    "\n",
    "\n",
    "def next_batch(X, y, offset, step):\n",
    "    \"\"\"\n",
    "    returns a batch of observations and new offset, given offset and step\n",
    "    if the batch will run off the end, loops back around\n",
    "    \"\"\"\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    assert len(X) == len(y)\n",
    "    \n",
    "    if offset + step >= len(X):\n",
    "        new_offset = offset + step - len(X)\n",
    "        X_batch = list(X[offset:])\n",
    "        X_batch.extend(list(X[:new_offset]))\n",
    "        y_batch = list(y[offset:])\n",
    "        y_batch.extend(list(y[:new_offset]))\n",
    "    else:\n",
    "        new_offset = offset + step\n",
    "        X_batch = X[offset:offset + step]\n",
    "        y_batch = y[offset:offset + step]\n",
    "    \n",
    "    return X_batch, y_batch, new_offset\n",
    "\n",
    "\n",
    "def one_hot(y, classes):\n",
    "    \"\"\"\n",
    "    takes as input a list of response values as strings, returns\n",
    "    a one-hot matrix given the class ordering provided, and the map\n",
    "    to return the index to classes\n",
    "    \"\"\"\n",
    "    class_map = {}\n",
    "    for i, cl in enumerate(classes):\n",
    "        class_map[i] = cl\n",
    "        \n",
    "    one_hot_matrix = []\n",
    "    for response in y:\n",
    "        row = [0] * len(classes)\n",
    "        row[classes.index(response)] = 1\n",
    "        one_hot_matrix.append(row)\n",
    "    \n",
    "    one_hot_matrix = np.array(one_hot_matrix)\n",
    "    return one_hot_matrix, class_map\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############\n",
      "RGB 32x32 Data summary:\n",
      "3072 features\n",
      "Raw observations:\n",
      "4140 observations\n",
      "Class counts:\n",
      "shore - 793\n",
      "sky - 496\n",
      "ocean - 504\n",
      "ship - 2347\n",
      "\n",
      "##############\n",
      "Grayscale 28x28 Data summary:\n",
      "784 features\n",
      "Raw observations:\n",
      "4140 observations\n",
      "Class counts:\n",
      "shore - 793\n",
      "sky - 496\n",
      "ocean - 504\n",
      "ship - 2347\n",
      "len of feature_sample = 4, length of sample = 4, length of color sample = 400\n"
     ]
    }
   ],
   "source": [
    "# import data from data.csv\n",
    "y_rgb = []\n",
    "X_rgb = []\n",
    "y_rgb_numeric = []\n",
    "y_rgb_binary = []\n",
    "r_rgb_binary_numeric = []\n",
    "\n",
    "y_gray = []\n",
    "X_gray = []\n",
    "y_gray_numeric = []\n",
    "y_gray_binary = []\n",
    "y_gray_binary_numeric = []\n",
    "\n",
    "X_rgb, y_rgb, y_rgb_numeric, y_rgb_binary, y_rgb_binary_numeric = prepare_data(\"data_rgb.csv\", \"RGB 32x32\")\n",
    "X_gray, y_gray, y_gray_numeric, y_gray_binary, y_gray_binary_numeric = prepare_data(\"data_gray.csv\", \"Grayscale 28x28\")\n",
    "\n",
    "feature_sample = {}\n",
    "# some information about the features\n",
    "# get 100 samples from each of the classes\n",
    "for cl in classes:\n",
    "    sample_red = []\n",
    "    sample_green = []\n",
    "    sample_blue = []\n",
    "    sample_gray = []\n",
    "    \n",
    "    i = 0\n",
    "    while len(sample_red) < 400:\n",
    "        cla = y_rgb[i]\n",
    "        if cla == cl:\n",
    "            # add the average of RGB values for each observation\n",
    "            sample = X_rgb[i][0::3]\n",
    "            sample_red.append(sum(sample)/len(sample))\n",
    "            sample = X_rgb[i][1::3]\n",
    "            sample_green.append(sum(sample)/len(sample))\n",
    "            sample = X_rgb[i][2::3]\n",
    "            sample_blue.append(sum(sample)/len(sample))\n",
    "            sample = X_gray[i]\n",
    "            sample_gray.append(sum(sample)/len(sample))\n",
    "        i += 1\n",
    "    \n",
    "    # add samples to the feature_sample\n",
    "    feature_sample[cl] = [sample_red, sample_green, sample_blue, sample_gray]\n",
    "\n",
    "print(\"len of feature_sample = %d, length of sample = %d, length of color sample = %d\" % (len(feature_sample), \n",
    "                                                                                          len(feature_sample['ship']),\n",
    "                                                                                          len(feature_sample['ship'][0])))\n",
    "#print(feature_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def color_histogram(samples, axis, class_names, super_class_name=''):\n",
    "    # plot some histograms for colors\n",
    "    num_bins = 100\n",
    "    \n",
    "    \n",
    "    if len(class_names) == 1:\n",
    "        class_name = class_names[0]\n",
    "        # red\n",
    "        n, bins, patches = plt.hist(samples[class_name][0], num_bins, color='white', histtype='step')\n",
    "        (mu,sigma) = norm.fit(samples[class_name][0])\n",
    "        y_plot = mlab.normpdf(bins, mu, sigma)\n",
    "        axis.plot(bins, y_plot, 'r-', linewidth=2)\n",
    "\n",
    "        # green\n",
    "        n, bins, patches = plt.hist(samples[class_name][1], num_bins, color='white', histtype='step')\n",
    "        (mu,sigma) = norm.fit(samples[class_name][1])\n",
    "        y_plot = mlab.normpdf(bins, mu, sigma)\n",
    "        axis.plot(bins, y_plot,'g-', linewidth=2)\n",
    "\n",
    "        # blue\n",
    "        n, bins, patches = plt.hist(samples[class_name][2], num_bins, color='white', histtype='step')\n",
    "        (mu,sigma) = norm.fit(samples[class_name][2])\n",
    "        y_plot = mlab.normpdf(bins, mu, sigma)\n",
    "        axis.plot(bins, y_plot,'b-', linewidth=2)\n",
    "\n",
    "        # gray\n",
    "        n, bins, patches = plt.hist(samples[class_name][3], num_bins, color='white', histtype='step')\n",
    "        (mu,sigma) = norm.fit(samples[class_name][3])\n",
    "        y_plot = mlab.normpdf(bins, mu, sigma)\n",
    "        axis.plot(bins, y_plot,'gray', linewidth=2)\n",
    "\n",
    "        # set axis limits\n",
    "        #axes = axis.gca()\n",
    "        axis.set_xlim([0,1])\n",
    "        axis.set_ylim([0,5])\n",
    "    \n",
    "        axis.set_title(\"%s\" % (class_name) )\n",
    "    else:\n",
    "        # red\n",
    "        red_data = samples[class_names[0]][0]\n",
    "        red_data.extend(samples[class_names[1]][0])\n",
    "        n, bins, patches = plt.hist(red_data, num_bins, color='white', histtype='step')\n",
    "        (mu,sigma) = norm.fit(red_data)\n",
    "        y_plot = mlab.normpdf(bins, mu, sigma)\n",
    "        axis.plot(bins,y_plot,'r-',linewidth=2)\n",
    "\n",
    "        # green\n",
    "        green_data = samples[class_names[0]][1]\n",
    "        green_data.extend(samples[class_names[1]][1])\n",
    "        n, bins, patches = plt.hist(green_data, num_bins, color='white', histtype='step')\n",
    "        (mu,sigma) = norm.fit(green_data)\n",
    "        y_plot = mlab.normpdf(bins, mu, sigma)\n",
    "        axis.plot(bins,y_plot,'g-',linewidth=2)\n",
    "\n",
    "        # blue\n",
    "        green_data = samples[class_names[0]][2]\n",
    "        green_data.extend(samples[class_names[1]][2])\n",
    "        n, bins, patches = plt.hist(green_data, num_bins, color='white', histtype='step')\n",
    "        (mu,sigma) = norm.fit(green_data)\n",
    "        y_plot = mlab.normpdf(bins, mu, sigma)\n",
    "        axis.plot(bins,y_plot,'b-',linewidth=2)\n",
    "\n",
    "        # gray\n",
    "        gray_data = samples[class_names[0]][3]\n",
    "        gray_data.extend(samples[class_names[1]][3])\n",
    "        n, bins, patches = plt.hist(gray_data, num_bins, color='white', histtype='step')\n",
    "        (mu,sigma) = norm.fit(gray_data)\n",
    "        y_plot = mlab.normpdf(bins, mu, sigma)\n",
    "        axis.plot(bins,y_plot,'gray',linewidth=2)\n",
    "        \n",
    "        # set axis limits\n",
    "        #axes = axis.gca()\n",
    "        axis.set_xlim([0,1])\n",
    "        axis.set_ylim([0,5])\n",
    "        \n",
    "        axis.set_title(\"%s\" % (super_class_name))\n",
    "\n",
    "    \n",
    "# color histograms for sub-classes\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row')\n",
    "color_histogram(feature_sample, ax1, ['ocean'])\n",
    "color_histogram(feature_sample, ax2, ['ship'])\n",
    "color_histogram(feature_sample, ax3, ['shore'])\n",
    "color_histogram(feature_sample, ax4, ['sky'])\n",
    "plt.suptitle(\"Color norms by class - four classes\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()\n",
    "\n",
    "# color histograms for classes\n",
    "f, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "color_histogram(feature_sample, ax1, ['ocean', 'sky'], 'non-obstacle')\n",
    "color_histogram(feature_sample, ax2, ['ship', 'shore'], 'obstacle')\n",
    "plt.suptitle(\"Color norms by class - two classes\")\n",
    "plt.xlabel(\"value\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create training and test sets\n",
    "# first method - split data set into two random sets\n",
    "# randomly select from the original data into training and test sets\n",
    "\n",
    "# validation sets\n",
    "# cross-validate the validation set sizes\n",
    "test_set_sizes = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "def generate_validation_sets(X, y, test_size):\n",
    "    \"\"\"\n",
    "    generates training and test sets using validation set split, given the test_size variable,\n",
    "    which is a float between 0 and 1\n",
    "    \"\"\"\n",
    "    print(\"generating train/test split with test size = %s\" % (test_size))\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, \n",
    "                                                                        y, \n",
    "                                                                        test_size=test_size, \n",
    "                                                                        random_state=0)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors\n",
    "Inputs: number of neighbors, training and test sets\n",
    "\n",
    "Outputs: accuracy score, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################\n",
      "test set size = 0.1\n",
      "generating train/test split with test size = 0.1\n",
      "generating train/test split with test size = 0.1\n",
      "generating train/test split with test size = 0.1\n",
      "generating train/test split with test size = 0.1\n",
      "Color, four classes:\n",
      "max k = 30\n",
      "k = 1, metric = (1, 0.6908212560386473, array([[ 17,   8,   6,  12],\n",
      "       [ 19, 202,   7,   8],\n",
      "       [ 18,  10,  30,  10],\n",
      "       [ 16,   5,   9,  37]])) (5.273974670277767s)\n",
      "k = 5, metric = (5, 0.70772946859903385, array([[ 19,   9,  10,   5],\n",
      "       [ 21, 201,  10,   4],\n",
      "       [ 15,  12,  36,   5],\n",
      "       [ 16,   9,   5,  37]])) (5.804723996826851s)\n",
      "k = 10, metric = (10, 0.69565217391304346, array([[ 14,  15,   9,   5],\n",
      "       [ 17, 202,  13,   4],\n",
      "       [ 10,  16,  35,   7],\n",
      "       [ 11,  12,   7,  37]])) (5.439763425851538s)\n",
      "k = 15, metric = (15, 0.70048309178743962, array([[ 15,  15,  10,   3],\n",
      "       [ 17, 206,   7,   6],\n",
      "       [ 11,  17,  33,   7],\n",
      "       [ 14,  12,   5,  36]])) (5.331760438639549s)\n",
      "k = 20, metric = (20, 0.68599033816425126, array([[ 15,  16,   8,   4],\n",
      "       [ 19, 202,   8,   7],\n",
      "       [ 14,  16,  32,   6],\n",
      "       [ 13,  14,   5,  35]])) (5.408880375079775s)\n",
      "k = 25, metric = (25, 0.68357487922705318, array([[ 15,  16,   7,   5],\n",
      "       [ 17, 206,   8,   5],\n",
      "       [ 16,  18,  29,   5],\n",
      "       [ 11,  17,   6,  33]])) (5.319006226779379s)\n",
      "k = 30, metric = (30, 0.6908212560386473, array([[ 16,  16,   6,   5],\n",
      "       [ 20, 206,   6,   4],\n",
      "       [ 14,  18,  30,   6],\n",
      "       [ 11,  17,   5,  34]])) (5.362913654601584s)\n",
      "Color, two classes:\n",
      "max k = 30\n",
      "k = 1, metric = (1, 0.79951690821256038, array([[ 82,  28],\n",
      "       [ 55, 249]])) (5.199154307489607s)\n",
      "k = 5, metric = (5, 0.79468599033816423, array([[ 73,  37],\n",
      "       [ 48, 256]])) (5.256115694695836s)\n",
      "k = 10, metric = (10, 0.79227053140096615, array([[ 70,  40],\n",
      "       [ 46, 258]])) (5.343083529643309s)\n",
      "k = 15, metric = (15, 0.79468599033816423, array([[ 69,  41],\n",
      "       [ 44, 260]])) (5.3728662987188045s)\n",
      "k = 20, metric = (20, 0.78260869565217395, array([[ 66,  44],\n",
      "       [ 46, 258]])) (5.388386458777404s)\n",
      "k = 25, metric = (25, 0.78985507246376807, array([[ 66,  44],\n",
      "       [ 43, 261]])) (5.323049045094194s)\n",
      "k = 30, metric = (30, 0.78985507246376807, array([[ 66,  44],\n",
      "       [ 43, 261]])) (5.490389363883196s)\n",
      "Grayscale, four classes:\n",
      "max k = 30\n",
      "k = 1, metric = (1, 0.63285024154589375, array([[ 14,  10,  11,   8],\n",
      "       [ 14, 195,  12,  15],\n",
      "       [ 25,  12,  22,   9],\n",
      "       [ 14,  10,  12,  31]])) (1.2509212783913881s)\n",
      "k = 5, metric = (5, 0.65700483091787443, array([[ 10,  15,  12,   6],\n",
      "       [ 10, 208,  11,   7],\n",
      "       [ 18,  20,  24,   6],\n",
      "       [ 10,  20,   7,  30]])) (1.2366668196093542s)\n",
      "k = 10, metric = (10, 0.67149758454106279, array([[ 10,  14,  15,   4],\n",
      "       [  8, 214,   9,   5],\n",
      "       [ 18,  23,  22,   5],\n",
      "       [  5,  22,   8,  32]])) (1.242650299385332s)\n",
      "k = 15, metric = (15, 0.64734299516908211, array([[ 10,  19,  11,   3],\n",
      "       [  8, 207,  10,  11],\n",
      "       [ 15,  25,  22,   6],\n",
      "       [  5,  23,  10,  29]])) (1.1745004509796217s)\n",
      "k = 20, metric = (20, 0.6376811594202898, array([[  7,  23,  11,   2],\n",
      "       [  8, 208,  10,  10],\n",
      "       [ 15,  25,  22,   6],\n",
      "       [  5,  24,  11,  27]])) (1.24534832896029s)\n",
      "k = 25, metric = (25, 0.63285024154589375, array([[  9,  21,  11,   2],\n",
      "       [  8, 207,  10,  11],\n",
      "       [ 16,  27,  19,   6],\n",
      "       [  6,  25,   9,  27]])) (1.1779815068152857s)\n",
      "k = 30, metric = (30, 0.63285024154589375, array([[ 11,  24,   7,   1],\n",
      "       [  8, 209,  11,   8],\n",
      "       [ 16,  28,  17,   7],\n",
      "       [  6,  26,  10,  25]])) (1.2559804611705658s)\n",
      "Grayscale, two classes:\n",
      "max k = 30\n",
      "k = 1, metric = (1, 0.7439613526570048, array([[ 67,  43],\n",
      "       [ 63, 241]])) (1.1400488048132047s)\n",
      "k = 5, metric = (5, 0.78019323671497587, array([[ 58,  52],\n",
      "       [ 39, 265]])) (1.2505370099142965s)\n",
      "k = 10, metric = (10, 0.76086956521739135, array([[ 54,  56],\n",
      "       [ 43, 261]])) (1.2130833606015585s)\n",
      "k = 15, metric = (15, 0.7560386473429952, array([[ 50,  60],\n",
      "       [ 41, 263]])) (1.24674262282997s)\n",
      "k = 20, metric = (20, 0.75120772946859904, array([[ 46,  64],\n",
      "       [ 39, 265]])) (1.1926009149997299s)\n",
      "k = 25, metric = (25, 0.76328502415458932, array([[ 46,  64],\n",
      "       [ 34, 270]])) (1.2403920806282258s)\n",
      "k = 30, metric = (30, 0.76811594202898548, array([[ 45,  65],\n",
      "       [ 31, 273]])) (1.2582936185472136s)\n",
      "\n",
      "####################\n",
      "test set size = 0.2\n",
      "generating train/test split with test size = 0.2\n",
      "generating train/test split with test size = 0.2\n",
      "generating train/test split with test size = 0.2\n",
      "generating train/test split with test size = 0.2\n",
      "Color, four classes:\n",
      "max k = 28\n",
      "k = 1, metric = (1, 0.65458937198067635, array([[ 51,  19,  21,  19],\n",
      "       [ 38, 373,  17,  17],\n",
      "       [ 46,  33,  63,  18],\n",
      "       [ 37,   9,  12,  55]])) (8.343066625452593s)\n",
      "k = 5, metric = (5, 0.68236714975845414, array([[ 56,  25,  15,  14],\n",
      "       [ 34, 386,  16,   9],\n",
      "       [ 39,  32,  74,  15],\n",
      "       [ 36,  19,   9,  49]])) (8.539000684619623s)\n",
      "k = 10, metric = (10, 0.67995169082125606, array([[ 44,  36,  16,  14],\n",
      "       [ 33, 389,  12,  11],\n",
      "       [ 34,  37,  74,  15],\n",
      "       [ 30,  20,   7,  56]])) (8.540408562213997s)\n",
      "k = 15, metric = (15, 0.6871980676328503, array([[ 47,  37,  14,  12],\n",
      "       [ 37, 388,   9,  11],\n",
      "       [ 28,  39,  79,  14],\n",
      "       [ 22,  28,   8,  55]])) (8.562720886455736s)\n",
      "k = 20, metric = (20, 0.67270531400966183, array([[ 43,  39,  16,  12],\n",
      "       [ 32, 385,  13,  15],\n",
      "       [ 36,  36,  76,  12],\n",
      "       [ 22,  30,   8,  53]])) (8.7040830865119s)\n",
      "k = 25, metric = (25, 0.67874396135265702, array([[ 43,  41,  14,  12],\n",
      "       [ 31, 392,   8,  14],\n",
      "       [ 30,  42,  75,  13],\n",
      "       [ 20,  32,   9,  52]])) (8.593128506865014s)\n",
      "Color, two classes:\n",
      "max k = 28\n",
      "k = 1, metric = (1, 0.78260869565217395, array([[162,  61],\n",
      "       [119, 486]])) (8.209392813545719s)\n",
      "k = 5, metric = (5, 0.8031400966183575, array([[157,  66],\n",
      "       [ 97, 508]])) (8.521832064110413s)\n",
      "k = 10, metric = (10, 0.80434782608695654, array([[149,  74],\n",
      "       [ 88, 517]])) (8.522079891620024s)\n",
      "k = 15, metric = (15, 0.7874396135265701, array([[135,  88],\n",
      "       [ 88, 517]])) (8.606138998330152s)\n",
      "k = 20, metric = (20, 0.78864734299516903, array([[132,  91],\n",
      "       [ 84, 521]])) (8.60060287588567s)\n",
      "k = 25, metric = (25, 0.78019323671497587, array([[123, 100],\n",
      "       [ 82, 523]])) (8.634213840427037s)\n",
      "Grayscale, four classes:\n",
      "max k = 28\n",
      "k = 1, metric = (1, 0.61352657004830913, array([[ 43,  23,  28,  16],\n",
      "       [ 27, 367,  31,  20],\n",
      "       [ 48,  45,  50,  17],\n",
      "       [ 28,  21,  16,  48]])) (1.853623896246745s)\n",
      "k = 5, metric = (5, 0.62439613526570048, array([[ 37,  39,  25,   9],\n",
      "       [ 25, 383,  20,  17],\n",
      "       [ 40,  52,  52,  16],\n",
      "       [ 19,  39,  10,  45]])) (1.8310794412439009s)\n",
      "k = 10, metric = (10, 0.6376811594202898, array([[ 29,  43,  28,  10],\n",
      "       [ 18, 401,  12,  14],\n",
      "       [ 37,  57,  53,  13],\n",
      "       [ 16,  41,  11,  45]])) (1.840342032149465s)\n",
      "k = 15, metric = (15, 0.64130434782608692, array([[ 28,  57,  15,  10],\n",
      "       [ 16, 403,   9,  17],\n",
      "       [ 34,  58,  56,  12],\n",
      "       [ 15,  41,  13,  44]])) (1.843712003303608s)\n",
      "k = 20, metric = (20, 0.63164251207729472, array([[ 25,  60,  17,   8],\n",
      "       [ 15, 402,  12,  16],\n",
      "       [ 32,  62,  53,  13],\n",
      "       [ 13,  43,  14,  43]])) (1.9008206984569824s)\n",
      "k = 25, metric = (25, 0.6280193236714976, array([[ 25,  60,  17,   8],\n",
      "       [ 15, 398,  15,  17],\n",
      "       [ 29,  63,  56,  12],\n",
      "       [ 11,  45,  16,  41]])) (1.843900666145828s)\n",
      "Grayscale, two classes:\n",
      "max k = 28\n",
      "k = 1, metric = (1, 0.75845410628019327, array([[135,  88],\n",
      "       [112, 493]])) (1.8752310742493137s)\n",
      "k = 5, metric = (5, 0.75724637681159424, array([[114, 109],\n",
      "       [ 92, 513]])) (1.8343660988871306s)\n",
      "k = 10, metric = (10, 0.75483091787439616, array([[106, 117],\n",
      "       [ 86, 519]])) (1.9209463448914903s)\n",
      "k = 15, metric = (15, 0.75362318840579712, array([[ 94, 129],\n",
      "       [ 75, 530]])) (1.8586682878590182s)\n",
      "k = 20, metric = (20, 0.75724637681159424, array([[ 91, 132],\n",
      "       [ 69, 536]])) (1.8540579717143828s)\n",
      "k = 25, metric = (25, 0.7560386473429952, array([[ 86, 137],\n",
      "       [ 65, 540]])) (1.8460396499867784s)\n",
      "\n",
      "####################\n",
      "test set size = 0.3\n",
      "generating train/test split with test size = 0.3\n",
      "generating train/test split with test size = 0.3\n",
      "generating train/test split with test size = 0.3\n",
      "generating train/test split with test size = 0.3\n",
      "Color, four classes:\n",
      "max k = 26\n",
      "k = 1, metric = (1, 0.67230273752012881, array([[ 77,  24,  29,  25],\n",
      "       [ 54, 579,  32,  26],\n",
      "       [ 69,  41,  95,  26],\n",
      "       [ 49,  15,  17,  84]])) (10.219965177367158s)\n",
      "k = 5, metric = (5, 0.69887278582930756, array([[ 82,  38,  15,  20],\n",
      "       [ 51, 599,  27,  14],\n",
      "       [ 64,  36, 109,  22],\n",
      "       [ 45,  28,  14,  78]])) (10.775765307046413s)\n",
      "k = 10, metric = (10, 0.69565217391304346, array([[ 66,  47,  17,  25],\n",
      "       [ 57, 600,  18,  16],\n",
      "       [ 53,  49, 111,  18],\n",
      "       [ 38,  31,   9,  87]])) (10.76249732853421s)\n",
      "k = 15, metric = (15, 0.69162640901771333, array([[ 69,  48,  20,  18],\n",
      "       [ 51, 603,  17,  20],\n",
      "       [ 57,  48, 106,  20],\n",
      "       [ 34,  40,  10,  81]])) (10.857543555458506s)\n",
      "k = 20, metric = (20, 0.6908212560386473, array([[ 68,  48,  20,  19],\n",
      "       [ 50, 605,  19,  17],\n",
      "       [ 54,  50, 108,  19],\n",
      "       [ 34,  42,  12,  77]])) (10.829512181280734s)\n",
      "k = 25, metric = (25, 0.69162640901771333, array([[ 63,  53,  18,  21],\n",
      "       [ 44, 616,  14,  17],\n",
      "       [ 51,  55, 105,  20],\n",
      "       [ 37,  43,  10,  75]])) (10.831223126864643s)\n",
      "Color, two classes:\n",
      "max k = 26\n",
      "k = 1, metric = (1, 0.7906602254428341, array([[235,  85],\n",
      "       [175, 747]])) (10.415216729835947s)\n",
      "k = 5, metric = (5, 0.80273752012882449, array([[224,  96],\n",
      "       [149, 773]])) (10.762487065275309s)\n",
      "k = 10, metric = (10, 0.81078904991948475, array([[215, 105],\n",
      "       [130, 792]])) (11.501481833428443s)\n",
      "k = 15, metric = (15, 0.79951690821256038, array([[202, 118],\n",
      "       [131, 791]])) (11.706767532362392s)\n",
      "k = 20, metric = (20, 0.79710144927536231, array([[199, 121],\n",
      "       [131, 791]])) (11.208231254157909s)\n",
      "k = 25, metric = (25, 0.80032206119162641, array([[191, 129],\n",
      "       [119, 803]])) (11.324556536669661s)\n",
      "Grayscale, four classes:\n",
      "max k = 26\n",
      "k = 1, metric = (1, 0.63446054750402581, array([[ 57,  34,  38,  26],\n",
      "       [ 46, 576,  38,  31],\n",
      "       [ 66,  68,  76,  21],\n",
      "       [ 34,  28,  24,  79]])) (2.253468679553407s)\n",
      "k = 5, metric = (5, 0.65217391304347827, array([[ 51,  45,  38,  21],\n",
      "       [ 32, 607,  28,  24],\n",
      "       [ 66,  68,  77,  20],\n",
      "       [ 21,  53,  16,  75]])) (2.372612735858638s)\n",
      "k = 10, metric = (10, 0.65539452495974238, array([[ 45,  56,  33,  21],\n",
      "       [ 21, 628,  21,  21],\n",
      "       [ 72,  73,  73,  13],\n",
      "       [ 23,  59,  15,  68]])) (2.3174181384379153s)\n",
      "k = 15, metric = (15, 0.65136876006441224, array([[ 43,  72,  26,  14],\n",
      "       [ 22, 621,  19,  29],\n",
      "       [ 51,  89,  79,  12],\n",
      "       [ 20,  59,  20,  66]])) (2.5631972223995945s)\n",
      "k = 20, metric = (20, 0.64734299516908211, array([[ 40,  80,  22,  13],\n",
      "       [ 20, 627,  18,  26],\n",
      "       [ 55,  91,  71,  14],\n",
      "       [ 21,  60,  18,  66]])) (2.3421154628667864s)\n",
      "k = 25, metric = (25, 0.64412238325281801, array([[ 37,  80,  26,  12],\n",
      "       [ 20, 628,  18,  25],\n",
      "       [ 44,  99,  73,  15],\n",
      "       [ 18,  65,  20,  62]])) (2.3925584734065524s)\n",
      "Grayscale, two classes:\n",
      "max k = 26\n",
      "k = 1, metric = (1, 0.76811594202898548, array([[196, 124],\n",
      "       [164, 758]])) (2.3064590912549647s)\n",
      "k = 5, metric = (5, 0.76972624798711753, array([[175, 145],\n",
      "       [141, 781]])) (2.3063540437842676s)\n",
      "k = 10, metric = (10, 0.77697262479871176, array([[169, 151],\n",
      "       [126, 796]])) (2.346778906467989s)\n",
      "k = 15, metric = (15, 0.77697262479871176, array([[144, 176],\n",
      "       [101, 821]])) (2.6653576987118868s)\n",
      "k = 20, metric = (20, 0.77616747181964574, array([[145, 175],\n",
      "       [103, 819]])) (2.36327377423504s)\n",
      "k = 25, metric = (25, 0.78099838969404189, array([[137, 183],\n",
      "       [ 89, 833]])) (2.3778705428780995s)\n",
      "\n",
      "####################\n",
      "test set size = 0.4\n",
      "generating train/test split with test size = 0.4\n",
      "generating train/test split with test size = 0.4\n",
      "generating train/test split with test size = 0.4\n",
      "generating train/test split with test size = 0.4\n",
      "Color, four classes:\n",
      "max k = 24\n",
      "k = 1, metric = (1, 0.67149758454106279, array([[101,  29,  37,  39],\n",
      "       [ 75, 772,  51,  37],\n",
      "       [ 95,  50, 134,  29],\n",
      "       [ 58,  23,  21, 105]])) (11.991201066896338s)\n",
      "k = 5, metric = (5, 0.70350241545893721, array([[106,  46,  24,  30],\n",
      "       [ 76, 804,  40,  15],\n",
      "       [ 82,  54, 146,  26],\n",
      "       [ 50,  33,  15, 109]])) (12.202868701528587s)\n",
      "k = 10, metric = (10, 0.69263285024154586, array([[ 89,  56,  29,  32],\n",
      "       [ 80, 805,  32,  18],\n",
      "       [ 76,  63, 147,  22],\n",
      "       [ 45,  44,  12, 106]])) (12.222080012364131s)\n",
      "k = 15, metric = (15, 0.69504830917874394, array([[ 94,  58,  25,  29],\n",
      "       [ 75, 805,  30,  25],\n",
      "       [ 71,  68, 149,  20],\n",
      "       [ 41,  45,  18, 103]])) (12.852259184710874s)\n",
      "k = 20, metric = (20, 0.70048309178743962, array([[ 94,  62,  22,  28],\n",
      "       [ 68, 824,  22,  21],\n",
      "       [ 77,  66, 142,  23],\n",
      "       [ 40,  50,  17, 100]])) (12.428846035423703s)\n",
      "Color, two classes:\n",
      "max k = 24\n",
      "k = 1, metric = (1, 0.79106280193236711, array([[ 303,  110],\n",
      "       [ 236, 1007]])) (11.932712567178896s)\n",
      "k = 5, metric = (5, 0.81461352657004826, array([[ 295,  118],\n",
      "       [ 189, 1054]])) (13.265444694921825s)\n",
      "k = 10, metric = (10, 0.80132850241545894, array([[ 270,  143],\n",
      "       [ 186, 1057]])) (12.669988843234478s)\n",
      "k = 15, metric = (15, 0.80374396135265702, array([[ 265,  148],\n",
      "       [ 177, 1066]])) (12.271304411872734s)\n",
      "k = 20, metric = (20, 0.80132850241545894, array([[ 263,  150],\n",
      "       [ 179, 1064]])) (14.103187104034532s)\n",
      "Grayscale, four classes:\n",
      "max k = 24\n",
      "k = 1, metric = (1, 0.64975845410628019, array([[ 85,  44,  45,  32],\n",
      "       [ 73, 775,  50,  37],\n",
      "       [ 90,  77, 114,  27],\n",
      "       [ 38,  43,  24, 102]])) (2.688862372116546s)\n",
      "k = 5, metric = (5, 0.65760869565217395, array([[ 73,  66,  44,  23],\n",
      "       [ 52, 812,  40,  31],\n",
      "       [ 83,  87, 112,  26],\n",
      "       [ 28,  67,  20,  92]])) (2.842621381144909s)\n",
      "k = 10, metric = (10, 0.65036231884057971, array([[ 69,  85,  31,  21],\n",
      "       [ 40, 829,  40,  26],\n",
      "       [ 84, 108,  97,  19],\n",
      "       [ 26,  77,  22,  82]])) (2.9398102142363314s)\n",
      "k = 15, metric = (15, 0.65458937198067635, array([[ 64,  93,  29,  20],\n",
      "       [ 38, 838,  28,  31],\n",
      "       [ 76, 116, 102,  14],\n",
      "       [ 18,  83,  26,  80]])) (2.7137067026324075s)\n",
      "k = 20, metric = (20, 0.64432367149758452, array([[ 58, 103,  29,  16],\n",
      "       [ 38, 838,  31,  28],\n",
      "       [ 78, 122,  92,  16],\n",
      "       [ 19,  87,  22,  79]])) (2.7136487454072267s)\n",
      "Grayscale, two classes:\n",
      "max k = 24\n",
      "k = 1, metric = (1, 0.768719806763285, array([[ 257,  156],\n",
      "       [ 227, 1016]])) (2.511659363653507s)\n",
      "k = 5, metric = (5, 0.77355072463768115, array([[ 227,  186],\n",
      "       [ 189, 1054]])) (2.762171922863672s)\n",
      "k = 10, metric = (10, 0.78442028985507251, array([[ 221,  192],\n",
      "       [ 165, 1078]])) (2.6476858766691294s)\n",
      "k = 15, metric = (15, 0.77657004830917875, array([[ 185,  228],\n",
      "       [ 142, 1101]])) (2.7493313788868363s)\n",
      "k = 20, metric = (20, 0.77898550724637683, array([[ 182,  231],\n",
      "       [ 135, 1108]])) (2.830262606601991s)\n",
      "\n",
      "####################\n",
      "test set size = 0.5\n",
      "generating train/test split with test size = 0.5\n",
      "generating train/test split with test size = 0.5\n",
      "generating train/test split with test size = 0.5\n",
      "generating train/test split with test size = 0.5\n",
      "Color, four classes:\n",
      "max k = 22\n",
      "k = 1, metric = (1, 0.66280193236714979, array([[116,  53,  48,  51],\n",
      "       [ 87, 958,  54,  65],\n",
      "       [105,  67, 159,  43],\n",
      "       [ 68,  32,  25, 139]])) (12.416505976235385s)\n",
      "k = 5, metric = (5, 0.69227053140096617, array([[ 111,   71,   43,   43],\n",
      "       [  82, 1002,   47,   33],\n",
      "       [  87,   73,  182,   32],\n",
      "       [  55,   52,   19,  138]])) (14.337367498312233s)\n",
      "k = 10, metric = (10, 0.69275362318840583, array([[ 110,   78,   40,   40],\n",
      "       [  87, 1006,   37,   34],\n",
      "       [  86,   84,  176,   28],\n",
      "       [  44,   58,   20,  142]])) (13.018063335172883s)\n",
      "k = 15, metric = (15, 0.69806763285024154, array([[ 108,   82,   39,   39],\n",
      "       [  75, 1022,   35,   32],\n",
      "       [  81,   89,  179,   25],\n",
      "       [  43,   64,   21,  136]])) (13.029691003468997s)\n",
      "k = 20, metric = (20, 0.69420289855072459, array([[ 110,   84,   35,   39],\n",
      "       [  71, 1030,   36,   27],\n",
      "       [  81,   95,  168,   30],\n",
      "       [  37,   75,   23,  129]])) (12.969126608765691s)\n",
      "Color, two classes:\n",
      "max k = 22\n",
      "k = 1, metric = (1, 0.778743961352657, array([[ 374,  158],\n",
      "       [ 300, 1238]])) (12.574073861653687s)\n",
      "k = 5, metric = (5, 0.79565217391304344, array([[ 346,  186],\n",
      "       [ 237, 1301]])) (13.456621733717839s)\n",
      "k = 10, metric = (10, 0.79903381642512072, array([[ 338,  194],\n",
      "       [ 222, 1316]])) (12.734257066857026s)\n",
      "k = 15, metric = (15, 0.79371980676328502, array([[ 311,  221],\n",
      "       [ 206, 1332]])) (12.663260069765784s)\n",
      "k = 20, metric = (20, 0.80289855072463767, array([[ 311,  221],\n",
      "       [ 187, 1351]])) (12.661246961773486s)\n",
      "Grayscale, four classes:\n",
      "max k = 22\n",
      "k = 1, metric = (1, 0.63574879227053138, array([[ 97,  70,  62,  39],\n",
      "       [ 78, 956,  59,  71],\n",
      "       [103,  95, 135,  41],\n",
      "       [ 47,  58,  31, 128]])) (2.5296366685706744s)\n",
      "k = 5, metric = (5, 0.64492753623188404, array([[  82,   97,   61,   28],\n",
      "       [  59, 1002,   50,   53],\n",
      "       [  93,  109,  140,   32],\n",
      "       [  35,   90,   28,  111]])) (2.5451142662918755s)\n",
      "k = 10, metric = (10, 0.63913043478260867, array([[  70,  115,   59,   24],\n",
      "       [  42, 1023,   50,   49],\n",
      "       [  83,  141,  127,   23],\n",
      "       [  27,  104,   30,  103]])) (2.5466090797240213s)\n",
      "k = 15, metric = (15, 0.63188405797101455, array([[  62,  126,   58,   22],\n",
      "       [  42, 1023,   49,   50],\n",
      "       [  74,  153,  127,   20],\n",
      "       [  25,  113,   30,   96]])) (2.5445869159152608s)\n",
      "k = 20, metric = (20, 0.62753623188405794, array([[  57,  136,   58,   17],\n",
      "       [  37, 1036,   47,   44],\n",
      "       [  62,  169,  124,   19],\n",
      "       [  24,  126,   32,   82]])) (2.677179463340053s)\n",
      "Grayscale, two classes:\n",
      "max k = 22\n",
      "k = 1, metric = (1, 0.7516908212560387, array([[ 311,  221],\n",
      "       [ 293, 1245]])) (2.4237729670599037s)\n",
      "k = 5, metric = (5, 0.76521739130434785, array([[ 256,  276],\n",
      "       [ 210, 1328]])) (2.6335440118718907s)\n",
      "k = 10, metric = (10, 0.76763285024154593, array([[ 244,  288],\n",
      "       [ 193, 1345]])) (2.6265680146148043s)\n",
      "k = 15, metric = (15, 0.76425120772946864, array([[ 206,  326],\n",
      "       [ 162, 1376]])) (2.546730427663988s)\n",
      "k = 20, metric = (20, 0.76763285024154593, array([[ 192,  340],\n",
      "       [ 141, 1397]])) (2.661078523588003s)\n",
      "\n",
      "####################\n",
      "test set size = 0.6\n",
      "generating train/test split with test size = 0.6\n",
      "generating train/test split with test size = 0.6\n",
      "generating train/test split with test size = 0.6\n",
      "generating train/test split with test size = 0.6\n",
      "Color, four classes:\n",
      "max k = 20\n",
      "k = 1, metric = (1, 0.67471819645732689, array([[ 131,   62,   60,   57],\n",
      "       [  95, 1172,   62,   70],\n",
      "       [ 127,   83,  205,   42],\n",
      "       [  71,   47,   32,  168]])) (10.579690279003444s)\n",
      "k = 5, metric = (5, 0.68679549114331728, array([[ 123,   82,   57,   48],\n",
      "       [  93, 1206,   60,   40],\n",
      "       [ 118,   86,  223,   30],\n",
      "       [  65,   67,   32,  154]])) (10.770178773891075s)\n",
      "k = 10, metric = (10, 0.69243156199677935, array([[ 114,   89,   59,   48],\n",
      "       [  92, 1218,   54,   35],\n",
      "       [  99,  103,  228,   27],\n",
      "       [  44,   74,   40,  160]])) (10.766265453750748s)\n",
      "k = 15, metric = (15, 0.6968599033816425, array([[ 117,   89,   60,   44],\n",
      "       [  85, 1225,   54,   35],\n",
      "       [  92,  100,  234,   31],\n",
      "       [  37,   89,   37,  155]])) (10.858781485564123s)\n",
      "k = 20, metric = (20, 0.69404186795491141, array([[ 110,  103,   57,   40],\n",
      "       [  79, 1239,   50,   31],\n",
      "       [  85,  118,  225,   29],\n",
      "       [  41,   97,   30,  150]])) (10.877310893422418s)\n",
      "Color, two classes:\n",
      "max k = 20\n",
      "k = 1, metric = (1, 0.78462157809983901, array([[ 427,  201],\n",
      "       [ 334, 1522]])) (10.635379324001406s)\n",
      "k = 5, metric = (5, 0.78904991948470204, array([[ 385,  243],\n",
      "       [ 281, 1575]])) (10.918720425212996s)\n",
      "k = 10, metric = (10, 0.80716586151368763, array([[ 391,  237],\n",
      "       [ 242, 1614]])) (10.88127492613512s)\n",
      "k = 15, metric = (15, 0.80032206119162641, array([[ 356,  272],\n",
      "       [ 224, 1632]])) (10.849054934997184s)\n",
      "k = 20, metric = (20, 0.81119162640901776, array([[ 349,  279],\n",
      "       [ 190, 1666]])) (10.950674779068322s)\n",
      "Grayscale, four classes:\n",
      "max k = 20\n",
      "k = 1, metric = (1, 0.6409017713365539, array([[ 118,   80,   65,   47],\n",
      "       [  89, 1161,   72,   77],\n",
      "       [ 134,  120,  165,   38],\n",
      "       [  51,   81,   38,  148]])) (1.8782092304118123s)\n",
      "k = 5, metric = (5, 0.64452495974235102, array([[  86,  112,   68,   44],\n",
      "       [  65, 1221,   52,   61],\n",
      "       [ 106,  143,  168,   40],\n",
      "       [  35,  113,   44,  126]])) (1.8284028437674351s)\n",
      "k = 10, metric = (10, 0.63526570048309183, array([[  71,  135,   71,   33],\n",
      "       [  54, 1232,   56,   57],\n",
      "       [  98,  184,  154,   21],\n",
      "       [  30,  132,   35,  121]])) (1.9080919153293507s)\n",
      "k = 15, metric = (15, 0.63204508856682773, array([[  64,  147,   70,   29],\n",
      "       [  43, 1250,   56,   50],\n",
      "       [  86,  198,  151,   22],\n",
      "       [  31,  150,   32,  105]])) (1.9357278521893022s)\n",
      "k = 20, metric = (20, 0.63043478260869568, array([[  57,  157,   70,   26],\n",
      "       [  32, 1259,   54,   54],\n",
      "       [  67,  217,  156,   17],\n",
      "       [  22,  162,   40,   94]])) (1.9022460839632913s)\n",
      "Grayscale, two classes:\n",
      "max k = 20\n",
      "k = 1, metric = (1, 0.75764895330112725, array([[ 364,  264],\n",
      "       [ 338, 1518]])) (1.8473213498718906s)\n",
      "k = 5, metric = (5, 0.76650563607085342, array([[ 290,  338],\n",
      "       [ 242, 1614]])) (1.862201263708812s)\n",
      "k = 10, metric = (10, 0.76529790660225439, array([[ 265,  363],\n",
      "       [ 220, 1636]])) (1.8284052586514008s)\n",
      "k = 15, metric = (15, 0.76288244766505631, array([[ 209,  419],\n",
      "       [ 170, 1686]])) (2.025793983798394s)\n",
      "k = 20, metric = (20, 0.76731078904991945, array([[ 208,  420],\n",
      "       [ 158, 1698]])) (2.069310200351538s)\n",
      "\n",
      "####################\n",
      "test set size = 0.7\n",
      "generating train/test split with test size = 0.7\n",
      "generating train/test split with test size = 0.7\n",
      "generating train/test split with test size = 0.7\n",
      "generating train/test split with test size = 0.7\n",
      "Color, four classes:\n",
      "max k = 17\n",
      "k = 1, metric = (1, 0.66080055210489996, array([[ 136,   80,   80,   70],\n",
      "       [ 116, 1350,   75,   84],\n",
      "       [ 139,  112,  233,   68],\n",
      "       [  62,   65,   32,  196]])) (9.108247796116302s)\n",
      "k = 5, metric = (5, 0.67391304347826086, array([[ 131,  105,   68,   62],\n",
      "       [ 117, 1397,   66,   45],\n",
      "       [ 126,  130,  255,   41],\n",
      "       [  64,   82,   39,  170]])) (9.304558275386626s)\n",
      "k = 10, metric = (10, 0.68771566597653555, array([[ 118,  116,   82,   50],\n",
      "       [  87, 1437,   57,   44],\n",
      "       [ 103,  152,  258,   39],\n",
      "       [  47,   96,   32,  180]])) (9.376648007660151s)\n",
      "k = 15, metric = (15, 0.68840579710144922, array([[ 112,  131,   72,   51],\n",
      "       [  74, 1456,   59,   36],\n",
      "       [  91,  166,  250,   45],\n",
      "       [  43,  103,   32,  177]])) (9.319688431616669s)\n",
      "Color, two classes:\n",
      "max k = 17\n",
      "k = 1, metric = (1, 0.77087646652864039, array([[ 464,  257],\n",
      "       [ 407, 1770]])) (9.427890043069965s)\n",
      "k = 5, metric = (5, 0.78640441683919948, array([[ 423,  298],\n",
      "       [ 321, 1856]])) (9.228048399112595s)\n",
      "k = 10, metric = (10, 0.79710144927536231, array([[ 397,  324],\n",
      "       [ 264, 1913]])) (9.3093485004174s)\n",
      "k = 15, metric = (15, 0.79917184265010355, array([[ 370,  351],\n",
      "       [ 231, 1946]])) (9.37252549816003s)\n",
      "Grayscale, four classes:\n",
      "max k = 17\n",
      "k = 1, metric = (1, 0.63354037267080743, array([[ 118,  105,   78,   65],\n",
      "       [  95, 1345,   83,  102],\n",
      "       [ 130,  162,  190,   70],\n",
      "       [  42,  100,   30,  183]])) (1.7028427415216356s)\n",
      "k = 5, metric = (5, 0.63423050379572121, array([[  83,  149,   75,   59],\n",
      "       [  73, 1414,   66,   72],\n",
      "       [ 110,  198,  191,   53],\n",
      "       [  33,  137,   35,  150]])) (1.684140669430235s)\n",
      "k = 10, metric = (10, 0.63112491373360935, array([[  68,  172,   79,   47],\n",
      "       [  46, 1453,   54,   72],\n",
      "       [ 100,  243,  172,   37],\n",
      "       [  25,  160,   34,  136]])) (1.7237378304917002s)\n",
      "k = 15, metric = (15, 0.62905452035886822, array([[  54,  193,   79,   40],\n",
      "       [  33, 1475,   52,   65],\n",
      "       [  73,  270,  179,   30],\n",
      "       [  20,  187,   33,  115]])) (1.7647313984471111s)\n",
      "Grayscale, two classes:\n",
      "max k = 17\n",
      "k = 1, metric = (1, 0.75500345065562458, array([[ 408,  313],\n",
      "       [ 397, 1780]])) (1.736284664156301s)\n",
      "k = 5, metric = (5, 0.75879917184265011, array([[ 313,  408],\n",
      "       [ 291, 1886]])) (1.8434810799844854s)\n",
      "k = 10, metric = (10, 0.76466528640441689, array([[ 283,  438],\n",
      "       [ 244, 1933]])) (1.7147813261817646s)\n",
      "k = 15, metric = (15, 0.75914423740510695, array([[ 216,  505],\n",
      "       [ 193, 1984]])) (1.845675908026351s)\n",
      "\n",
      "####################\n",
      "test set size = 0.8\n",
      "generating train/test split with test size = 0.8\n",
      "generating train/test split with test size = 0.8\n",
      "generating train/test split with test size = 0.8\n",
      "generating train/test split with test size = 0.8\n",
      "Color, four classes:\n",
      "max k = 14\n",
      "k = 1, metric = (1, 0.63979468599033817, array([[ 142,  104,   84,   82],\n",
      "       [ 117, 1525,  105,  106],\n",
      "       [ 162,  141,  239,   93],\n",
      "       [  65,   89,   45,  213]])) (5.890119138321097s)\n",
      "k = 5, metric = (5, 0.65972222222222221, array([[ 129,  140,   87,   56],\n",
      "       [  89, 1603,  102,   59],\n",
      "       [ 141,  184,  255,   55],\n",
      "       [  47,  125,   42,  198]])) (6.034721207634902s)\n",
      "k = 10, metric = (10, 0.66998792270531404, array([[ 104,  179,   77,   52],\n",
      "       [  61, 1671,   84,   37],\n",
      "       [  96,  225,  256,   58],\n",
      "       [  37,  148,   39,  188]])) (6.015146758561059s)\n",
      "Color, two classes:\n",
      "max k = 14\n",
      "k = 1, metric = (1, 0.75845410628019327, array([[ 502,  322],\n",
      "       [ 478, 2010]])) (5.9155324759685755s)\n",
      "k = 5, metric = (5, 0.78291062801932365, array([[ 425,  399],\n",
      "       [ 320, 2168]])) (6.054527787470761s)\n",
      "k = 10, metric = (10, 0.78864734299516903, array([[ 396,  428],\n",
      "       [ 272, 2216]])) (5.927924455171706s)\n",
      "Grayscale, four classes:\n",
      "max k = 14\n",
      "k = 1, metric = (1, 0.61201690821256038, array([[ 115,  138,   80,   79],\n",
      "       [  94, 1521,  104,  134],\n",
      "       [ 134,  203,  187,  111],\n",
      "       [  46,  122,   40,  204]])) (1.2045313494254515s)\n",
      "k = 5, metric = (5, 0.60597826086956519, array([[ 106,  186,   56,   64],\n",
      "       [  82, 1593,   64,  114],\n",
      "       [ 132,  270,  157,   76],\n",
      "       [  42,  182,   37,  151]])) (1.2544125974854978s)\n",
      "k = 10, metric = (10, 0.61473429951690817, array([[  78,  238,   63,   33],\n",
      "       [  56, 1683,   51,   63],\n",
      "       [ 101,  340,  140,   54],\n",
      "       [  27,  224,   26,  135]])) (1.27410416845305s)\n",
      "Grayscale, two classes:\n",
      "max k = 14\n",
      "k = 1, metric = (1, 0.74245169082125606, array([[ 444,  380],\n",
      "       [ 473, 2015]])) (1.1327875493398096s)\n",
      "k = 5, metric = (5, 0.73520531400966183, array([[ 326,  498],\n",
      "       [ 379, 2109]])) (1.2622609717254818s)\n",
      "k = 10, metric = (10, 0.7533212560386473, array([[ 275,  549],\n",
      "       [ 268, 2220]])) (1.2603740414424465s)\n",
      "\n",
      "####################\n",
      "test set size = 0.9\n",
      "generating train/test split with test size = 0.9\n",
      "generating train/test split with test size = 0.9\n",
      "generating train/test split with test size = 0.9\n",
      "generating train/test split with test size = 0.9\n",
      "Color, four classes:\n",
      "max k = 10\n",
      "k = 1, metric = (1, 0.6127214170692431, array([[ 134,  114,  130,   82],\n",
      "       [ 168, 1647,  163,  111],\n",
      "       [ 209,  149,  297,   65],\n",
      "       [  74,  101,   77,  205]])) (2.518053373774819s)\n",
      "k = 5, metric = (5, 0.6427804616210413, array([[ 115,  145,  146,   54],\n",
      "       [ 101, 1770,  156,   62],\n",
      "       [ 155,  182,  319,   64],\n",
      "       [  64,  125,   77,  191]])) (2.725086241558529s)\n",
      "k = 10, metric = (10, 0.65271068169618895, array([[  57,  208,  152,   43],\n",
      "       [  39, 1862,  137,   51],\n",
      "       [  76,  247,  345,   52],\n",
      "       [  44,  169,   76,  168]])) (2.5950241911041303s)\n",
      "Color, two classes:\n",
      "max k = 10\n",
      "k = 1, metric = (1, 0.73832528180354262, array([[ 495,  422],\n",
      "       [ 553, 2256]])) (2.51386083262787s)\n",
      "k = 5, metric = (5, 0.77482555018786903, array([[ 386,  531],\n",
      "       [ 308, 2501]])) (2.6054507562812432s)\n",
      "k = 10, metric = (10, 0.78529253891572737, array([[ 313,  604],\n",
      "       [ 196, 2613]])) (2.735212153629618s)\n",
      "Grayscale, four classes:\n",
      "max k = 10\n",
      "k = 1, metric = (1, 0.57944176060118091, array([[ 104,  163,  116,   77],\n",
      "       [ 127, 1617,  162,  183],\n",
      "       [ 154,  233,  236,   97],\n",
      "       [  35,  158,   62,  202]])) (0.688316608247078s)\n",
      "k = 5, metric = (5, 0.59205582393988188, array([[  70,  223,  112,   55],\n",
      "       [  81, 1765,  118,  125],\n",
      "       [ 129,  320,  209,   62],\n",
      "       [  48,  193,   54,  162]])) (0.7900336514139781s)\n",
      "k = 10, metric = (10, 0.59634997316156735, array([[  40,  280,  105,   35],\n",
      "       [  45, 1875,   76,   93],\n",
      "       [  81,  408,  191,   40],\n",
      "       [  32,  258,   51,  116]])) (0.7566502897261671s)\n",
      "Grayscale, two classes:\n",
      "max k = 10\n",
      "k = 1, metric = (1, 0.71551261406333866, array([[ 418,  499],\n",
      "       [ 561, 2248]])) (0.715171631870362s)\n",
      "k = 5, metric = (5, 0.7310789049919485, array([[ 291,  626],\n",
      "       [ 376, 2433]])) (0.7396878399704292s)\n",
      "k = 10, metric = (10, 0.74476650563607083, array([[ 228,  689],\n",
      "       [ 262, 2547]])) (0.7678900672181044s)\n"
     ]
    }
   ],
   "source": [
    "def knn(X_train, y_train, X_test, y_test, class_list):\n",
    "    \"\"\"\n",
    "    Performs K-nearest Neighbors with provided parameters, returns a tuple containing\n",
    "    k, accuracy, precision, confusion matrix\n",
    "    \"\"\"\n",
    "    # record metrics for the cross-validated value - n-neighbors\n",
    "    # format of knn_metrics is [index, neighbors, score, confusion matrix]\n",
    "    # score is the mean accuracy and is provided by the score function of the classifier\n",
    "    knn_metrics = pd.DataFrame(columns=('neighbors', 'mean_accuracy', 'confusion matrix'))\n",
    "    \n",
    "    max_k = int(math.sqrt(len(X_train))/2)\n",
    "    #max_k = 4\n",
    "    print(\"max k = \" + str(max_k))\n",
    "\n",
    "    # TODO - look into parallelizing this\n",
    "    i = 0\n",
    "    for k in range(1, max_k + 1):\n",
    "        start = timer()\n",
    "        \n",
    "        classifier = neighbors.KNeighborsClassifier(k, 'distance', n_jobs=-1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        y_prediction = classifier.predict(X_test)\n",
    "        cm = metrics.confusion_matrix(y_test, y_prediction, labels=class_list)\n",
    "        mean_accuracy = classifier.score(X_test, y_test)\n",
    "        #obstacle_accuracy = cm\n",
    "        \n",
    "        end = timer()\n",
    "        \n",
    "        metric = (k, mean_accuracy, cm)\n",
    "        if k == 1 or k%5 == 0:\n",
    "            print(\"k = %s, metric = %s (%ss)\" % (k, metric, end - start))\n",
    "        knn_metrics.loc[i] = metric\n",
    "        i += 1\n",
    "    \n",
    "    return knn_metrics\n",
    "\n",
    "knn_metrics_rgb = {}\n",
    "knn_metrics_gray = {}\n",
    "knn_metrics_rgb_b = {}\n",
    "knn_metrics_gray_b = {}\n",
    "\n",
    "for t in test_set_sizes:\n",
    "    print(\"\\n####################\\ntest set size = %s\" % (t))\n",
    "    # generate validation sets\n",
    "    X_train_rgb, y_train_rgb, X_test_rgb, y_test_rgb = generate_validation_sets(X_rgb, y_rgb, t)\n",
    "    X_train_gray, y_train_gray, X_test_gray, y_test_gray = generate_validation_sets(X_gray, y_gray, t)\n",
    "    X_train_rgb_b, y_train_rgb_b, X_test_rgb_b, y_test_rgb_b = generate_validation_sets(X_rgb, y_rgb_binary, t)\n",
    "    X_train_gray_b, y_train_gray_b, X_test_gray_b, y_test_gray_b = generate_validation_sets(X_gray, y_gray_binary, t)\n",
    "    \n",
    "    # generate metrics for knn for rgb and gray-scale\n",
    "    print(\"Color, four classes:\")\n",
    "    knn_metrics_rgb[t] = knn(X_train_rgb, y_train_rgb, X_test_rgb, y_test_rgb, classes)\n",
    "    print(\"Color, two classes:\")\n",
    "    knn_metrics_rgb_b[t] = knn(X_train_rgb_b, y_train_rgb_b, X_test_rgb_b, y_test_rgb_b, binary_classes)\n",
    "    print(\"Grayscale, four classes:\")\n",
    "    knn_metrics_gray[t] = knn(X_train_gray, y_train_gray, X_test_gray, y_test_gray, classes)\n",
    "    print(\"Grayscale, two classes:\")\n",
    "    knn_metrics_gray_b[t] = knn(X_train_gray_b, y_train_gray_b, X_test_gray_b, y_test_gray_b, binary_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########\n",
      "Four class, color plots\n",
      "Normalized confusion matrix\n",
      "[[ 0.44186047  0.30232558  0.13953488  0.11627907]\n",
      " [ 0.08898305  0.85169492  0.04661017  0.01271186]\n",
      " [ 0.17647059  0.19117647  0.54411765  0.08823529]\n",
      " [ 0.20895522  0.13432836  0.10447761  0.55223881]]\n",
      "\n",
      "#########\n",
      "Four class, grayscale plots\n",
      "Normalized confusion matrix\n",
      "[[ 0.23255814  0.3255814   0.30232558  0.13953488]\n",
      " [ 0.03813559  0.90677966  0.03813559  0.01694915]\n",
      " [ 0.22058824  0.29411765  0.38235294  0.10294118]\n",
      " [ 0.11940299  0.29850746  0.10447761  0.47761194]]\n",
      "\n",
      "#########\n",
      "Two class, color plots\n",
      "Normalized confusion matrix\n",
      "[[ 0.71428571  0.28571429]\n",
      " [ 0.15205149  0.84794851]]\n",
      "\n",
      "#########\n",
      "Two class, grayscale plots\n",
      "Normalized confusion matrix\n",
      "[[ 0.71428571  0.28571429]\n",
      " [ 0.15205149  0.84794851]]\n"
     ]
    }
   ],
   "source": [
    "# generate plots for the KNN method\n",
    "def knn_plots(knn_metrics, test_set_sizes, classes, color):\n",
    "    \"\"\"\n",
    "    generates some plots for the KNN method given the metrics data structure and\n",
    "    the test-set sizes\n",
    "    \"\"\"\n",
    "    max_accuracy = -1\n",
    "    max_accuracy_key = -1\n",
    "    # plot for effect of test size on accuracy - color\n",
    "    mean_accuracies = pd.DataFrame()\n",
    "    for key in test_set_sizes:\n",
    "        accuracies = knn_metrics[key]['mean_accuracy']\n",
    "        max_acc = max(accuracies)\n",
    "        \n",
    "        if max_acc > max_accuracy:\n",
    "            max_accuracy = max_acc\n",
    "            max_accuracy_key = key\n",
    "        \n",
    "        mean_accuracies = pd.concat([mean_accuracies, accuracies.rename(key)], axis=1)\n",
    "        #plt.boxplot(mean_accuracies)\n",
    "    \n",
    "    mean_accuracies.plot(kind='box')\n",
    "    plt.title(\"Effect of test set size on KNN mean accuracy (%s, %s classes)\" % (color, len(classes)))\n",
    "    plt.xlabel(\"test set size\")\n",
    "    plt.ylabel(\"mean accuracy\")\n",
    "    plt.show()\n",
    "    \n",
    "    # show the confusion matrix for the best results\n",
    "    best_index = knn_metrics[max_accuracy_key]['mean_accuracy'].idxmax()\n",
    "    best_metric = knn_metrics[max_accuracy_key].loc[best_index]\n",
    "    best_cm = best_metric['confusion matrix']\n",
    "    best_k = best_metric['neighbors']\n",
    "    plot_confusion_matrix(best_cm, classes=classes, \n",
    "                          title=\"KNN best mean accuracy confusion matrix, k=%d, test size=%s, %s\" % (best_k, max_accuracy_key, color))\n",
    "    plt.show()\n",
    "    \n",
    "    return max_accuracy_key\n",
    "\n",
    "print(\"\\n#########\\nFour class, color plots\")\n",
    "best_rgb_test = knn_plots(knn_metrics_rgb, test_set_sizes, classes, 'color')\n",
    "print(\"\\n#########\\nFour class, grayscale plots\")\n",
    "best_gray_test = knn_plots(knn_metrics_gray, test_set_sizes, classes, 'grayscale')\n",
    "print(\"\\n#########\\nTwo class, color plots\")\n",
    "best_rgb_b_test = knn_plots(knn_metrics_rgb_b, test_set_sizes, binary_classes, 'color')\n",
    "print(\"\\n#########\\nTwo class, grayscale plots\")\n",
    "best_gray_b_test = knn_plots(knn_metrics_rgb_b, test_set_sizes, binary_classes, 'grayscale')\n",
    "    \n",
    "#print(\"Color: best test size mean accuracy = %s @ test_size = %s\", (highest_mean_accuracy_rgb, best_test_size_rgb))\n",
    "#print(\"Grayscale: best test size mean accuracy = %s @ test_size = %s\", (highest_mean_accuracy_gray, best_test_size_gray))\n",
    "#print(\"COlor: best mean accuracy\")\n",
    "\n",
    "#max_accuracy_rgb = knn_metrics_rgb[best_test_size_rgb].loc[knn_metrics_rgb['mean_accuracy'].idxmax()]\n",
    "#print(\"best mean accuracy for RGB = %s @ k = %s\" % (max_accuracy_rgb['mean_accuracy'], max_accuracy_rgb['neighbors']))\n",
    "\n",
    "#max_accuracy_gray = knn_metrics_gray[best_test_size_gray].loc[knn_metrics_gray['mean_accuracy'].idxmax()]\n",
    "#print(\"best mean accuracy for grayscale = %s @ k = %s\" % (max_accuracy_gray['score'], max_accuracy_gray['neighbors']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    neighbors  mean_accuracy  \\\n",
      "0         1.0       0.690821   \n",
      "1         2.0       0.690821   \n",
      "2         3.0       0.700483   \n",
      "3         4.0       0.695652   \n",
      "4         5.0       0.707729   \n",
      "5         6.0       0.710145   \n",
      "6         7.0       0.700483   \n",
      "7         8.0       0.698068   \n",
      "8         9.0       0.693237   \n",
      "9        10.0       0.695652   \n",
      "10       11.0       0.693237   \n",
      "11       12.0       0.683575   \n",
      "12       13.0       0.681159   \n",
      "13       14.0       0.683575   \n",
      "14       15.0       0.700483   \n",
      "15       16.0       0.695652   \n",
      "16       17.0       0.690821   \n",
      "17       18.0       0.690821   \n",
      "18       19.0       0.695652   \n",
      "19       20.0       0.685990   \n",
      "20       21.0       0.681159   \n",
      "21       22.0       0.681159   \n",
      "22       23.0       0.683575   \n",
      "23       24.0       0.681159   \n",
      "24       25.0       0.683575   \n",
      "25       26.0       0.683575   \n",
      "26       27.0       0.693237   \n",
      "27       28.0       0.693237   \n",
      "28       29.0       0.700483   \n",
      "29       30.0       0.690821   \n",
      "\n",
      "                                     confusion matrix  \n",
      "0   [[17, 8, 6, 12], [19, 202, 7, 8], [18, 10, 30,...  \n",
      "1   [[17, 8, 6, 12], [19, 202, 7, 8], [18, 10, 30,...  \n",
      "2   [[16, 8, 9, 10], [15, 207, 8, 6], [17, 8, 33, ...  \n",
      "3   [[18, 8, 8, 9], [20, 202, 7, 7], [18, 11, 33, ...  \n",
      "4   [[19, 9, 10, 5], [21, 201, 10, 4], [15, 12, 36...  \n",
      "5   [[19, 13, 6, 5], [21, 201, 11, 3], [12, 13, 37...  \n",
      "6   [[19, 13, 6, 5], [18, 200, 14, 4], [13, 14, 34...  \n",
      "7   [[17, 12, 8, 6], [18, 203, 11, 4], [12, 16, 33...  \n",
      "8   [[17, 13, 8, 5], [21, 197, 13, 5], [10, 16, 36...  \n",
      "9   [[14, 15, 9, 5], [17, 202, 13, 4], [10, 16, 35...  \n",
      "10  [[15, 15, 8, 5], [20, 203, 8, 5], [10, 17, 33,...  \n",
      "11  [[14, 14, 10, 5], [18, 204, 9, 5], [10, 19, 30...  \n",
      "12  [[12, 16, 10, 5], [17, 204, 10, 5], [11, 19, 3...  \n",
      "13  [[13, 15, 11, 4], [20, 202, 9, 5], [11, 17, 32...  \n",
      "14  [[15, 15, 10, 3], [17, 206, 7, 6], [11, 17, 33...  \n",
      "15  [[14, 16, 10, 3], [17, 205, 8, 6], [11, 17, 33...  \n",
      "16  [[14, 14, 10, 5], [17, 204, 9, 6], [12, 17, 33...  \n",
      "17  [[13, 16, 10, 4], [18, 204, 8, 6], [12, 16, 34...  \n",
      "18  [[15, 14, 10, 4], [18, 205, 7, 6], [13, 17, 33...  \n",
      "19  [[15, 16, 8, 4], [19, 202, 8, 7], [14, 16, 32,...  \n",
      "20  [[18, 16, 6, 3], [18, 201, 9, 8], [17, 16, 28,...  \n",
      "21  [[15, 15, 8, 5], [17, 202, 9, 8], [14, 17, 31,...  \n",
      "22  [[15, 13, 10, 5], [17, 203, 9, 7], [16, 17, 30...  \n",
      "23  [[15, 15, 8, 5], [17, 204, 8, 7], [16, 18, 29,...  \n",
      "24  [[15, 16, 7, 5], [17, 206, 8, 5], [16, 18, 29,...  \n",
      "25  [[17, 15, 7, 4], [17, 205, 8, 6], [15, 19, 28,...  \n",
      "26  [[16, 16, 6, 5], [17, 207, 7, 5], [14, 19, 30,...  \n",
      "27  [[16, 16, 6, 5], [17, 207, 7, 5], [14, 19, 29,...  \n",
      "28  [[16, 15, 7, 5], [18, 208, 6, 4], [13, 19, 31,...  \n",
      "29  [[16, 16, 6, 5], [20, 206, 6, 4], [14, 18, 30,...  \n"
     ]
    }
   ],
   "source": [
    "print(knn_metrics_rgb[0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgb_plot, = plt.plot(knn_metrics_rgb[best_rgb_test]['neighbors'], knn_metrics_rgb[best_rgb_test]['mean_accuracy'], \n",
    "                     label='color accuracy', color='blue')\n",
    "gray_plot, = plt.plot(knn_metrics_gray[best_gray_test]['neighbors'], knn_metrics_gray[best_gray_test]['mean_accuracy'], \n",
    "                      label='grayscale accuracy', color='red')\n",
    "plt.title(\"Four-class mean accuracy vs k, test size = %s\" % (best_rgb_test))\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"mean accuracy\")\n",
    "\n",
    "# show a legend\n",
    "plt.legend(handles=[rgb_plot, gray_plot], loc=5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "rgb_plot, = plt.plot(knn_metrics_rgb[best_rgb_b_test]['neighbors'], knn_metrics_rgb[best_rgb_b_test]['mean_accuracy'], \n",
    "                     label='color accuracy', color='blue')\n",
    "gray_plot, = plt.plot(knn_metrics_gray[best_gray_b_test]['neighbors'], knn_metrics_gray[best_gray_b_test]['mean_accuracy'], \n",
    "                      label='grayscale accuracy', color='red')\n",
    "plt.title(\"Two-class mean accuracy vs k, test size = %s\" % (best_rgb_b_test))\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"mean accuracy\")\n",
    "\n",
    "# show a legend\n",
    "plt.legend(handles=[rgb_plot, gray_plot], loc=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Variables: N/A\n",
    "\n",
    "Output: accuracy score, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating train/test split with test size = 0.1\n",
      "test size = 0.100000, score = 0.746377\n",
      "generating train/test split with test size = 0.2\n",
      "test size = 0.200000, score = 0.743961\n",
      "generating train/test split with test size = 0.3\n",
      "test size = 0.300000, score = 0.760870\n",
      "generating train/test split with test size = 0.4\n",
      "test size = 0.400000, score = 0.757850\n",
      "generating train/test split with test size = 0.5\n",
      "test size = 0.500000, score = 0.760870\n",
      "generating train/test split with test size = 0.6\n",
      "test size = 0.600000, score = 0.758052\n",
      "generating train/test split with test size = 0.7\n",
      "test size = 0.700000, score = 0.767771\n",
      "generating train/test split with test size = 0.8\n",
      "test size = 0.800000, score = 0.746981\n",
      "generating train/test split with test size = 0.9\n",
      "test size = 0.900000, score = 0.752013\n"
     ]
    }
   ],
   "source": [
    "# binary classifier\n",
    "# test different validation set sizes\n",
    "two_scores = []\n",
    "for t in test_set_sizes:\n",
    "    X_train_rgb, y_train_rgb, X_test_rgb, y_test_rgb = generate_validation_sets(X_rgb, y_rgb_binary, t)\n",
    "    classifier = linear_model.LogisticRegression(n_jobs=-1)\n",
    "    classifier.fit(X_train_rgb, y_train_rgb)\n",
    "    y_pred = classifier.predict(X_test_rgb)\n",
    "    cm = metrics.confusion_matrix(y_test_rgb, y_pred, labels=binary_classes)\n",
    "    score = classifier.score(X_test_rgb, y_test_rgb)\n",
    "    two_scores.append([score, cm])\n",
    "    print(\"test size = %f, score = %f\" % (t,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating train/test split with test size = 0.1\n",
      "test size = 0.100000, score = 0.676329\n",
      "generating train/test split with test size = 0.2\n",
      "test size = 0.200000, score = 0.669082\n",
      "generating train/test split with test size = 0.3\n",
      "test size = 0.300000, score = 0.673108\n",
      "generating train/test split with test size = 0.4\n",
      "test size = 0.400000, score = 0.681159\n",
      "generating train/test split with test size = 0.5\n",
      "test size = 0.500000, score = 0.668599\n",
      "generating train/test split with test size = 0.6\n",
      "test size = 0.600000, score = 0.680757\n",
      "generating train/test split with test size = 0.7\n",
      "test size = 0.700000, score = 0.676674\n",
      "generating train/test split with test size = 0.8\n",
      "test size = 0.800000, score = 0.661836\n",
      "generating train/test split with test size = 0.9\n",
      "test size = 0.900000, score = 0.651637\n"
     ]
    }
   ],
   "source": [
    "# multi-class classifier\n",
    "# test different validation set sizes\n",
    "four_scores = []\n",
    "for t in test_set_sizes:\n",
    "    X_train_rgb, y_train_rgb, X_test_rgb, y_test_rgb = generate_validation_sets(X_rgb, y_rgb, t)\n",
    "    classifier = linear_model.LogisticRegression(n_jobs=-1)\n",
    "    classifier.fit(X_train_rgb, y_train_rgb)\n",
    "    y_pred = classifier.predict(X_test_rgb)\n",
    "    cm = metrics.confusion_matrix(y_test_rgb, y_pred, labels=classes)\n",
    "    score = classifier.score(X_test_rgb, y_test_rgb)\n",
    "    four_scores.append([score, cm])\n",
    "    print(\"test size = %f, score = %f\" % (t,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.74637681159420288, array([[ 48,  62],\n",
      "       [ 43, 261]])], [0.7439613526570048, array([[ 97, 126],\n",
      "       [ 86, 519]])], [0.76086956521739135, array([[147, 173],\n",
      "       [124, 798]])], [0.75785024154589375, array([[ 186,  227],\n",
      "       [ 174, 1069]])], [0.76086956521739135, array([[ 245,  287],\n",
      "       [ 208, 1330]])], [0.75805152979066026, array([[ 273,  355],\n",
      "       [ 246, 1610]])], [0.76777087646652864, array([[ 330,  391],\n",
      "       [ 282, 1895]])], [0.7469806763285024, array([[ 380,  444],\n",
      "       [ 394, 2094]])], [0.75201288244766507, array([[ 412,  505],\n",
      "       [ 419, 2390]])]]\n",
      "[[0.67632850241545894, array([[  7,  18,  14,   4],\n",
      "       [ 20, 202,   4,  10],\n",
      "       [  4,  16,  40,   8],\n",
      "       [  7,  21,   8,  31]])], [0.66908212560386471, array([[ 22,  46,  36,   6],\n",
      "       [ 26, 383,  20,  16],\n",
      "       [ 14,  37,  96,  13],\n",
      "       [ 14,  28,  18,  53]])], [0.67310789049919484, array([[ 35,  55,  47,  18],\n",
      "       [ 39, 595,  34,  23],\n",
      "       [ 29,  44, 133,  25],\n",
      "       [ 19,  49,  24,  73]])], [0.6811594202898551, array([[ 46,  72,  56,  32],\n",
      "       [ 50, 805,  51,  29],\n",
      "       [ 41,  57, 184,  26],\n",
      "       [ 26,  60,  28,  93]])], [0.66859903381642516, array([[ 57,  99,  76,  36],\n",
      "       [ 57, 997,  61,  49],\n",
      "       [ 56,  78, 216,  24],\n",
      "       [ 37,  77,  36, 114]])], [0.68075684380032209, array([[  68,  118,   80,   44],\n",
      "       [  65, 1209,   81,   44],\n",
      "       [  63,   87,  274,   33],\n",
      "       [  36,   90,   52,  140]])], [0.67667356797791578, array([[  78,  134,  100,   54],\n",
      "       [  83, 1385,  101,   56],\n",
      "       [  62,  104,  338,   48],\n",
      "       [  34,  102,   59,  160]])], [0.66183574879227058, array([[  94,  168,   94,   56],\n",
      "       [  99, 1559,  104,   91],\n",
      "       [  98,  121,  356,   60],\n",
      "       [  48,  119,   62,  183]])], [0.65163714439076759, array([[  73,  203,  133,   51],\n",
      "       [ 111, 1747,  148,   83],\n",
      "       [ 121,  148,  391,   60],\n",
      "       [  51,  126,   63,  217]])]]\n"
     ]
    }
   ],
   "source": [
    "print(two_scores)\n",
    "print(four_scores)\n",
    "\n",
    "two_accs = []\n",
    "\n",
    "\n",
    "two_plot, = plt.plot(test_set_sizes, [a for a,b in two_scores], label='two-class accuracy', color='red')\n",
    "four_plot, = plt.plot(test_set_sizes, [a for a,b in four_scores], label='four-class accuracy', color='blue')\n",
    "plt.title(\"Logistic regression accuracy vs. test size, color\")\n",
    "plt.xlabel(\"test set size\")\n",
    "plt.ylabel(\"mean accuracy\")\n",
    "plt.legend(handles=[two_plot, four_plot], loc=5)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Softmax Regression (precursor to CNN)\n",
    "\n",
    "This section is largely adapted from:\n",
    "\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# setup inputs\n",
    "# x has 3072 features since it consists of 32x32 pixels\n",
    "# y_ is a one-hot multi-dimensional vector the size of the number\n",
    "# of classes\n",
    "num_features = len(X_train_rgb[0])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, num_features])\n",
    "y_ = tf.placeholder(tf.int32, [None, len(classes)])\n",
    "\n",
    "# define weights (W) and biases (b)\n",
    "W = tf.Variable(tf.zeros([num_features, len(classes)]))\n",
    "b = tf.Variable(tf.zeros([len(classes)]))\n",
    "\n",
    "# initialize variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement the regression model\n",
    "model = tf.matmul(x, W) + b\n",
    "\n",
    "# specify the loss function\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=model))\n",
    "\n",
    "# use steepest gradient descent to train the model\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now repeatedly run train_step to perform gradient descent\n",
    "offset = 0\n",
    "batch_size = 100\n",
    "for _ in range(1000):\n",
    "    X_batch, y_batch, offset = next_batch(X_train_rgb, y_train_rgb, offset, batch_size)\n",
    "    # generate one-hot encoding for the response\n",
    "    y_one_hot, class_map = one_hot(y_batch, classes)\n",
    "    sess.run(train_step, feed_dict={x: X_batch, y_: y_one_hot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.700119\n",
      "Normalized confusion matrix\n",
      "[[  4.78087649e-02   9.48207171e-01   3.98406375e-03]\n",
      " [  4.29922614e-03   9.94840929e-01   8.59845228e-04]\n",
      " [  7.69230769e-02   9.11538462e-01   1.15384615e-02]]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "y_true = tf.argmax(y_, 1)\n",
    "y_pred = tf.argmax(model, 1)\n",
    "\n",
    "predictions = y_pred.eval(feed_dict={ x: X_test_rgb })\n",
    "\n",
    "# define metric\n",
    "correct_prediction = tf.equal(y_pred, y_true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "y_one_hot, _ = one_hot(y_test_rgb, classes)\n",
    "#print(accuracy.eval(feed_dict={x: X_test, y_: y_one_hot}))\n",
    "accuracy_metric = sess.run(accuracy, feed_dict={x: X_test_rgb, y_: y_one_hot})\n",
    "print(\"accuracy = %s\" % (accuracy_metric))\n",
    "\n",
    "sess.close()\n",
    "\n",
    "# calculate confusion matrix\n",
    "# return the predictions to class names\n",
    "class_predictions = []\n",
    "for pred in predictions:\n",
    "    class_predictions.append(class_map[pred])\n",
    "cm = metrics.confusion_matrix(y_test_rgb, np.array(class_predictions))\n",
    "plot_confusion_matrix(cm, classes, title=\"Softmax Regression Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "This code adapted from https://www.tensorflow.org/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a confusion matrix\n",
    "def tf_confusion_matrix(predictions, labels, classes):\n",
    "    \"\"\"\n",
    "    produces and returns a confusion matrix given the predictions generated by\n",
    "    tensorflow (in one-hot format), and string labels.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred_strings = []\n",
    "    labels = labels.tolist()\n",
    "    \n",
    "    for p in predictions:\n",
    "        max_value = max(p)\n",
    "        max_index = p.tolist().index(max_value)\n",
    "        y_pred_strings.append(classes[max_index])\n",
    "    \n",
    "    #print(type(y_pred_strings))\n",
    "    #print(type(labels))\n",
    "    \n",
    "    cm = metrics.confusion_matrix(labels, y_pred_strings)\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# tensorflow cnn function\n",
    "def tensorflow_cnn(X, y, classes, test_set_sizes, iterations=5000):\n",
    "    \"\"\"\n",
    "    creates a tensor flow cnn and executes the computational graph,\n",
    "    returns a dict containing test set size: accuracy pairs\n",
    "    \"\"\"\n",
    "    # CNN Setup\n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    num_features = len(X[0])\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, num_features])\n",
    "    y_ = tf.placeholder(tf.int32, [None, num_classes])\n",
    "\n",
    "    # initialize weights with small amount of noise\n",
    "    def weight_variable(shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    # give the neurons a slightly positive bias to avoid dead neurons\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    # conv2d uses stride of one and are zero-padded - output\n",
    "    # is the same size as the input\n",
    "    def conv2d(x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    # pooling is max pooling over 2x2 blocks\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                             strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # first convolutional layer\n",
    "    # convolution , followed by max pooling\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "\n",
    "    # reshape x to a 4d tensor\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # reshape x_image with weight tensor, add the bias, apply ReLU function\n",
    "    # finally max pool\n",
    "    # max_pool_2x2 reduces image to 14x14\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "    \n",
    "    # second convolutional layer\n",
    "    # 64 features for each 5x5 patch\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "\n",
    "    # max_pool_2x2 reduces image size to 7x7\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "    \n",
    "    # densely connected layer\n",
    "    # fully-connected layer with 1024 neurons\n",
    "    # reshape the tensor from the pooling layer into a batch of vectors\n",
    "    # multiply by weight matrix, add a bias, and apply ReLU\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "    # dropout - reduces overfitting\n",
    "    # turned on during training, turned off during testing, controlled by the keep_prob placeholder\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    # readout layer\n",
    "    W_fc2 = weight_variable([1024, num_classes])\n",
    "    b_fc2 = bias_variable([num_classes])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    true_positives = tf.metrics.true_positives(y_, y_conv)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # perform classification for all of the test sizes\n",
    "    tests = {}\n",
    "    best_accuracy = -1\n",
    "    best_test_size = -1\n",
    "    batch_size = 50\n",
    "\n",
    "    for t in test_set_sizes:\n",
    "        offset = 0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"test_set_size = %g\" % t)\n",
    "        X_train, y_train, X_test, y_test = generate_validation_sets(X, y, t)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            X_batch, y_batch, offset = next_batch(X_train, y_train, offset, batch_size)\n",
    "            y_batch_one_hot, _ = one_hot(y_batch, classes)\n",
    "            if i%200 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x: X_batch, y_: y_batch_one_hot, keep_prob: 1.0 })\n",
    "                print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "            train_step.run(feed_dict={ x: X_batch, y_: y_batch_one_hot, keep_prob: 0.5})\n",
    "\n",
    "        y_test_one_hot, _ = one_hot(y_test, classes)\n",
    "\n",
    "        # score is mean accuracy of the classifier\n",
    "        score = accuracy.eval(feed_dict={ x: X_test, y_: y_test_one_hot, keep_prob: 1.0})\n",
    "\n",
    "        # create the confusion matrix\n",
    "        feed_dict = {x: X_test, keep_prob: 1.0}\n",
    "        classification = y_conv.eval(feed_dict)\n",
    "        cm = tf_confusion_matrix(classification, y_test, classes)\n",
    "        \n",
    "        tests[t] = (score, cm)\n",
    "\n",
    "        print(\"test accuracy %g\\n\" % (tests[t][0]))\n",
    "\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set_size = 0.1\n",
      "generating train/test split with test size = 0.1\n",
      "step 0, training accuracy 0.18\n",
      "step 200, training accuracy 0.62\n",
      "step 400, training accuracy 0.6\n",
      "step 600, training accuracy 0.7\n",
      "step 800, training accuracy 0.74\n",
      "step 1000, training accuracy 0.84\n",
      "step 1200, training accuracy 0.8\n",
      "step 1400, training accuracy 0.78\n",
      "step 1600, training accuracy 0.8\n",
      "step 1800, training accuracy 0.82\n",
      "step 2000, training accuracy 0.86\n",
      "step 2200, training accuracy 0.88\n",
      "step 2400, training accuracy 0.86\n",
      "step 2600, training accuracy 0.94\n",
      "step 2800, training accuracy 0.9\n",
      "step 3000, training accuracy 0.96\n",
      "step 3200, training accuracy 0.92\n",
      "step 3400, training accuracy 0.86\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 0.94\n",
      "step 4000, training accuracy 0.88\n",
      "step 4200, training accuracy 0.94\n",
      "step 4400, training accuracy 0.98\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 0.96\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 0.96\n",
      "step 5600, training accuracy 0.94\n",
      "step 5800, training accuracy 0.98\n",
      "step 6000, training accuracy 0.98\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 0.98\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 0.98\n",
      "step 7200, training accuracy 0.98\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 0.98\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.763285\n",
      "\n",
      "test_set_size = 0.2\n",
      "generating train/test split with test size = 0.2\n",
      "step 0, training accuracy 0.3\n",
      "step 200, training accuracy 0.7\n",
      "step 400, training accuracy 0.74\n",
      "step 600, training accuracy 0.78\n",
      "step 800, training accuracy 0.76\n",
      "step 1000, training accuracy 0.8\n",
      "step 1200, training accuracy 0.86\n",
      "step 1400, training accuracy 0.88\n",
      "step 1600, training accuracy 0.92\n",
      "step 1800, training accuracy 0.86\n",
      "step 2000, training accuracy 0.86\n",
      "step 2200, training accuracy 0.94\n",
      "step 2400, training accuracy 0.96\n",
      "step 2600, training accuracy 0.98\n",
      "step 2800, training accuracy 0.98\n",
      "step 3000, training accuracy 0.98\n",
      "step 3200, training accuracy 0.94\n",
      "step 3400, training accuracy 0.98\n",
      "step 3600, training accuracy 0.98\n",
      "step 3800, training accuracy 0.96\n",
      "step 4000, training accuracy 0.96\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 0.96\n",
      "step 4600, training accuracy 0.96\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 0.96\n",
      "step 5400, training accuracy 0.98\n",
      "step 5600, training accuracy 0.98\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 0.98\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.747585\n",
      "\n",
      "test_set_size = 0.3\n",
      "generating train/test split with test size = 0.3\n",
      "step 0, training accuracy 0.14\n",
      "step 200, training accuracy 0.74\n",
      "step 400, training accuracy 0.72\n",
      "step 600, training accuracy 0.88\n",
      "step 800, training accuracy 0.8\n",
      "step 1000, training accuracy 0.86\n",
      "step 1200, training accuracy 0.86\n",
      "step 1400, training accuracy 0.88\n",
      "step 1600, training accuracy 0.84\n",
      "step 1800, training accuracy 0.92\n",
      "step 2000, training accuracy 0.84\n",
      "step 2200, training accuracy 0.94\n",
      "step 2400, training accuracy 0.9\n",
      "step 2600, training accuracy 0.96\n",
      "step 2800, training accuracy 0.92\n",
      "step 3000, training accuracy 0.98\n",
      "step 3200, training accuracy 0.96\n",
      "step 3400, training accuracy 0.98\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 0.98\n",
      "step 4000, training accuracy 0.96\n",
      "step 4200, training accuracy 0.98\n",
      "step 4400, training accuracy 0.98\n",
      "step 4600, training accuracy 0.96\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 0.98\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.761675\n",
      "\n",
      "test_set_size = 0.4\n",
      "generating train/test split with test size = 0.4\n",
      "step 0, training accuracy 0.48\n",
      "step 200, training accuracy 0.6\n",
      "step 400, training accuracy 0.64\n",
      "step 600, training accuracy 0.68\n",
      "step 800, training accuracy 0.76\n",
      "step 1000, training accuracy 0.86\n",
      "step 1200, training accuracy 0.82\n",
      "step 1400, training accuracy 0.86\n",
      "step 1600, training accuracy 0.88\n",
      "step 1800, training accuracy 0.88\n",
      "step 2000, training accuracy 0.8\n",
      "step 2200, training accuracy 0.94\n",
      "step 2400, training accuracy 0.94\n",
      "step 2600, training accuracy 0.86\n",
      "step 2800, training accuracy 0.92\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 0.96\n",
      "step 3600, training accuracy 0.96\n",
      "step 3800, training accuracy 0.98\n",
      "step 4000, training accuracy 0.96\n",
      "step 4200, training accuracy 0.98\n",
      "step 4400, training accuracy 0.98\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 0.96\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 0.98\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 0.98\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 0.98\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.759662\n",
      "\n",
      "test_set_size = 0.5\n",
      "generating train/test split with test size = 0.5\n",
      "step 0, training accuracy 0.62\n",
      "step 200, training accuracy 0.68\n",
      "step 400, training accuracy 0.74\n",
      "step 600, training accuracy 0.84\n",
      "step 800, training accuracy 0.74\n",
      "step 1000, training accuracy 0.82\n",
      "step 1200, training accuracy 0.84\n",
      "step 1400, training accuracy 0.88\n",
      "step 1600, training accuracy 0.96\n",
      "step 1800, training accuracy 0.92\n",
      "step 2000, training accuracy 0.96\n",
      "step 2200, training accuracy 0.96\n",
      "step 2400, training accuracy 1\n",
      "step 2600, training accuracy 0.98\n",
      "step 2800, training accuracy 0.98\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 0.98\n",
      "step 3400, training accuracy 0.98\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.741063\n",
      "\n",
      "test_set_size = 0.6\n",
      "generating train/test split with test size = 0.6\n",
      "step 0, training accuracy 0.32\n",
      "step 200, training accuracy 0.66\n",
      "step 400, training accuracy 0.76\n",
      "step 600, training accuracy 0.72\n",
      "step 800, training accuracy 0.8\n",
      "step 1000, training accuracy 0.86\n",
      "step 1200, training accuracy 0.92\n",
      "step 1400, training accuracy 0.84\n",
      "step 1600, training accuracy 0.94\n",
      "step 1800, training accuracy 0.94\n",
      "step 2000, training accuracy 1\n",
      "step 2200, training accuracy 0.96\n",
      "step 2400, training accuracy 0.98\n",
      "step 2600, training accuracy 1\n",
      "step 2800, training accuracy 0.98\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 0.98\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.730676\n",
      "\n",
      "test_set_size = 0.7\n",
      "generating train/test split with test size = 0.7\n",
      "step 0, training accuracy 0.1\n",
      "step 200, training accuracy 0.74\n",
      "step 400, training accuracy 0.7\n",
      "step 600, training accuracy 0.88\n",
      "step 800, training accuracy 0.86\n",
      "step 1000, training accuracy 0.9\n",
      "step 1200, training accuracy 0.96\n",
      "step 1400, training accuracy 0.92\n",
      "step 1600, training accuracy 0.96\n",
      "step 1800, training accuracy 1\n",
      "step 2000, training accuracy 0.98\n",
      "step 2200, training accuracy 1\n",
      "step 2400, training accuracy 0.98\n",
      "step 2600, training accuracy 1\n",
      "step 2800, training accuracy 0.98\n",
      "step 3000, training accuracy 0.98\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 0.98\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.729814\n",
      "\n",
      "test_set_size = 0.8\n",
      "generating train/test split with test size = 0.8\n",
      "step 0, training accuracy 0.2\n",
      "step 200, training accuracy 0.88\n",
      "step 400, training accuracy 0.82\n",
      "step 600, training accuracy 0.96\n",
      "step 800, training accuracy 0.92\n",
      "step 1000, training accuracy 0.98\n",
      "step 1200, training accuracy 0.96\n",
      "step 1400, training accuracy 0.96\n",
      "step 1600, training accuracy 0.98\n",
      "step 1800, training accuracy 0.98\n",
      "step 2000, training accuracy 1\n",
      "step 2200, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2600, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.697766\n",
      "\n",
      "test_set_size = 0.9\n",
      "generating train/test split with test size = 0.9\n",
      "step 0, training accuracy 0.54\n",
      "step 200, training accuracy 0.84\n",
      "step 400, training accuracy 0.9\n",
      "step 600, training accuracy 0.96\n",
      "step 800, training accuracy 1\n",
      "step 1000, training accuracy 1\n",
      "step 1200, training accuracy 1\n",
      "step 1400, training accuracy 1\n",
      "step 1600, training accuracy 1\n",
      "step 1800, training accuracy 1\n",
      "step 2000, training accuracy 1\n",
      "step 2200, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2600, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.677939\n",
      "\n",
      "test_set_size = 0.1\n",
      "generating train/test split with test size = 0.1\n",
      "step 0, training accuracy 0.2\n",
      "step 200, training accuracy 0.88\n",
      "step 400, training accuracy 0.7\n",
      "step 600, training accuracy 0.88\n",
      "step 800, training accuracy 0.88\n",
      "step 1000, training accuracy 0.9\n",
      "step 1200, training accuracy 0.86\n",
      "step 1400, training accuracy 0.88\n",
      "step 1600, training accuracy 0.94\n",
      "step 1800, training accuracy 0.92\n",
      "step 2000, training accuracy 0.92\n",
      "step 2200, training accuracy 0.92\n",
      "step 2400, training accuracy 0.92\n",
      "step 2600, training accuracy 0.98\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 0.98\n",
      "step 3200, training accuracy 0.98\n",
      "step 3400, training accuracy 0.98\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 0.96\n",
      "step 4000, training accuracy 0.94\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 0.98\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 0.96\n",
      "step 6400, training accuracy 0.98\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.855072\n",
      "\n",
      "test_set_size = 0.2\n",
      "generating train/test split with test size = 0.2\n",
      "step 0, training accuracy 0.74\n",
      "step 200, training accuracy 0.82\n",
      "step 400, training accuracy 0.84\n",
      "step 600, training accuracy 0.92\n",
      "step 800, training accuracy 0.92\n",
      "step 1000, training accuracy 0.8\n",
      "step 1200, training accuracy 0.9\n",
      "step 1400, training accuracy 0.8\n",
      "step 1600, training accuracy 0.98\n",
      "step 1800, training accuracy 0.92\n",
      "step 2000, training accuracy 0.92\n",
      "step 2200, training accuracy 0.94\n",
      "step 2400, training accuracy 0.96\n",
      "step 2600, training accuracy 0.94\n",
      "step 2800, training accuracy 0.96\n",
      "step 3000, training accuracy 0.98\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 0.96\n",
      "step 3600, training accuracy 0.98\n",
      "step 3800, training accuracy 0.98\n",
      "step 4000, training accuracy 0.98\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 0.96\n",
      "step 4600, training accuracy 0.98\n",
      "step 4800, training accuracy 0.98\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 0.98\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 0.98\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.84058\n",
      "\n",
      "test_set_size = 0.3\n",
      "generating train/test split with test size = 0.3\n",
      "step 0, training accuracy 0.26\n",
      "step 200, training accuracy 0.8\n",
      "step 400, training accuracy 0.78\n",
      "step 600, training accuracy 0.92\n",
      "step 800, training accuracy 0.9\n",
      "step 1000, training accuracy 0.9\n",
      "step 1200, training accuracy 0.84\n",
      "step 1400, training accuracy 0.86\n",
      "step 1600, training accuracy 0.88\n",
      "step 1800, training accuracy 0.9\n",
      "step 2000, training accuracy 0.88\n",
      "step 2200, training accuracy 0.94\n",
      "step 2400, training accuracy 0.88\n",
      "step 2600, training accuracy 0.98\n",
      "step 2800, training accuracy 0.96\n",
      "step 3000, training accuracy 0.98\n",
      "step 3200, training accuracy 0.98\n",
      "step 3400, training accuracy 0.96\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 0.92\n",
      "step 4200, training accuracy 0.96\n",
      "step 4400, training accuracy 0.98\n",
      "step 4600, training accuracy 0.98\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 0.98\n",
      "step 5800, training accuracy 0.98\n",
      "step 6000, training accuracy 0.98\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.835749\n",
      "\n",
      "test_set_size = 0.4\n",
      "generating train/test split with test size = 0.4\n",
      "step 0, training accuracy 0.34\n",
      "step 200, training accuracy 0.78\n",
      "step 400, training accuracy 0.74\n",
      "step 600, training accuracy 0.88\n",
      "step 800, training accuracy 0.9\n",
      "step 1000, training accuracy 0.94\n",
      "step 1200, training accuracy 0.9\n",
      "step 1400, training accuracy 0.88\n",
      "step 1600, training accuracy 0.9\n",
      "step 1800, training accuracy 0.92\n",
      "step 2000, training accuracy 0.92\n",
      "step 2200, training accuracy 0.96\n",
      "step 2400, training accuracy 0.9\n",
      "step 2600, training accuracy 0.92\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 0.94\n",
      "step 3200, training accuracy 0.98\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 0.98\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.844203\n",
      "\n",
      "test_set_size = 0.5\n",
      "generating train/test split with test size = 0.5\n",
      "step 0, training accuracy 0.62\n",
      "step 200, training accuracy 0.84\n",
      "step 400, training accuracy 0.82\n",
      "step 600, training accuracy 0.86\n",
      "step 800, training accuracy 0.88\n",
      "step 1000, training accuracy 0.92\n",
      "step 1200, training accuracy 0.96\n",
      "step 1400, training accuracy 0.96\n",
      "step 1600, training accuracy 0.94\n",
      "step 1800, training accuracy 0.94\n",
      "step 2000, training accuracy 0.98\n",
      "step 2200, training accuracy 0.98\n",
      "step 2400, training accuracy 0.98\n",
      "step 2600, training accuracy 0.98\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 0.98\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 0.98\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.823188\n",
      "\n",
      "test_set_size = 0.6\n",
      "generating train/test split with test size = 0.6\n",
      "step 0, training accuracy 0.36\n",
      "step 200, training accuracy 0.8\n",
      "step 400, training accuracy 0.84\n",
      "step 600, training accuracy 0.88\n",
      "step 800, training accuracy 0.86\n",
      "step 1000, training accuracy 0.88\n",
      "step 1200, training accuracy 0.96\n",
      "step 1400, training accuracy 0.88\n",
      "step 1600, training accuracy 0.98\n",
      "step 1800, training accuracy 0.92\n",
      "step 2000, training accuracy 1\n",
      "step 2200, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2600, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 0.98\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.820451\n",
      "\n",
      "test_set_size = 0.7\n",
      "generating train/test split with test size = 0.7\n",
      "step 0, training accuracy 0.72\n",
      "step 200, training accuracy 0.9\n",
      "step 400, training accuracy 0.8\n",
      "step 600, training accuracy 0.92\n",
      "step 800, training accuracy 0.9\n",
      "step 1000, training accuracy 0.98\n",
      "step 1200, training accuracy 1\n",
      "step 1400, training accuracy 0.98\n",
      "step 1600, training accuracy 0.98\n",
      "step 1800, training accuracy 1\n",
      "step 2000, training accuracy 0.98\n",
      "step 2200, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2600, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.832298\n",
      "\n",
      "test_set_size = 0.8\n",
      "generating train/test split with test size = 0.8\n",
      "step 0, training accuracy 0.3\n",
      "step 200, training accuracy 0.88\n",
      "step 400, training accuracy 0.86\n",
      "step 600, training accuracy 0.94\n",
      "step 800, training accuracy 0.94\n",
      "step 1000, training accuracy 0.94\n",
      "step 1200, training accuracy 1\n",
      "step 1400, training accuracy 1\n",
      "step 1600, training accuracy 0.98\n",
      "step 1800, training accuracy 1\n",
      "step 2000, training accuracy 1\n",
      "step 2200, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2600, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.812802\n",
      "\n",
      "test_set_size = 0.9\n",
      "generating train/test split with test size = 0.9\n",
      "step 0, training accuracy 0.46\n",
      "step 200, training accuracy 0.86\n",
      "step 400, training accuracy 0.96\n",
      "step 600, training accuracy 1\n",
      "step 800, training accuracy 1\n",
      "step 1000, training accuracy 1\n",
      "step 1200, training accuracy 1\n",
      "step 1400, training accuracy 1\n",
      "step 1600, training accuracy 1\n",
      "step 1800, training accuracy 1\n",
      "step 2000, training accuracy 1\n",
      "step 2200, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2600, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10600, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 14000, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16600, training accuracy 1\n",
      "step 16800, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18800, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "test accuracy 0.801127\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification for four class and two class schemes\n",
    "results_four = tensorflow_cnn(X_gray, y_gray, classes, test_set_sizes, iterations=20000)\n",
    "results_two = tensorflow_cnn(X_gray, y_gray_binary, binary_classes, test_set_sizes, iterations=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.1: (0.76328504, array([[ 14,   7,  16,   6],\n",
      "       [  3, 220,   9,   4],\n",
      "       [  9,  13,  42,   4],\n",
      "       [  5,   8,  14,  40]])), 0.2: (0.74758452, array([[ 38,  16,  48,   8],\n",
      "       [  7, 413,  16,   9],\n",
      "       [ 17,  32,  99,  12],\n",
      "       [ 14,  13,  17,  69]])), 0.3: (0.7616747, array([[ 60,  25,  46,  24],\n",
      "       [ 12, 638,  28,  13],\n",
      "       [ 29,  46, 142,  14],\n",
      "       [ 17,  25,  17, 106]])), 0.6: (0.73067635, array([[ 117,   64,   87,   42],\n",
      "       [  36, 1285,   60,   18],\n",
      "       [  95,   94,  238,   30],\n",
      "       [  44,   52,   47,  175]])), 0.8: (0.69776571, array([[ 132,   84,  162,   34],\n",
      "       [  48, 1617,  173,   15],\n",
      "       [ 121,  124,  357,   33],\n",
      "       [  55,   75,   77,  205]])), 0.5: (0.74106282, array([[ 107,   53,   81,   27],\n",
      "       [  31, 1071,   48,   14],\n",
      "       [  59,   78,  213,   24],\n",
      "       [  31,   49,   41,  143]])), 0.7: (0.72981364, array([[ 138,   63,  120,   45],\n",
      "       [  41, 1474,   96,   14],\n",
      "       [ 103,  112,  305,   32],\n",
      "       [  46,   55,   56,  198]])), 0.9: (0.67793882, array([[ 102,  150,  152,   56],\n",
      "       [  44, 1857,  138,   50],\n",
      "       [ 119,  211,  331,   59],\n",
      "       [  42,  117,   62,  236]])), 0.4: (0.75966185, array([[ 81,  36,  65,  24],\n",
      "       [ 14, 873,  35,  13],\n",
      "       [ 45,  63, 182,  18],\n",
      "       [ 21,  33,  31, 122]]))}\n",
      "{0.1: (0.85507244, array([[ 71,  39],\n",
      "       [ 21, 283]])), 0.2: (0.84057969, array([[136,  87],\n",
      "       [ 45, 560]])), 0.3: (0.83574879, array([[189, 131],\n",
      "       [ 73, 849]])), 0.6: (0.8204509, array([[ 329,  299],\n",
      "       [ 147, 1709]])), 0.8: (0.81280196, array([[ 453,  371],\n",
      "       [ 249, 2239]])), 0.5: (0.82318842, array([[ 341,  191],\n",
      "       [ 175, 1363]])), 0.7: (0.83229816, array([[ 392,  329],\n",
      "       [ 157, 2020]])), 0.9: (0.8011272, array([[ 436,  481],\n",
      "       [ 260, 2549]])), 0.4: (0.84420288, array([[ 243,  170],\n",
      "       [  88, 1155]]))}\n",
      "Normalized confusion matrix\n",
      "[[ 0.3255814   0.1627907   0.37209302  0.13953488]\n",
      " [ 0.01271186  0.93220339  0.03813559  0.01694915]\n",
      " [ 0.13235294  0.19117647  0.61764706  0.05882353]\n",
      " [ 0.07462687  0.11940299  0.20895522  0.59701493]]\n",
      "Normalized confusion matrix\n",
      "[[ 0.64545455  0.35454545]\n",
      " [ 0.06907895  0.93092105]]\n"
     ]
    }
   ],
   "source": [
    "print(results_four)\n",
    "print(results_two)\n",
    "\n",
    "results_four_acc = []\n",
    "results_two_acc = []\n",
    "for t in test_set_sizes:\n",
    "    results_four_acc.append(results_four[t][0])\n",
    "    results_two_acc.append(results_two[t][0])\n",
    "\n",
    "\n",
    "four_plot, = plt.plot(test_set_sizes, results_four_acc, label='4-class accuracy', color='blue')\n",
    "two_plot, = plt.plot(test_set_sizes, results_two_acc, label='2-class accuracy', color='red')\n",
    "plt.title(\"CNN accuracy vs. test size\")\n",
    "plt.xlabel(\"test set size\")\n",
    "plt.ylabel(\"mean accuracy\")\n",
    "plt.legend(handles=[two_plot, four_plot], loc=5)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,1])\n",
    "plt.show()\n",
    "\n",
    "# plot the confusion matrix for the best results\n",
    "plot_confusion_matrix(results_four[0.1][1], classes, title='Four class CNN confusion matrix, test size=0.1')\n",
    "plt.show()\n",
    "plot_confusion_matrix(results_two[0.1][1], binary_classes, title='Two class CNN confusion matrix, test size=0.1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
